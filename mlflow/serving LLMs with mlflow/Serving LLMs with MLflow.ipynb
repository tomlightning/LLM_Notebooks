{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3bf3ed-ad11-478e-9d53-8dd01fca91b0",
   "metadata": {},
   "source": [
    "# Serving LLMs with MLflow: Leveraging Custom PyFunc\n",
    "\n",
    "# mlflow pyfunc\n",
    "- https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html\n",
    "- https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/part1-named-flavors.html\n",
    "- https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/part2-pyfunc-components.html\n",
    "\n",
    "\n",
    "\n",
    "# microsoft/Phi-3-mini-4k-instruct\n",
    "\n",
    "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
    "\n",
    "The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Phi-3 family with the Mini version in two variants 4K and 128K which is the context length (in tokens) that it can support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "238917c1-22fe-44aa-bab9-a92693ddf15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# Disable tokenizers warnings when constructing pipelines\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Disable a few less-than-useful UserWarnings from setuptools and pydantic\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d901880-7f82-412a-9f62-c6cfd4c339d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 28 16:01:50 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.05              Driver Version: 545.84       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650        On  | 00000000:01:00.0  On |                  N/A |\n",
      "| 54%   50C    P8              N/A /  75W |   3699MiB /  4096MiB |      3%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        28      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        29      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        29      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        33      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A     14569      C   /python3.9                                N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e01e48-672d-47ae-b83c-8a939cdff90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import accelerate\n",
    "import torch\n",
    "import transformers\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7437e4c6-9796-4ccc-82d2-2e8b9bbc8d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a185dd-8dce-4b70-8db9-551e73003f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2610178db5804575903895c3072d75c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7616dd68cf214d0e9f0205e06c067678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dbf43cf18043b2ac4140a81510db39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42db2d1bde8749769903a282941f9fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NOTICE.md:   0%|          | 0.00/1.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e56954ee554729bec7d023c5fd8523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23c2670e6bc4396b81f1d7ca8a191b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CODE_OF_CONDUCT.md:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258428067b364e538bbb5dca12be7ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/18.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4f31d960f34b999d900e6705f63d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SECURITY.md:   0%|          | 0.00/2.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bccec7e347541a9a2a7004215db6dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba7247d643f4a15ab2409eb0e2a4bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi3.py:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3eb5bd047ae4ca6a8652a4a093e9b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf024d94d6e41278f0172edfdf29c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5616aad6af4efcb9b4f076cd6f3909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0813fb38c12d4c64bceb449cc791232b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi3.py:   0%|          | 0.00/73.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77de1522eca0401e8df720c78d349a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sample_finetune.py:   0%|          | 0.00/6.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d56c05caac446bbb7d8a32ff96ee45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8fcf0fce314b99b4b4d4377ce32a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e8bf85daef49d3b0b0d4f70430290e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9af57338e9f4934b5a14ca2004109f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70e06eebcb644eb89283a9ce53daad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/568 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the  instruct model and tokenizer to a local directory cache\n",
    "snapshot_location = snapshot_download(repo_id=\"microsoft/Phi-3-mini-128k-instruct\", local_dir=\"Phi-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ce35ddd-896d-430e-9a60-3b19f5bc6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi3(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        This method initializes the tokenizer and language model\n",
    "        using the specified model snapshot directory.\n",
    "        \"\"\"\n",
    "        # Initialize tokenizer and language model\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"], padding_side=\"left\"\n",
    "        )\n",
    "\n",
    "        config = transformers.AutoConfig.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"], trust_remote_code=True\n",
    "        )\n",
    "        # If you are running this in a system that has a sufficiently powerful GPU with available VRAM,\n",
    "        # uncomment the configuration setting below to leverage triton.\n",
    "        # Note that triton dramatically improves the inference speed performance\n",
    "\n",
    "        #config.attn_config[\"attn_impl\"] = \"triton\"\n",
    "\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            context.artifacts[\"snapshot\"],\n",
    "            config=config,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # NB: If you do not have a CUDA-capable device or have torch installed with CUDA support\n",
    "        # this setting will not function correctly. Setting device to 'cpu' is valid, but\n",
    "        # the performance will be very slow.\n",
    "        #self.model.to(device=\"cpu\")\n",
    "        # If running on a GPU-compatible environment, uncomment the following line:\n",
    "        self.model.to(device=\"cuda\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def _build_prompt(self, instruction):\n",
    "        \"\"\"\n",
    "        This method generates the prompt for the model.\n",
    "        \"\"\"\n",
    "        INSTRUCTION_KEY = \"### Instruction:\"\n",
    "        RESPONSE_KEY = \"### Response:\"\n",
    "        INTRO_BLURB = (\n",
    "            \"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\"\n",
    "        )\n",
    "\n",
    "        return f\"\"\"{INTRO_BLURB}\n",
    "        {INSTRUCTION_KEY}\n",
    "        {instruction}\n",
    "        {RESPONSE_KEY}\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        \"\"\"\n",
    "        This method generates prediction for the given input.\n",
    "        \"\"\"\n",
    "        prompt = model_input[\"prompt\"][0]\n",
    "\n",
    "        # Retrieve or use default values for temperature and max_tokens\n",
    "        temperature = params.get(\"temperature\", 0.1) if params else 0.1\n",
    "        max_tokens = params.get(\"max_tokens\", 1000) if params else 1000\n",
    "\n",
    "        # Build the prompt\n",
    "        prompt = self._build_prompt(prompt)\n",
    "\n",
    "        # Encode the input and generate prediction\n",
    "        # NB: Sending the tokenized inputs to the GPU here explicitly will not work if your system does not have CUDA support.\n",
    "        # If attempting to run this with GPU support, change 'cpu' to 'cuda' for maximum performance\n",
    "        encoded_input = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        output = self.model.generate(\n",
    "            encoded_input,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_tokens,\n",
    "        )\n",
    "\n",
    "        # Removing the prompt from the generated text\n",
    "        prompt_length = len(self.tokenizer.encode(prompt, return_tensors=\"pt\")[0])\n",
    "        generated_response = self.tokenizer.decode(\n",
    "            output[0][prompt_length:], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return {\"candidates\": [generated_response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8a1135-a46f-4102-b55e-c1a2b2959d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import ColSpec, DataType, ParamSchema, ParamSpec, Schema\n",
    "\n",
    "# Define input and output schema\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"prompt\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(DataType.string, \"candidates\")])\n",
    "\n",
    "parameters = ParamSchema(\n",
    "    [\n",
    "        ParamSpec(\"temperature\", DataType.float, np.float32(0.1), None),\n",
    "        ParamSpec(\"max_tokens\", DataType.integer, np.int32(1000), None),\n",
    "    ]\n",
    ")\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=parameters)\n",
    "\n",
    "\n",
    "# Define input example\n",
    "input_example = pd.DataFrame({\"prompt\": [\"What is Neo4J?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbdd8830-65c6-4301-80be-713f424b37b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/olonok/mlflow/mlruns/12', creation_time=1716907379382, experiment_id='12', last_update_time=1716907379382, lifecycle_stage='active', name='phi3-instruct-evaluation', tags={}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name=\"phi3-instruct-evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4da0b5-1432-47d9-b0a5-7cdabafcc786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907eddf5d20946ff9fea1833d16640d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olonok/anaconda3/envs/mlflow2/lib/python3.9/site-packages/_distutils_hack/__init__.py:11: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/home/olonok/anaconda3/envs/mlflow2/lib/python3.9/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Get the current base version of torch that is installed, without specific version modifiers\n",
    "torch_version = torch.__version__.split(\"+\")[0]\n",
    "\n",
    "# Start an MLflow run context and log the PHi3 model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        \"phi3-instruct\",\n",
    "        python_model=Phi3(),\n",
    "        # NOTE: the artifacts dictionary mapping is critical! This dict is used by the load_context() method in our PHi3() class.\n",
    "        artifacts={\"snapshot\": snapshot_location},\n",
    "        pip_requirements=[\n",
    "            f\"torch=={torch_version}\",\n",
    "            f\"transformers=={transformers.__version__}\",\n",
    "            f\"accelerate=={accelerate.__version__}\",\n",
    "\n",
    "        ],\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8039c59-e08d-45d8-ab71-d05fe0c5e839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs:/4c8990d57d2a4f90a2af7c8f7b245618/phi3-instruct'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info.model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88f1a532-a6a6-4578-978a-41a7914e6e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28617042a9e8474ebe87a069d0c87869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb8d0582-1627-4122-873d-bb6af4d5da13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'candidates': [\"\\n        Machine learning is a subset of artificial intelligence that involves the use of algorithms and statistical models to enable computers to improve their performance on a specific task through experience. It is the science of getting computers to learn and act like humans do, and improve their learning over time without being explicitly programmed. These algorithms build a predictive model from sample data, known as training data, and apply this model to new data. This can be used in a variety of fields, such as image and speech recognition, medical diagnosis, stock market trading, and many more. It's an area of computer science that's incredibly vast and continually evolving.\"]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(pd.DataFrame(\n",
    "    {\"prompt\": [\"What is machine learning?\"]}), params={\"temperature\": 0.6}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c6da5-6c81-4566-bdaf-a5abe053037f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow 3.9",
   "language": "python",
   "name": "mlflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
