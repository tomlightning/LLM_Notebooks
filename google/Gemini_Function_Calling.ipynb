{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/google/Gemini_Function_Calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVriLoUiRZmT"
      },
      "source": [
        "# Gemini API: Function Calling\n",
        "\n",
        "Function calling lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with. Function calling lets you use functions as tools in generative AI applications, and you can define more than one function within a single request.\n",
        "\n",
        "https://ai.google.dev/gemini-api/docs/function-calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PEbg4MtTTiG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wko9cvMiNwme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4901c4ff-1bef-4e8a-a02e-e5fe91e98205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fvYqqsZnOILn"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ygVKV-nxOK60"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use function calling, pass a list of functions to the `tools` parameter when creating a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel). The model uses the function name, docstring, parameters, and parameter type annotations to decide if it needs the function to best answer a prompt.\n",
        "\n",
        "> Important: The SDK converts function parameter type annotations to a format the API understands (`genai.protos.FunctionDeclaration`). The API only supports a limited selection of parameter types, and the Python SDK's automatic conversion only supports a subset of that: `AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict`"
      ],
      "metadata": {
        "id": "bRyFRnvm4phT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a: float, b: float):\n",
        "    \"\"\"returns a + b.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: float, b: float):\n",
        "    \"\"\"returns a - b.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "def multiply(a: float, b: float):\n",
        "    \"\"\"returns a * b.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def divide(a: float, b: float):\n",
        "    \"\"\"returns a / b.\"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\", tools=[add, subtract, multiply, divide]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model2 = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\")\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcxrnRfE4ySI",
        "outputId": "dee22c3b-01cd-4742-a7e6-bf1f4ed37282"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-1.5-flash',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=<google.generativeai.types.content_types.FunctionLibrary object at 0x7f374058fbe0>,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function calls naturally fit in to [multi-turn chats](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#multi-turn) as they capture a back and forth interaction between the user and model. The Python SDK's [`ChatSession`](https://ai.google.dev/api/python/google/generativeai/ChatSession) is a great interface for chats because handles the conversation history for you, and using the parameter `enable_automatic_function_calling` simplifies function calling even further."
      ],
      "metadata": {
        "id": "U4LQuzMw48sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(enable_automatic_function_calling=True)"
      ],
      "metadata": {
        "id": "nXK5yuUS4yOw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    \"\"\"\n",
        "    You are a clever calculator.\n",
        "\n",
        "    I have two questions and answer both. explain the secuence of step you took.\n",
        "    Problem statement:\n",
        "    I bought 57 boxes of wine, each one has 12 bottles. If the cost per bottle is 5 dollars, but if you buy more than 30, then individual price it is reduce to 4 to all of them.\n",
        "\n",
        "    Questions:\n",
        "    1)how many bottels I bought?\n",
        "    2)how much I have paid to buy the wine\"\"\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "euGaW-kk4yDQ",
        "outputId": "7cbefdd0-8c20-49b6-88b3-37e231075d1e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I paid 2766 dollars. Here's how I solved it: \n",
            "1. I calculated the total number of bottles: 57 boxes * 12 bottles/box = 684 bottles.\n",
            "2. I calculated the cost of the first 30 bottles: 30 bottles * $5/bottle = $150\n",
            "3. I calculated the number of bottles that were discounted: 684 bottles - 30 bottles = 654 bottles\n",
            "4. I calculated the cost of the discounted bottles: 654 bottles * $4/bottle = $2616\n",
            "5. I added the cost of the first 30 bottles and the discounted bottles: $150 + $2616 = $2766 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "57 *12, 57 * 12 *4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rUv0pnx4x_i",
        "outputId": "c7984b80-f5e0-4b7a-bb84-52188b8c5113"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(684, 2736)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat2 = model2.start_chat()"
      ],
      "metadata": {
        "id": "koBlzqS1asJa"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = chat2.send_message(\n",
        "    \"\"\"\n",
        "    You are a clever calculator.\n",
        "\n",
        "    I have two questions and answer both. explain the secuence of step you took.\n",
        "    Problem statement:\n",
        "    I bought 57 boxes of wine, each one has 12 bottles. If the cost per bottle is 5 dollars, but if you buy more than 30, then individual price it is reduce to 4 to all of them.\n",
        "\n",
        "    Questions:\n",
        "    1)how many bottels I bought?\n",
        "    2)how much I have paid to buy the wine\"\"\"\n",
        ")\n",
        "print(response2.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "WX1JsVV8ayD6",
        "outputId": "225ecbd9-95e4-4d4e-cd7d-0ac8c95909d8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's how we can solve this problem:\n",
            "\n",
            "**1) How many bottles did you buy?**\n",
            "\n",
            "* **Step 1:** Calculate the total number of bottles: 57 boxes * 12 bottles/box = 684 bottles\n",
            "\n",
            "**2) How much did you pay for the wine?**\n",
            "\n",
            "* **Step 1:** Determine how many bottles qualify for the discount: 684 bottles - 30 bottles = 654 bottles\n",
            "* **Step 2:** Calculate the cost of the discounted bottles: 654 bottles * $4/bottle = $2616\n",
            "* **Step 3:** Calculate the cost of the non-discounted bottles: 30 bottles * $5/bottle = $150\n",
            "* **Step 4:** Add the cost of both types of bottles: $2616 + $150 = $2766\n",
            "\n",
            "**Answer:**\n",
            "\n",
            "1. You bought **684** bottles.\n",
            "2. You paid **$2766** for the wine. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ChatSession.history` property stores a chronological record of the conversation between the user and the Gemini model. Each turn in the conversation is represented by a [`genai.protos.Content`](https://ai.google.dev/api/python/google/generativeai/protos/Content) object, which contains the following information:\n",
        "\n",
        "*   **Role**: Identifies whether the content originated from the \"user\" or the \"model\".\n",
        "*   **Parts**: A list of [`genai.protos.Part`](https://ai.google.dev/api/python/google/generativeai/protos/Part) objects that represent individual components of the message. With a text-only model, these parts can be:\n",
        "    *   **Text**: Plain text messages.\n",
        "    *   **Function Call** ([`genai.protos.FunctionCall`](https://ai.google.dev/api/python/google/generativeai/protos/FunctionCall)): A request from the model to execute a specific function with provided arguments.\n",
        "    *   **Function Response** ([`genai.protos.FunctionResponse`](https://ai.google.dev/api/python/google/generativeai/protos/FunctionResponse)): The result returned by the user after executing the requested function.\n",
        "\n",
        " In the previous example with the mittens calculation, the history shows the following sequence:\n",
        "\n",
        "1.  **User**: Asks the question about the total number of mittens.\n",
        "1.  **Model**: Determines that the multiply function is helpful and sends a FunctionCall request to the user.\n",
        "1.  **User**: The `ChatSession` automatically executes the function (due to `enable_automatic_function_calling` being set) and sends back a `FunctionResponse` with the calculated result.\n",
        "1.  **Model**: Uses the function's output to formulate the final answer and presents it as a text response."
      ],
      "metadata": {
        "id": "SVDzLiNM9iL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTFbsYUe9hfV",
        "outputId": "bca3051a-be4d-4d38-bb67-041131c8abca"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': '\\n    You are a clever calculator.\\n\\n    I have two questions and answer both. explain the secuence of step you took.\\n    Problem statement:\\n    I bought 57 boxes of wine, each one has 12 bottles. If the cost per bottle is 5 dollars, but if you buy more than 30, then individual price it is reduce to 4 to all of them.\\n\\n    Questions:\\n    1)how many bottels I bought?\\n    2)how much I have paid to buy the wine'}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'multiply', 'args': {'a': 57.0, 'b': 12.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'multiply', 'response': {'result': 684.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'text': 'I bought 684 bottles. \\n'}, {'function_call': {'name': 'multiply', 'args': {'b': 5.0, 'a': 30.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'multiply', 'response': {'result': 150.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'subtract', 'args': {'a': 684.0, 'b': 30.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'subtract', 'response': {'result': 654.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'multiply', 'args': {'a': 654.0, 'b': 4.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'multiply', 'response': {'result': 2616.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'add', 'args': {'a': 2616.0, 'b': 150.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'add', 'response': {'result': 2766.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'text': \"I paid 2766 dollars. Here's how I solved it: \\n1. I calculated the total number of bottles: 57 boxes * 12 bottles/box = 684 bottles.\\n2. I calculated the cost of the first 30 bottles: 30 bottles * $5/bottle = $150\\n3. I calculated the number of bottles that were discounted: 684 bottles - 30 bottles = 654 bottles\\n4. I calculated the cost of the discounted bottles: 654 bottles * $4/bottle = $2616\\n5. I added the cost of the first 30 bottles and the discounted bottles: $150 + $2616 = $2766 \\n\"}]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsMUFmUaTZzt"
      },
      "source": [
        "## Single Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLIDhRAfTgfI"
      },
      "source": [
        "Gemini has the ability to call user-defined functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IIe-cXLHOWoo"
      },
      "outputs": [],
      "source": [
        "def get_full_menu(service: str):\n",
        "    \"\"\"List all items on the menu of Gemini's Trattoria for the given service.\n",
        "\n",
        "    Args:\n",
        "        name: The type of service, lunch or dinner.\n",
        "    \"\"\"\n",
        "    return [\"Chicken Caesar Salad\", \"Margherita Pizza\", \"Spaghetti and Meatballs\", \"Eggplant Parmesan\"]\n",
        "\n",
        "\n",
        "def find_vegetarian_items(items: list[str]):\n",
        "    \"\"\"List all dishes in items that are vegetarian.\n",
        "\n",
        "    Args:\n",
        "        items: A list of dinner dishes.\n",
        "    \"\"\"\n",
        "    return [\"Margherita Pizza\", \"Eggplant Parmesan\"]\n",
        "\n",
        "def enter_restaurant():\n",
        "    \"\"\"You enter Gemini's Trattoria, moving the creaky door.\"\"\"\n",
        "    print(\"The door swings open, making a loud noise.\")\n",
        "    return True\n",
        "\n",
        "functions = {\"get_full_menu\": get_full_menu,\n",
        "             \"find_vegetarian_items\": find_vegetarian_items,\n",
        "             \"enter_restaurant\": enter_restaurant}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQDJ_k4qfhb_"
      },
      "source": [
        "After this, we go ahead and define our Gemini model. Notice how we include the argument for tools, which tells the model which functions it has available to use. We create a chat and set automatic function calling to True, which we will touch on later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "V8stPgRtOaEN"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\", tools=functions.values()\n",
        ")\n",
        "\n",
        "chat = model.start_chat(enable_automatic_function_calling=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSR_d9Mofu_5"
      },
      "source": [
        "Now, we go ahead and send a prompt that requires the model to call our user-defined functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "enDA7oh7OeGO",
        "outputId": "af0c304a-0c2a-441c-ac35-f750ef221e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dinner menu includes: Chicken Caesar Salad, Margherita Pizza, Spaghetti and Meatballs, and Eggplant Parmesan. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\n",
        "    \"What items are on Gemini's Trattoria's dinner menu?\"\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7miJvuiCLJF"
      },
      "source": [
        "As we can see in the chat history, the model initially sends back a function call, to which we automatically respond, which then leads to the final model output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mZCERSwOhuh",
        "outputId": "47cbffe1-fcc6-45a4-dd99-56da76feafad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': \"What items are on Gemini's Trattoria's dinner menu?\"}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'get_full_menu', 'args': {'service': 'dinner'}}}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'get_full_menu', 'response': {'result': ['Chicken Caesar Salad', 'Margherita Pizza', 'Spaghetti and Meatballs', 'Eggplant Parmesan']}}}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'text': 'The dinner menu includes: Chicken Caesar Salad, Margherita Pizza, Spaghetti and Meatballs, and Eggplant Parmesan. \\n'}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 180)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiGwwQs6TkiH"
      },
      "source": [
        "## Behind The Scenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ1gZGNyTrPr"
      },
      "source": [
        "In general, the processes goes as follows:\n",
        "1. The user submits a query to the model.\n",
        "2. The model responds with a function call.\n",
        "3. The user runs the function and returns the result of the function.\n",
        "4. Now, the model will either go back to Step 2 or output a final response, as seen above.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://video.udacity-data.com/topher/2024/June/66749652_function_calling/function_calling.jpeg\" width=\"500\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in9bG0g7kXn3"
      },
      "source": [
        "We can see this step-by-step in action by running the previous example with manual function calling, by setting the automatic function calling argument to False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "yp-3ixnGkuHr",
        "outputId": "cd51cf45-c0df-4be2-e9f0-204894971cea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "function_call {\n",
              "  name: \"get_full_menu\"\n",
              "  args {\n",
              "    fields {\n",
              "      key: \"service\"\n",
              "      value {\n",
              "        string_value: \"dinner\"\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "chat = model.start_chat(enable_automatic_function_calling=False)\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"What items are on Gemini's Trattoria's dinner menu?\"\n",
        ")\n",
        "\n",
        "part = response.candidates[0].content.parts[0]\n",
        "part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kYKYoDElpzz"
      },
      "source": [
        "After this, we can reply to the model as specified in Step 3, using the functions dictionary we made earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "bCD47Ry7lpKx",
        "outputId": "a65d139c-78b7-4380-8144-9c400e599781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dinner menu includes: Chicken Caesar Salad, Margherita Pizza, Spaghetti and Meatballs, Eggplant Parmesan. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import google.ai.generativelanguage as glm\n",
        "from google.protobuf.struct_pb2 import Struct\n",
        "\n",
        "# Put the result in a protobuf Struct\n",
        "s = Struct()\n",
        "result = functions[part.function_call.name](**part.function_call.args)\n",
        "s.update({\"result\": result})\n",
        "\n",
        "function_response = glm.Part(\n",
        "    function_response=glm.FunctionResponse(name=\"get_full_menu\", response=s)\n",
        ")\n",
        "\n",
        "# Generate the next response\n",
        "response = chat.send_message(function_response)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3q5nWqyoj6m"
      },
      "source": [
        "## Multiple Function Calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iug5VBZ5kWw2"
      },
      "source": [
        "Our model can also call multiple functions in a row, either at the same time or one after the other, as we see in both cases below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "S7_gRd2QQCId",
        "outputId": "32ef2c0d-d312-4a25-f8b5-125ff1bdd213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vegetarian items on the dinner menu are Margherita Pizza and Eggplant Parmesan. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"What items on Gemini's Trattoria's dinner menu are vegetarian?\"\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJNvCH77jaYf",
        "outputId": "054ee37e-2ab7-4b00-9875-daf525ee08a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': \"Your are standing outside of Gemini's Trattoria. Enter the restaurant and read out the items on the menu.\"}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'enter_restaurant', 'args': {}}}, {'function_call': {'name': 'get_full_menu', 'args': {'service': 'dinner'}}}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'enter_restaurant', 'response': {'result': True}}}, {'function_response': {'name': 'get_full_menu', 'response': {'result': ['Chicken Caesar Salad', 'Margherita Pizza', 'Spaghetti and Meatballs', 'Eggplant Parmesan']}}}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'text': 'I enter the restaurant, pushing open the creaky door. The menu features the following items:\\n\\n* Chicken Caesar Salad\\n* Margherita Pizza\\n* Spaghetti and Meatballs\\n* Eggplant Parmesan \\n'}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 180)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "9WqeUW_zoadm",
        "outputId": "50b20fde-cdf9-4b74-8e97-3745e7a99447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The door swings open, making a loud noise.\n",
            "I enter the restaurant, pushing open the creaky door. The aroma of garlic and herbs hits me immediately. It's a cozy little place, full of chatter and laughter. Looking at the menu, I see the following items:\n",
            "\n",
            "* Chicken Caesar Salad\n",
            "* Margherita Pizza\n",
            "* Spaghetti and Meatballs\n",
            "* Eggplant Parmesan \n",
            "\n"
          ]
        }
      ],
      "source": [
        "chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"Your are standing outside of Gemini's Trattoria. Enter the restaurant and read out the items on the menu.\"\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaP34mbbpUyx",
        "outputId": "6a5dc494-d015-496d-8f56-3e05a415da02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': \"Your are standing outside of Gemini's Trattoria. Enter the restaurant and read out the items on the menu.\"}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'enter_restaurant', 'args': {}}}, {'function_call': {'name': 'get_full_menu', 'args': {'service': 'dinner'}}}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'enter_restaurant', 'response': {'result': True}}}, {'function_response': {'name': 'get_full_menu', 'response': {'result': ['Chicken Caesar Salad', 'Margherita Pizza', 'Spaghetti and Meatballs', 'Eggplant Parmesan']}}}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'text': \"I enter the restaurant, pushing open the creaky door. The aroma of garlic and herbs hits me immediately. It's a cozy little place, full of chatter and laughter. Looking at the menu, I see the following items:\\n\\n* Chicken Caesar Salad\\n* Margherita Pizza\\n* Spaghetti and Meatballs\\n* Eggplant Parmesan \\n\"}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 180)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hAZc4iCp20w"
      },
      "source": [
        "## Function Calling Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXF1laiBqVkW"
      },
      "source": [
        "# Function calling mode\n",
        "You can use the Function Calling mode parameter to modify the execution behavior of the feature. There are three modes available:\n",
        "\n",
        "- AUTO: The default model behavior. The model decides to predict either a function call or a natural language response.\n",
        "- ANY: The model is constrained to always predict a function call. If allowed_function_names is not provided, the model picks from all of the available function declarations. If allowed_function_names is provided, the model picks from the set of allowed functions.\n",
        "- NONE: The model won't predict a function call. In this case, the model behavior is the same as if you don't pass any function declarations.\n",
        "\n",
        "You can also pass a set of allowed_function_names that, when provided, limits the functions that the model will call. You should only include allowed_function_names when the mode is ANY. Function names should match function declaration names. With the mode set to ANY and the allowed_function_names set, the model will predict a function call from the set of function names provided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MS_lpBLRpuli"
      },
      "outputs": [],
      "source": [
        "from google.generativeai.types import content_types\n",
        "from collections.abc import Iterable\n",
        "\n",
        "\n",
        "def tool_config_from_mode(mode: str, fns: Iterable[str] = ()):\n",
        "    \"\"\"Create a tool config with the specified function calling mode.\"\"\"\n",
        "    return content_types.to_tool_config(\n",
        "        {\"function_calling_config\": {\"mode\": mode, \"allowed_function_names\": fns}}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MkyISN3rCVd"
      },
      "source": [
        "First, we use the NONE mode, which tells the model to not make any function calls. In this example, the model knows about the get_full_menu function but is unable to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "IOnPq6zVrMCq",
        "outputId": "e88e25e5-34d9-4ab3-be6d-a93a3a62c539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': \"What items on Gemini's Trattoria's dinner menu are vegetarian?\"}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'text': \"Please provide me with the dinner menu of Gemini's Trattoria so I can identify the vegetarian options. \\n\"}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "tool_config = tool_config_from_mode(\"none\")\n",
        "chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"What items on Gemini's Trattoria's dinner menu are vegetarian?\", tool_config=tool_config\n",
        ")\n",
        "\n",
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 180)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIDmVQUfrqh_"
      },
      "source": [
        "AUTO mode lets the model decide whether to reply with text or to call specific functions. In this example, we see that the model calls a function to get all menu items but is able to reason on its own on which items are pasta dishes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "B3kOMeGirZAp",
        "outputId": "8338438a-83c0-4bbb-8c04-6632bd5a539e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': \"What items on Gemini's Trattoria's dinner menu are pasta dishes?\"}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'get_full_menu', 'args': {'service': 'dinner'}}}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'get_full_menu', 'response': {'result': ['Chicken Caesar Salad', 'Margherita Pizza', 'Spaghetti and Meatballs', 'Eggplant Parmesan']}}}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'text': 'The dinner menu has one pasta dish: Spaghetti and Meatballs. \\n'}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "tool_config = tool_config_from_mode(\"auto\")\n",
        "chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"What items on Gemini's Trattoria's dinner menu are pasta dishes?\", tool_config=tool_config\n",
        ")\n",
        "\n",
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 180)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMwHF05_sTW0"
      },
      "source": [
        "Finally, ANY mode forces the model to make function calls, as seen in the below example. We decide to switch over to the Gemini 1.5 Pro model to achieve the intended behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "5rn0Xoi6ryDv",
        "outputId": "dc6ae6c1-0e86-4d9e-d751-27ce60c94102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': \"Enter Gemini's Trattoria.\"}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'enter_restaurant', 'args': {}}}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-pro\", tools=functions.values()\n",
        ")\n",
        "\n",
        "tool_config = tool_config_from_mode(\"any\")\n",
        "chat = model.start_chat()\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"Enter Gemini's Trattoria.\", tool_config=tool_config\n",
        ")\n",
        "\n",
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 180)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ribpz_fszjN"
      },
      "source": [
        "By setting allowed_function_names, the model will only choose from those functions. If it is not set, all of the functions in tools are candidates for function calling. In the below example, since we are in ANY mode, the model is forced to use a function, even though it is not necessarily the best option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "Z_lU1Xntu9ps",
        "outputId": "a6505929-c1ce-468f-88d3-f4027147b481"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': \"Enter Gemini's Trattoria.\"}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'get_full_menu', 'args': {'service': 'lunch'}}}]\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "tool_config = tool_config_from_mode(\"any\", [\"get_full_menu\"])\n",
        "chat = model.start_chat()\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"Enter Gemini's Trattoria.\", tool_config=tool_config\n",
        ")\n",
        "\n",
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 180)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd2ALTSCxA47"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (vertex)",
      "language": "python",
      "name": "vertex"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}