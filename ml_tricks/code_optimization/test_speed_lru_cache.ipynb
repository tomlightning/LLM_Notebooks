{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227891f7-c485-4a43-b7c1-0e1178821b05",
   "metadata": {},
   "source": [
    "# https://docs.python.org/3/library/functools.html\n",
    "\n",
    "The Least Recently Used (LRU) cache is a cache eviction algorithm that organizes elements in order of use. In LRU, as the name suggests, the element that hasn't been used for the longest time will be evicted from the cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defc6e3-8022-4a92-8792-fc6224679d8f",
   "metadata": {},
   "source": [
    "# https://numpy.org/doc/stable/reference/generated/numpy.vectorize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a6cd8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import lru_cache , cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcbb66f0-737f-4693-9dd2-1b49504d95f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d556897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "prt = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e43d2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document_path):\n",
    "    \n",
    "    with open(document_path, 'r', encoding = \"utf-8\") as file:\n",
    "        document = file.read()\n",
    "        tokens = document.split(\" \")\n",
    "        tokens_pun_lower = [i.lower() for i in tokens if i.isalnum()]\n",
    "        tokens_stop = [i for i in tokens_pun_lower if (len(i) > 1) ]\n",
    "        terms = [prt.stem(i) for i in tokens_stop]\n",
    "    \n",
    "    return \" \".join(tokens_stop)\n",
    "    \n",
    "# Least Recently Used\n",
    "@lru_cache(maxsize = 128) \n",
    "def preprocess_lru(document_path):\n",
    "    \n",
    "    with open(document_path, 'r', encoding = \"utf-8\") as file:\n",
    "        document = file.read()\n",
    "        tokens = document.split(\" \")\n",
    "        tokens_pun_lower = [i.lower() for i in tokens if i.isalnum()]\n",
    "        tokens_stop = [i for i in tokens_pun_lower if (len(i) > 1) ]\n",
    "        terms = [prt.stem(i) for i in tokens_stop]\n",
    "    \n",
    "    return \" \".join(terms)\n",
    "\n",
    "@cache\n",
    "def preprocess_cache(document_path):\n",
    "    \n",
    "    with open(document_path, 'r', encoding = \"utf-8\") as file:\n",
    "        document = file.read()\n",
    "        tokens = document.split(\" \")\n",
    "        tokens_pun_lower = [i.lower() for i in tokens if i.isalnum()]\n",
    "        tokens_stop = [i for i in tokens_pun_lower if (len(i) > 1) ]\n",
    "        terms = [prt.stem(i) for i in tokens_stop]\n",
    "    \n",
    "    return \" \".join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33b529a9-13e6-4ac7-869d-683303c95a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"D:/repos/custom_classifier/texto.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9b1330ca-cb3f-4ea7-bd33-6d9accdf3568",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'r', encoding = \"utf-8\") as file:\n",
    "    document = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cf70ad8c-8598-4286-bb40-9c4c556b1af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4541"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0d7e208b-d3d3-4b69-a2c6-8ed8d5b7bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 ms ± 154 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit documents = preprocess(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cdd5ee0d-92cc-4511-bfbc-20c8a23aeb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 ns ± 3.29 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit documents = preprocess_lru(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e92dc287-463f-4d6f-8286-f1456738cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 ns ± 1.66 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit documents = preprocess_cache(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "506ae440-5070-4d45-9b3b-efde24ede716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.87179487179488"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(14.6 * 1000) / 195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a92ff0fc-97a8-4e42-af2c-7100420d92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"documents_cv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "067f9295-69df-440c-84dd-b4718893df9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Document</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15073</th>\n",
       "      <td>Zorah Noar.txt</td>\n",
       "      <td>evaluation the document was created with for z...</td>\n",
       "      <td>non-cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15074</th>\n",
       "      <td>Zorina Yarranton.txt</td>\n",
       "      <td>evaluation the document was created with for z...</td>\n",
       "      <td>non-cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15075</th>\n",
       "      <td>Zsa zsa Feldmark.txt</td>\n",
       "      <td>evaluation the document was created with for z...</td>\n",
       "      <td>non-cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15076</th>\n",
       "      <td>Zulema Pedden.txt</td>\n",
       "      <td>evaluation the document was created with for z...</td>\n",
       "      <td>non-cv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15077</th>\n",
       "      <td>Zulema Road.txt</td>\n",
       "      <td>evaluation the document was created with for z...</td>\n",
       "      <td>non-cv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Title  \\\n",
       "15073        Zorah Noar.txt   \n",
       "15074  Zorina Yarranton.txt   \n",
       "15075  Zsa zsa Feldmark.txt   \n",
       "15076     Zulema Pedden.txt   \n",
       "15077       Zulema Road.txt   \n",
       "\n",
       "                                                Document   Class  \n",
       "15073  evaluation the document was created with for z...  non-cv  \n",
       "15074  evaluation the document was created with for z...  non-cv  \n",
       "15075  evaluation the document was created with for z...  non-cv  \n",
       "15076  evaluation the document was created with for z...  non-cv  \n",
       "15077  evaluation the document was created with for z...  non-cv  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "906d31c5-ab52-4a99-98ce-472220de6973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15078"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42e105e9-efff-40e1-9e5b-d6e2f430c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs= []\n",
    "for i in range(10):\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5e502a1e-bc19-42f4-8917-6042db4251ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe =  pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1272a742-80cb-40a7-82ee-a3d11002daa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150780"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e4a839f7-759f-4889-8fc8-1c59671fdca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text, n=2):\n",
    "    text_list = str(text).split(\" \")\n",
    "    total_count=0\n",
    "    for m in [n, n * 2, n*3]:\n",
    "        tokens_n_length = [x for x in text_list if len(x)== m]\n",
    "        total_count = total_count + len(tokens_n_length)\n",
    "    return total_count\n",
    "    \n",
    "@lru_cache(maxsize = 128) \n",
    "def count_words_lru(text, n=2):\n",
    "    text_list = str(text).split(\" \")\n",
    "    total_count=0\n",
    "    for m in [n, n * 2, n*3]:\n",
    "        tokens_n_length = [x for x in text_list if len(x)== m]\n",
    "        total_count = total_count + len(tokens_n_length)\n",
    "    return total_count\n",
    "\n",
    "@cache\n",
    "def count_words_cache(text, n=2):\n",
    "    text_list = str(text).split(\" \")\n",
    "    total_count=0\n",
    "    for m in [n, n * 2, n*3]:\n",
    "        tokens_n_length = [x for x in text_list if len(x)== m]\n",
    "        total_count = total_count + len(tokens_n_length)\n",
    "    return total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "70688291-5d4c-4881-bfbd-95d5fdf41ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3 s ± 93.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dataframe[\"count\"] = dataframe['Document'].apply(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "953343dc-7928-49e3-8444-17bb6c157730",
   "metadata": {},
   "outputs": [],
   "source": [
    "countwords_vectorized= np.vectorize(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "04d3b763-8ee0-4b01-ab5f-4c7bac10419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.98 s ± 59.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dataframe[\"count\"] = countwords_vectorized(dataframe['Document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6928485d-0798-4120-ab78-fef59aea08f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.15 s ± 61.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dataframe[\"count\"] = dataframe['Document'].apply(count_words_lru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb3e8e43-6d96-4bf7-b890-b88a0aad85c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 ms ± 2.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dataframe[\"count\"] = dataframe['Document'].apply(count_words_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97577ec4-a52f-4613-8d86-8247e5adcdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (azure_ml)",
   "language": "python",
   "name": "azure_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
