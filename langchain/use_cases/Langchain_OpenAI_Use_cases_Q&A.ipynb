{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPVq6V8TIIep961NIzUeYzK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/langchain/use_cases/Langchain_OpenAI_Use_cases_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "\n",
        "LangChain is a framework for developing applications powered by language models.\n",
        "\n",
        "https://python.langchain.com/docs/use_cases\n",
        "\n",
        "## Langchain QA\n",
        "\n",
        "https://python.langchain.com/docs/get_started/introduction\n",
        "\n",
        "https://python.langchain.com/docs/use_cases/question_answering/\n",
        "\n",
        "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
        "\n",
        "\n",
        "https://python.langchain.com/docs/modules/model_io/prompts/\n",
        "\n",
        "https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter\n",
        "\n",
        "https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf\n",
        "\n",
        "https://www.promptingguide.ai/techniques/knowledge\n",
        "\n"
      ],
      "metadata": {
        "id": "J2qcri2WPycg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PAO4dulnEUOJ"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community tiktoken -q\n",
        "!pip install -U accelerate -q\n",
        "! pip install -U unstructured numpy -q\n",
        "! pip install openai chromadb -q\n",
        "! pip install pypdf -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "2sGhLPYoKNal"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aMLt2iSKNX9",
        "outputId": "e6bd188e-e35d-430d-896f-b9345b436e16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('KEY_OPENAI')"
      ],
      "metadata": {
        "id": "XdFhRM0dKNVY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "import pprint\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=0, model_name='gpt-3.5-turbo-instruct', openai_api_key=openai_api_key)\n",
        "\n",
        "# Create our template\n",
        "template = \"\"\"\n",
        "%INSTRUCTIONS: Answer the question unsing the provided context. Detail as much as possible your answer.\n",
        "%CONTEXT:\n",
        "The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes.\n",
        "Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n",
        "\n",
        "Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes.\n",
        "The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n",
        "%QUESTION:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "# Create LangChain prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "Sku9VsGGKNO4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b31f03-2a06-44df-ff59-19a91b5bf4fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample1= \"\"\"\n",
        "Part of golf is trying to get a higher point total than others. Yes or No?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "76PrbUYdL9GT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = prompt.format(text=sample1)\n",
        "\n",
        "print(final_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTzA9z0LL9Db",
        "outputId": "5069d3e7-8644-4818-977d-f303d9e443e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "%INSTRUCTIONS: Answer the question unsing the provided context. Detail as much as possible your answer.\n",
            "%CONTEXT:\n",
            "The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. \n",
            "Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n",
            "\n",
            "Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. \n",
            "The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n",
            "%QUESTION:\n",
            "\n",
            "Part of golf is trying to get a higher point total than others. Yes or No?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.invoke(final_prompt)\n",
        "pprint.pprint(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL7P9JfDL9BK",
        "outputId": "695c42d3-f166-4ec3-8b1c-78742e051652"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('\\n'\n",
            " 'No. The objective of golf is to play the course with the fewest number of '\n",
            " 'strokes, so the goal is to have a lower point total than others.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Embeddings\n",
        "\n",
        "https://python.langchain.com/docs/integrations/vectorstores\n",
        "\n",
        "https://docs.trychroma.com/integrations/langchain\n",
        "\n"
      ],
      "metadata": {
        "id": "5lc527wLOCaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "\n",
        "# The vectorstore we'll be using\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# The LangChain component we'll use to get the documents\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# The easy document loader for text\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# The embedding engine that will convert our text to vectors\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "cycRNN9eL87L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_path = \"/content/drive/MyDrive/data/llm_doc.txt\"\n",
        "\n",
        "with open(text_path, 'r', encoding='windows-1252') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# PPrinting the first 1000 characters\n",
        "pprint.pprint(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwJjZN8oOLYR",
        "outputId": "cc9ae5b2-e8df-4141-ec0b-8497f31ffba0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('\\n'\n",
            " 'Large Language Models are Zero-Shot Reasoners\\n'\n",
            " '\\n'\n",
            " '\\n'\n",
            " '\\n'\n",
            " 'Takeshi Kojima\\n'\n",
            " 'The University of Tokyo\\n'\n",
            " 't.kojima@weblab.t.u-tokyo.ac.jp\\n'\n",
            " '\\n'\n",
            " 'Shixiang Shane Gu\\n'\n",
            " 'Google Research, Brain Team\\n'\n",
            " '\\n'\n",
            " '\\n'\n",
            " '\\n'\n",
            " 'Machel Reid\\n'\n",
            " 'Google Research?\\n'\n",
            " '\\n'\n",
            " 'Yutaka Matsuo\\n'\n",
            " 'The University of Tokyo\\n'\n",
            " '\\n'\n",
            " 'Yusuke Iwasawa\\n'\n",
            " 'The University of Tokyo\\n'\n",
            " '\\n'\n",
            " '\\n'\n",
            " '\\n'\n",
            " 'Abstract\\n'\n",
            " 'Pretrained large language models (LLMs) are widely used in many sub-fields '\n",
            " 'of natural language processing (NLP) and generally known as excellent '\n",
            " 'few-shot learners with task-specific exemplars. Notably, chain of thought '\n",
            " '(CoT) prompting, a recent technique for eliciting complex multi-step '\n",
            " 'reasoning through step-by- step answer examples, achieved the '\n",
            " 'state-of-the-art performances in arithmetics and symbolic reasoning, '\n",
            " 'difficult system-2 tasks that do not follow the standard scaling laws for '\n",
            " 'LLMs. While these successes are often attributed to LLMs’ ability for '\n",
            " 'few-shot learning, we show that LLMs are decent zero-shot reasoners by '\n",
            " 'simply adding “Let’s think step by step” before each answer. Experimental r')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader(text_path, encoding='windows-1252')\n",
        "doc = loader.load()\n",
        "print (f\"You have {len(doc)} document\")\n",
        "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNMQMZTGAJGd",
        "outputId": "1461a037-b30c-41d3-99e7-652a4c63a075"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 1 document\n",
            "You have 119397 characters in that document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=400)\n",
        "docs = text_splitter.split_documents(doc)"
      ],
      "metadata": {
        "id": "pGuietfoOhUT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D_0aDAj5TCxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of characters so we can see the average later\n",
        "num_total_characters = sum([len(x.page_content) for x in docs])\n",
        "\n",
        "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6kQktRNOhQ0",
        "outputId": "64aac72b-21bc-4cec-b1d0-9cba30ac67ac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now you have 38 documents that have an average of 3,261 characters (smaller pieces)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your embeddings engine ready\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "# Embed your documents and combine with the raw text in a pseudo db. Note: This will make an API call to OpenAI\n",
        "docsearch = Chroma.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7tyGKu9V1Zt",
        "outputId": "9252ce97-9bd1-4307-ca02-788ba725147a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-8_7W7pMnL8",
        "outputId": "f79d02c6-d77f-467e-8ddb-8f339bf90729"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3794"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-BXs4CRqBVPW",
        "outputId": "0125e94a-9858-4d09-c881-9aa889670b6b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Large Language Models are Zero-Shot Reasoners\\n\\n\\n\\nTakeshi Kojima\\nThe University of Tokyo\\nt.kojima@weblab.t.u-tokyo.ac.jp\\n\\nShixiang Shane Gu\\nGoogle Research, Brain Team\\n\\n\\n\\nMachel Reid\\nGoogle Research?\\n\\nYutaka Matsuo\\nThe University of Tokyo\\n\\nYusuke Iwasawa\\nThe University of Tokyo\\n\\n\\n\\nAbstract\\nPretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by- step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs’ ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding “Let’s think step by step” before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large-scale InstructGPT model (text-davinci- 002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.\\n\\n1 Introduction\\nScaling up the size of language models has been key ingredients of recent revolutions in natural language processing (NLP) [Vaswani et al., 2017, Devlin et al., 2019, Raffel et al., 2020, Brown et al., 2020, Thoppilan et al., 2022, Rae et al., 2021, Chowdhery et al., 2022]. The success of large language models (LLMs) is often attributed to (in-context) few-shot or zero-shot learning. It can solve various tasks by simply conditioning the models on a few examples (few-shot) or instructions describing the task (zero-shot). The method of conditioning the language model is called “prompting” [Liu et al., 2021b], and designing prompts either manually [Schick and Schütze, 2021, Reynolds and McDonell, 2021] or automatically [Gao et al., 2021, Shin et al., 2020] has become a hot topic in NLP.\\n\\n?Work done while at The University of Tokyo.\\n\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\\n\\n(a) Few-shot\\t(b) Few-shot-CoT\\n(c) Zero-shot\\t(d) Zero-shot-CoT (Ours)\\n\\t\\nFigure 1: Example inputs and outputs of GPT-3 with (a) standard Few-shot ([Brown et al., 2020]), (b) Few-shot-CoT ([Wei et al., 2022]), (c) standard Zero-shot, and (d) ours (Zero-shot-CoT). Similar to Few-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer where standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per task, ours does not need any examples and just uses the same prompt “Let’s think step by step” across all tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3xGUzpNMuhX",
        "outputId": "feb8564e-921b-4c32-814b-93fdc46bf123"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': '/content/drive/MyDrive/data/llm_doc.txt'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ],
      "metadata": {
        "id": "VJGn2ED3OhNq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Why Large Language models are zero reasoners?\"\n",
        "response = qa.invoke(query)"
      ],
      "metadata": {
        "id": "5RqaHhCrOhKi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t1S7FIXNTJS",
        "outputId": "2fa53f76-e331-4350-f119-07b6815604c4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'Why Large Language models are zero reasoners?',\n",
            " 'result': '\\n'\n",
            "           'Large language models are considered to be zero-shot reasoners '\n",
            "           'because they have the ability to solve various tasks without any '\n",
            "           'prior training or examples. This is achieved by simply '\n",
            "           'conditioning the models on a few examples or instructions '\n",
            "           'describing the task. This method, known as \"prompting\", has become '\n",
            "           'a popular topic in natural language processing. Additionally, '\n",
            "           'recent studies have shown that large language models have '\n",
            "           'excellent zero-shot abilities in many tasks, such as reading '\n",
            "           'comprehension, translation, and summarization. This suggests that '\n",
            "           'these models have untapped and understudied fundamental zero-shot '\n",
            "           'capabilities, which can be extracted by simple prompting.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduce a memory Buffer"
      ],
      "metadata": {
        "id": "sZb3CXPjEOC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "UHzlW8WRNox1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Use the following context (delimited by <ctx></ctx>) and the chat history (delimited by <hs></hs>) to answer the question:\n",
        "------\n",
        "<ctx>\n",
        "{context}\n",
        "</ctx>\n",
        "------\n",
        "<hs>\n",
        "{history}\n",
        "</hs>\n",
        "------\n",
        "{question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"context\", \"question\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "K6p3fJukDD9x"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type='stuff',\n",
        "    retriever=docsearch.as_retriever(),\n",
        "    chain_type_kwargs={\n",
        "        \"verbose\": False,\n",
        "        \"prompt\": prompt,\n",
        "        \"memory\": ConversationBufferMemory(\n",
        "            memory_key=\"history\",\n",
        "            input_key=\"question\"),\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "oEK8PHJfDD02"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Why Large Language models are zero reasoners?\"\n",
        "response = qa.invoke(query)\n",
        "pprint.pprint(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PMznsX9DWlT",
        "outputId": "cad60f9c-6d2d-40c4-bb47-fee87f2715a2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'Why Large Language models are zero reasoners?',\n",
            " 'result': '\\n'\n",
            "           'Large language models are zero-shot reasoners because they are '\n",
            "           'able to solve various tasks without any examples or instructions, '\n",
            "           'simply by conditioning the model on a few examples or instructions '\n",
            "           'describing the task. This method of conditioning the language '\n",
            "           'model is called \"prompting\" and has become a hot topic in natural '\n",
            "           'language processing. Recent studies have shown that pre-trained '\n",
            "           'models are not good at reasoning, but their ability can be '\n",
            "           'substantially increased by making them produce step-by-step '\n",
            "           'reasoning, either by fine-tuning or few-shot prompting. However, '\n",
            "           'the context suggests that LLMs are not only good at few-shot '\n",
            "           'learning, but also have decent zero-shot reasoning abilities, as '\n",
            "           'demonstrated by the success of the Zero-shot-CoT method. This '\n",
            "           'method uses a single fixed trigger prompt, \"Let\\'s think step by '\n",
            "           'step\", which significantly improves the zero-shot reasoning '\n",
            "           'ability of LLMs across a variety of tasks requiring complex '\n",
            "           'multi-hop thinking. This suggests that LLMs have untapped and '\n",
            "           'understudied fundamental zero-shot capabilities, and further '\n",
            "           'exploration and analysis of these capabilities could lead to even '\n",
            "           'more impressive results.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"can you elaborate what it is prompting in this context?\"\n",
        "response = qa.invoke(query)\n",
        "pprint.pprint(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKy5O5dZDWhn",
        "outputId": "3c9e4fb3-cfcf-4b69-bcdb-1a830d91b80a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'can you elaborate what it is prompting in this context?',\n",
            " 'result': '\\n'\n",
            "           'In this context, prompting refers to the method of conditioning a '\n",
            "           'language model on a few examples or instructions to guide its '\n",
            "           'generation of answers for desired tasks. This method has become '\n",
            "           'popular in natural language processing and has been shown to '\n",
            "           'significantly improve the performance of large language models. '\n",
            "           'The context suggests that prompting can be used to enhance both '\n",
            "           'few-shot and zero-shot reasoning abilities of LLMs, making them '\n",
            "           'more adept at solving complex tasks.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book_path = \"/content/drive/MyDrive/books/Generative AI on AWS.pdf\""
      ],
      "metadata": {
        "id": "OitIYaMQHTce"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "_Cpe221EHTZO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(book_path)\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "Ki2pkUiOHTWZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUHa0pdIHTTY",
        "outputId": "f196a366-cdfa-420f-9578-91ee0ef093ea"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='DATA“I am very excited about \\nthis book —it has a great \\nmix of all-important \\nbackground/theoretical \\ninfo and detailed, \\nhands-on code, scripts, \\nand walk-throughs. I \\nenjoyed reading it, and  \\nI know that you will too!” \\n—Jeff Barr\\nVP and Chief Evangelist @ AWSGenerative AI on AWS\\nTwitter: @oreillymedia\\nlinkedin.com/company/oreilly-media\\nyoutube.com/oreillymedia Companies today are moving rapidly to integrate generative  \\nAI into their products and services. But there’s a great deal  \\nof hype (and misunderstanding) about the impact and \\npromise of this technology. With this book, Chris Fregly,  \\nAntje Barth, and Shelbee Eigenbrode from AWS help CTOs,  \\nML practitioners, application developers, business analysts, \\ndata engineers, and data scientists find practical ways to  \\nuse this exciting new technology.\\nYou’ll learn the generative AI project life cycle including  \\nuse case definition, model selection, model fine-tuning, \\nretrieval-augmented generation, reinforcement learning  \\nfrom human feedback, and model quantization, optimization, \\nand deployment. And you’ll explore different types of models \\nincluding large language models (LLMs) and multimodal \\nmodels such as Stable Diffusion for generating images and \\nFlamingo/IDEFICS for answering questions about images.\\n• Apply generative AI to your business use cases\\n• Determine which generative AI models are best  \\nsuited to your task \\n• Perform prompt engineering and in-context learning\\n• Fine-tune generative AI models on your datasets with  \\nlow-rank adaptation (LoRA)\\n• Align generative AI models to human values with \\nreinforcement learning from human feedback (RLHF)\\n• Augment your model with retrieval-augmented  \\ngeneration (RAG) \\n• Explore libraries such as LangChain and ReAct to  \\ndevelop agents and actions\\n• Build generative AI applications with Amazon Bedrock\\n9781098 15922157999US $79.99  CAN $99.99\\nISBN: 978-1-098-15922-1Chris Fregly is a Principal Solutions \\nArchitect for generative AI at \\nAmazon Web Services and coauthor  \\nof Data Science on AWS  (O’Reilly). \\nAntje Barth is Principal Developer \\nAdvocate for generative AI at Amazon \\nWeb Services and coauthor of  \\nData Science on AWS .\\nShelbee Eigenbrode is a Principal \\nSolutions Architect for generative AI \\nat Amazon Web Services. She holds  \\nover 35 patents across various \\ntechnology domains.', metadata={'source': '/content/drive/MyDrive/books/Generative AI on AWS.pdf', 'page': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rVVZ9GFNf4b",
        "outputId": "51eeee6a-cf0c-42cf-8531-ff589de46bc7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "309"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = docsearch.add_documents(pages)"
      ],
      "metadata": {
        "id": "C_Xc-eGhHTQX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how we can fine tune a foundation model?\"\n",
        "response = qa.invoke(query)\n",
        "pprint.pprint(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phaN63EBIvLz",
        "outputId": "7db51adc-4561-43ad-8d14-c01a8e2c9766"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'how we can fine tune a foundation model?',\n",
            " 'result': '\\n'\n",
            "           'Fine-tuning a foundation model involves adapting it to a specific '\n",
            "           'dataset or use case. This can be done by presenting a mix of '\n",
            "           \"instructions across many different tasks to maintain the model's \"\n",
            "           'ability to serve as a general-purpose generative model. It is '\n",
            "           'recommended to establish a set of baseline evaluation metrics and '\n",
            "           \"compare the model's output before and after fine-tuning to measure \"\n",
            "           'its effectiveness. This process can be less costly than '\n",
            "           'pretraining a model from scratch and can be done using techniques '\n",
            "           'such as full fine-tuning or parameter-efficient methods.'}\n"
          ]
        }
      ]
    }
  ]
}