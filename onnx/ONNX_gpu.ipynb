{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNpG5knDJoRIprER4mPskF/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b54887e2f4f345cfb3145a5ea209f84c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64aa2cfcaba6436590a1960c87e154aa",
              "IPY_MODEL_efd551b1677449149a23ee397ece4a90",
              "IPY_MODEL_6ecc99fd66174c14a004d9fb7442f3ed"
            ],
            "layout": "IPY_MODEL_92a934ff27234308b53ff81f6462cc9d"
          }
        },
        "64aa2cfcaba6436590a1960c87e154aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b74503ebbc9343ad9f6b4de65f0889f5",
            "placeholder": "​",
            "style": "IPY_MODEL_877c0edafaf848ad8307b1be20c076a0",
            "value": "config.json: 100%"
          }
        },
        "efd551b1677449149a23ee397ece4a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d5ef5c5ae4b478886b5adbfc02cd03c",
            "max": 443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2ddfceb6482541088b7e177214fbbf6f",
            "value": 443
          }
        },
        "6ecc99fd66174c14a004d9fb7442f3ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7ad67f0391c4a3ba4645debc5a27c3b",
            "placeholder": "​",
            "style": "IPY_MODEL_153b38a0f1fb41ddac81b352f9ae4bbe",
            "value": " 443/443 [00:00&lt;00:00, 35.6kB/s]"
          }
        },
        "92a934ff27234308b53ff81f6462cc9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b74503ebbc9343ad9f6b4de65f0889f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "877c0edafaf848ad8307b1be20c076a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d5ef5c5ae4b478886b5adbfc02cd03c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ddfceb6482541088b7e177214fbbf6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7ad67f0391c4a3ba4645debc5a27c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "153b38a0f1fb41ddac81b352f9ae4bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee2d4d2545e14c888bb5a3f166c06cee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3180c0c182df4d189d31af4b62b48304",
              "IPY_MODEL_e53c4aae0f4b40769f87b58d29906998",
              "IPY_MODEL_5e97ce95d8fb497d91f47681d121ebc1"
            ],
            "layout": "IPY_MODEL_916f4fd0b64e4488a901658c42a96d25"
          }
        },
        "3180c0c182df4d189d31af4b62b48304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45576352462e4d5f825e7d0c835d5a9f",
            "placeholder": "​",
            "style": "IPY_MODEL_28cb41d6ca9242ddacae2dc48edd63db",
            "value": "vocab.txt: 100%"
          }
        },
        "e53c4aae0f4b40769f87b58d29906998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21ae0d4b9ce64f8fa4603d728105441f",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4aeeb8c9f497458499e32e17f3ce81d3",
            "value": 231508
          }
        },
        "5e97ce95d8fb497d91f47681d121ebc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c59b929a6854ffd93e39c614e216125",
            "placeholder": "​",
            "style": "IPY_MODEL_a3396c746b4147b5bfc121dd987aa0a4",
            "value": " 232k/232k [00:00&lt;00:00, 491kB/s]"
          }
        },
        "916f4fd0b64e4488a901658c42a96d25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45576352462e4d5f825e7d0c835d5a9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28cb41d6ca9242ddacae2dc48edd63db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21ae0d4b9ce64f8fa4603d728105441f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aeeb8c9f497458499e32e17f3ce81d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c59b929a6854ffd93e39c614e216125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3396c746b4147b5bfc121dd987aa0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d535f09eb70840839a57e3e07ca8aeb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1efd7942ada4e76b6b607fe75023e9e",
              "IPY_MODEL_c1f9f30653554ec8a4e18f4521c7db3b",
              "IPY_MODEL_357f4c71c0944fe89b44d5cd73bd7491"
            ],
            "layout": "IPY_MODEL_ba33ad4f010e48e2981d1505a6994e40"
          }
        },
        "e1efd7942ada4e76b6b607fe75023e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a2b68ac93d54e1594aeb293fd49bf9b",
            "placeholder": "​",
            "style": "IPY_MODEL_5dd5be345a78465fa1bc94708146df41",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c1f9f30653554ec8a4e18f4521c7db3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8f90ce3476b4efaa8d326fa049ea3ac",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eafc212465364f659800256db6239f10",
            "value": 48
          }
        },
        "357f4c71c0944fe89b44d5cd73bd7491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a3d8c05064a4197b9bbec46aa18e5e1",
            "placeholder": "​",
            "style": "IPY_MODEL_310eaa9bb9854881bc50f020b1bae970",
            "value": " 48.0/48.0 [00:00&lt;00:00, 3.86kB/s]"
          }
        },
        "ba33ad4f010e48e2981d1505a6994e40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a2b68ac93d54e1594aeb293fd49bf9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd5be345a78465fa1bc94708146df41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8f90ce3476b4efaa8d326fa049ea3ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eafc212465364f659800256db6239f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a3d8c05064a4197b9bbec46aa18e5e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "310eaa9bb9854881bc50f020b1bae970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1596ad1731874cc5a8b579abba46cea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f6b515c4411426089d4664776f6b109",
              "IPY_MODEL_44d724815c03480caec53c06e325e515",
              "IPY_MODEL_a3d2d074aa6c4695900629466b8a0aa6"
            ],
            "layout": "IPY_MODEL_ba09528f9d5445b2a5df2a2df28c4e2c"
          }
        },
        "2f6b515c4411426089d4664776f6b109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_262f9018b4774c5995489e3dda2085fb",
            "placeholder": "​",
            "style": "IPY_MODEL_60394e4e92954cb9815f0dac4cf51a5b",
            "value": "model.safetensors: 100%"
          }
        },
        "44d724815c03480caec53c06e325e515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30a7df5a1add427582444fcbd975b6dd",
            "max": 1340622760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22e09ef69bc341b7a11d07b71aebe660",
            "value": 1340622760
          }
        },
        "a3d2d074aa6c4695900629466b8a0aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_127db77ae3434921af221a549b6fbb0e",
            "placeholder": "​",
            "style": "IPY_MODEL_53b80948d32b47b6bf5d4cd7150f8342",
            "value": " 1.34G/1.34G [00:03&lt;00:00, 369MB/s]"
          }
        },
        "ba09528f9d5445b2a5df2a2df28c4e2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "262f9018b4774c5995489e3dda2085fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60394e4e92954cb9815f0dac4cf51a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30a7df5a1add427582444fcbd975b6dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e09ef69bc341b7a11d07b71aebe660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "127db77ae3434921af221a549b6fbb0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53b80948d32b47b6bf5d4cd7150f8342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/onnx/ONNX_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX\n",
        "\n",
        "https://onnxruntime.ai/docs/get-started/with-python.html\n",
        "\n",
        "https://github.com/onnx/onnx/blob/main/docs/Versioning.md"
      ],
      "metadata": {
        "id": "fEwBXpzgjHPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohO-VW_Si8ip",
        "outputId": "cf772eba-c102-43c6-a017-2a64ea06fb52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh97TAXGjWoG",
        "outputId": "69d81aca-936a-4010-9887-8b82e52b2869"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
            "Collecting onnxruntime-gpu\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/onnxruntime-gpu/1.18.1/onnxruntime_gpu-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (201.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.5/201.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/coloredlogs/15.0.1/coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.3.25)\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.12.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/humanfriendly/10/humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.18.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install onnx==1.14.1 transformers==4.33.1 psutil pandas py-cpuinfo py3nvml coloredlogs wget netron sympy protobuf -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URjYlcc4llYV",
        "outputId": "8b63ee81-acb1-40fc-841e-378e4b6a7d68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import onnx\n",
        "import onnxruntime\n",
        "import transformers\n",
        "print(\"pytorch:\", torch.__version__)\n",
        "print(\"onnxruntime:\", onnxruntime.__version__)\n",
        "print(\"onnx:\", onnx.__version__)\n",
        "print(\"transformers:\", transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MykQoa6Dka5d",
        "outputId": "3982a411-1a43-4b7f-c5a8-688c2b2d1e2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch: 2.3.0+cu121\n",
            "onnxruntime: 1.18.1\n",
            "onnx: 1.14.1\n",
            "transformers: 4.33.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cache_dir = \"./squad\"\n",
        "if not os.path.exists(cache_dir):\n",
        "    os.makedirs(cache_dir)\n",
        "\n",
        "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
        "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
        "if not os.path.exists(predict_file):\n",
        "    import wget\n",
        "    print(\"Start downloading predict file.\")\n",
        "    wget.download(predict_file_url, predict_file)\n",
        "    print(\"Predict file downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS_OiTfslzsN",
        "outputId": "fc55d51c-8f1f-431c-911d-0a5f498ca841"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start downloading predict file.\n",
            "Predict file downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whether allow overwriting existing ONNX model and download the latest script from GitHub\n",
        "enable_overwrite = True\n",
        "\n",
        "# Total samples to inference, so that we can get average latency\n",
        "total_samples = 1000\n",
        "\n",
        "# ONNX opset version\n",
        "opset_version=14"
      ],
      "metadata": {
        "id": "XMY7gzOHl41W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuned model from https://huggingface.co/models?search=squad\n",
        "model_name_or_path = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "max_seq_length = 128\n",
        "doc_stride = 128\n",
        "max_query_length = 64"
      ],
      "metadata": {
        "id": "KqReHcqrl6sk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code is adapted from HuggingFace transformers\n",
        "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
        "\n",
        "from transformers import (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
        "\n",
        "# Load pretrained model and tokenizer\n",
        "config_class, model_class, tokenizer_class = (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
        "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
        "model = model_class.from_pretrained(model_name_or_path,\n",
        "                                    from_tf=False,\n",
        "                                    config=config,\n",
        "                                    cache_dir=cache_dir)\n",
        "# load some examples\n",
        "from transformers.data.processors.squad import SquadV1Processor\n",
        "\n",
        "processor = SquadV1Processor()\n",
        "examples = processor.get_dev_examples(None, filename=predict_file)\n",
        "\n",
        "from transformers import squad_convert_examples_to_features\n",
        "features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples[:total_samples], # convert enough examples for this notebook\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=max_seq_length,\n",
        "            doc_stride=doc_stride,\n",
        "            max_query_length=max_query_length,\n",
        "            is_training=False,\n",
        "            return_dataset='pt'\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408,
          "referenced_widgets": [
            "b54887e2f4f345cfb3145a5ea209f84c",
            "64aa2cfcaba6436590a1960c87e154aa",
            "efd551b1677449149a23ee397ece4a90",
            "6ecc99fd66174c14a004d9fb7442f3ed",
            "92a934ff27234308b53ff81f6462cc9d",
            "b74503ebbc9343ad9f6b4de65f0889f5",
            "877c0edafaf848ad8307b1be20c076a0",
            "7d5ef5c5ae4b478886b5adbfc02cd03c",
            "2ddfceb6482541088b7e177214fbbf6f",
            "e7ad67f0391c4a3ba4645debc5a27c3b",
            "153b38a0f1fb41ddac81b352f9ae4bbe",
            "ee2d4d2545e14c888bb5a3f166c06cee",
            "3180c0c182df4d189d31af4b62b48304",
            "e53c4aae0f4b40769f87b58d29906998",
            "5e97ce95d8fb497d91f47681d121ebc1",
            "916f4fd0b64e4488a901658c42a96d25",
            "45576352462e4d5f825e7d0c835d5a9f",
            "28cb41d6ca9242ddacae2dc48edd63db",
            "21ae0d4b9ce64f8fa4603d728105441f",
            "4aeeb8c9f497458499e32e17f3ce81d3",
            "3c59b929a6854ffd93e39c614e216125",
            "a3396c746b4147b5bfc121dd987aa0a4",
            "d535f09eb70840839a57e3e07ca8aeb1",
            "e1efd7942ada4e76b6b607fe75023e9e",
            "c1f9f30653554ec8a4e18f4521c7db3b",
            "357f4c71c0944fe89b44d5cd73bd7491",
            "ba33ad4f010e48e2981d1505a6994e40",
            "9a2b68ac93d54e1594aeb293fd49bf9b",
            "5dd5be345a78465fa1bc94708146df41",
            "f8f90ce3476b4efaa8d326fa049ea3ac",
            "eafc212465364f659800256db6239f10",
            "4a3d8c05064a4197b9bbec46aa18e5e1",
            "310eaa9bb9854881bc50f020b1bae970",
            "1596ad1731874cc5a8b579abba46cea7",
            "2f6b515c4411426089d4664776f6b109",
            "44d724815c03480caec53c06e325e515",
            "a3d2d074aa6c4695900629466b8a0aa6",
            "ba09528f9d5445b2a5df2a2df28c4e2c",
            "262f9018b4774c5995489e3dda2085fb",
            "60394e4e92954cb9815f0dac4cf51a5b",
            "30a7df5a1add427582444fcbd975b6dd",
            "22e09ef69bc341b7a11d07b71aebe660",
            "127db77ae3434921af221a549b6fbb0e",
            "53b80948d32b47b6bf5d4cd7150f8342"
          ]
        },
        "id": "8mf4Oq4Ql9c0",
        "outputId": "918a8c1e-4e54-43aa-8462-00bb226e6a39"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b54887e2f4f345cfb3145a5ea209f84c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee2d4d2545e14c888bb5a3f166c06cee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d535f09eb70840839a57e3e07ca8aeb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1596ad1731874cc5a8b579abba46cea7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 48/48 [00:03<00:00, 12.21it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "convert squad examples to features: 100%|██████████| 1000/1000 [00:08<00:00, 121.80it/s]\n",
            "add example index and unique id: 100%|██████████| 1000/1000 [00:00<00:00, 550939.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = os.path.join(\".\", \"onnx_models\")\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "export_model_path = os.path.join(output_dir, 'bert-base-cased-squad_opset{}.onnx'.format(opset_version))\n",
        "\n",
        "import torch\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Get the first example data to run the model and export it to ONNX\n",
        "data = dataset[0]\n",
        "inputs = {\n",
        "    'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
        "    'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
        "    'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
        "}\n",
        "\n",
        "# Set model to inference mode, which is required before exporting the model because some operators behave differently in\n",
        "# inference and training mode.\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "if enable_overwrite or not os.path.exists(export_model_path):\n",
        "    with torch.no_grad():\n",
        "        symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
        "        torch.onnx.export(model,                                            # model being run\n",
        "                          args=tuple(inputs.values()),                      # model input (or a tuple for multiple inputs)\n",
        "                          f=export_model_path,                              # where to save the model (can be a file or file-like object)\n",
        "                          opset_version=opset_version,                      # the ONNX version to export the model to\n",
        "                          do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
        "                          input_names=['input_ids',                         # the model's input names\n",
        "                                       'input_mask',\n",
        "                                       'segment_ids'],\n",
        "                          output_names=['start', 'end'],                    # the model's output names\n",
        "                          dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
        "                                        'input_mask' : symbolic_names,\n",
        "                                        'segment_ids' : symbolic_names,\n",
        "                                        'start' : symbolic_names,\n",
        "                                        'end' : symbolic_names})\n",
        "        print(\"Model exported at \", export_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAMT8i2bmLu1",
        "outputId": "58aa2694-0759-4e81-ff09-ffcfcafa273a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Model exported at  ./onnx_models/bert-base-cased-squad_opset14.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Measure the latency. It is not accurate using Jupyter Notebook, it is recommended to use standalone python script.\n",
        "latency = []\n",
        "with torch.no_grad():\n",
        "    for i in range(total_samples):\n",
        "        data = dataset[i]\n",
        "        inputs = {\n",
        "            'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
        "            'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
        "            'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
        "        }\n",
        "        start = time.time()\n",
        "        outputs = model(**inputs)\n",
        "        latency.append(time.time() - start)\n",
        "print(\"PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VMe-tVZmseV",
        "outputId": "46b604c9-a722-445a-e76f-fe2ffb9cedc7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch cuda Inference time = 19.85 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnxruntime.get_available_providers()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voQ-avLWqa0S",
        "outputId": "bf1ddd1c-aab9-4ed2-c83f-6c8d383d8982"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import onnxruntime\n",
        "import numpy\n",
        "\n",
        "assert 'CUDAExecutionProvider' in onnxruntime.get_available_providers()\n",
        "device_name = 'gpu'\n",
        "\n",
        "sess_options = onnxruntime.SessionOptions()\n",
        "\n",
        "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
        "# Note that this will increase session creation time so enable it for debugging only.\n",
        "sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\n",
        "\n",
        "# Please change the value according to best setting in Performance Test Tool result.\n",
        "sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
        "\n",
        "session = onnxruntime.InferenceSession(export_model_path, sess_options, providers=[\"CUDAExecutionProvider\"])# CPUExecutionProvider\n",
        "\n",
        "latency = []\n",
        "for i in range(total_samples):\n",
        "    data = dataset[i]\n",
        "    ort_inputs = {\n",
        "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
        "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
        "        'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
        "    }\n",
        "    start = time.time()\n",
        "    ort_outputs = session.run(None, ort_inputs)\n",
        "    latency.append(time.time() - start)\n",
        "\n",
        "print(\"OnnxRuntime {} Inference time = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBrYULehmxvT",
        "outputId": "fbfd696a-0b54-4f38-c282-80eb307f3e86"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OnnxRuntime gpu Inference time = 237.35 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"***** Verifying correctness *****\")\n",
        "for i in range(2):\n",
        "    print('PyTorch and ONNX Runtime output {} are close:'.format(i), numpy.allclose(ort_outputs[i], outputs[i].cpu(), rtol=1e-02, atol=1e-02))\n",
        "    diff = ort_outputs[i] - outputs[i].cpu().numpy()\n",
        "    max_diff = numpy.max(numpy.abs(diff))\n",
        "    avg_diff = numpy.average(numpy.abs(diff))\n",
        "    print(f'maximum_diff={max_diff} average_diff={avg_diff}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DuoRINOMQFC",
        "outputId": "e95dcfa2-139a-4eda-ec72-ab060821b0f2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Verifying correctness *****\n",
            "PyTorch and ONNX Runtime output 0 are close: True\n",
            "maximum_diff=1.150369644165039e-05 average_diff=2.6873312890529633e-06\n",
            "PyTorch and ONNX Runtime output 1 are close: True\n",
            "maximum_diff=1.0251998901367188e-05 average_diff=2.399727236479521e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "latency = []\n",
        "lengths = []\n",
        "for i in range(total_samples):\n",
        "    data = dataset[i]\n",
        "    # Instead of using fixed length (128), we can use actual sequence length (less than 128), which helps to get better performance.\n",
        "    actual_sequence_length = sum(data[1].numpy())\n",
        "    lengths.append(actual_sequence_length)\n",
        "    opt_inputs = {\n",
        "        'input_ids':  data[0].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length),\n",
        "        'input_mask': data[1].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length),\n",
        "        'segment_ids': data[2].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length)\n",
        "    }\n",
        "    start = time.time()\n",
        "    opt_outputs = session.run(None, opt_inputs)\n",
        "    latency.append(time.time() - start)\n",
        "print(\"Average length\", statistics.mean(lengths))\n",
        "print(\"OnnxRuntime {} Inference time with actual sequence length = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruZdITjfMYIY",
        "outputId": "d8894d61-3d8b-4225-8f96-3693a6f8d423"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average length 94\n",
            "OnnxRuntime gpu Inference time with actual sequence length = 185.83 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "optimized_fp32_model_path = './onnx/bert-base-cased-squad_opt_{}_fp32.onnx'.format('gpu' if use_gpu else 'cpu')\n",
        "\n",
        "!{sys.executable} -m onnxruntime.transformers.optimizer --input $export_model_path --output $optimized_fp32_model_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zoz7Hk8KPdqw",
        "outputId": "a04e617c-f587-4bd2-bec0-32f9852de8cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               apply: Fused LayerNormalization: 49\n",
            "               apply: Fused Gelu: 24\n",
            "               apply: Fused SkipLayerNormalization: 48\n",
            "               apply: Fused Attention: 24\n",
            "         prune_graph: Removed 5 nodes\n",
            "               apply: Fused EmbedLayerNormalization(with mask): 1\n",
            "         prune_graph: Removed 10 nodes\n",
            "               apply: Fused BiasGelu: 24\n",
            "               apply: Fused SkipLayerNormalization(add bias): 48\n",
            "            optimize: opset version: 14\n",
            "get_operator_statistics: Operators:[('MatMul', 73), ('SkipLayerNormalization', 48), ('Attention', 24), ('BiasGelu', 24), ('Cast', 3), ('Squeeze', 2), ('Add', 1), ('EmbedLayerNormalization', 1), ('Split', 1)]\n",
            "get_fused_operator_statistics: Optimized operators: {'EmbedLayerNormalization': 1, 'Attention': 24, 'MultiHeadAttention': 0, 'Gelu': 0, 'FastGelu': 0, 'BiasGelu': 24, 'GemmFastGelu': 0, 'LayerNormalization': 0, 'SimplifiedLayerNormalization': 0, 'SkipLayerNormalization': 48, 'SkipSimplifiedLayerNormalization': 0, 'RotaryEmbedding': 0, 'QOrderedAttention': 0, 'QOrderedGelu': 0, 'QOrderedLayerNormalization': 0, 'QOrderedMatMul': 0}\n",
            "                main: The model has been fully optimized.\n",
            "  save_model_to_file: Sort graphs in topological order\n",
            "  save_model_to_file: Model saved to ./onnx/bert-base-cased-squad_opt_gpu_fp32.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import netron\n",
        "\n",
        "# change it to True if want to view the optimized model in browser\n",
        "enable_netron = False\n",
        "if enable_netron:\n",
        "    # If you encounter error \"access a socket in a way forbidden by its access permissions\", install Netron as standalone application instead.\n",
        "    netron.start(optimized_fp32_model_path)"
      ],
      "metadata": {
        "id": "ibdHPk7VPq8f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPU_OPTION = '--use_gpu --use_io_binding' if use_gpu else ''\n",
        "\n",
        "!{sys.executable} -m onnxruntime.transformers.bert_perf_test --model $optimized_fp32_model_path --batch_size 1 --sequence_length 128 --samples 1000 --test_times 1 $GPU_OPTION"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-DHze4MPweW",
        "outputId": "e2934e70-dfc7-43ab-e246-77ea61530209"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test setting TestSetting(batch_size=1, sequence_length=128, test_cases=1000, test_times=1, use_gpu=True, use_io_binding=True, provider=None, intra_op_num_threads=None, seed=3, verbose=False, log_severity=2, average_sequence_length=128, random_sequence_length=False)\n",
            "Generating 1000 samples for batch_size=1 sequence_length=128\n",
            "\u001b[1;31m2024-07-04 14:56:41.194953983 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:56:41.195011266 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-2:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:56:44.417968625 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:56:44.418004813 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-3:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:56:47.623814116 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:56:47.623855813 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-4:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:56:50.833367459 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:56:50.833405716 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-5:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:56:54.096420425 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:56:54.096459669 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-6:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:56:57.311906267 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:56:57.311944879 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-7:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:57:00.507262094 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:57:00.507299269 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-8:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:57:03.748461212 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:57:03.748500443 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-9:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:57:07.007973109 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:57:07.008010979 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-10:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:57:10.213477992 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:57:10.213520759 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-11:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:57:13.417310209 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:57:13.417347712 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-12:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "\u001b[1;31m2024-07-04 14:57:16.632502152 [E:onnxruntime:Default, provider_bridge_ort.cc:1745 TryGetProviderInfo_CUDA] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1426 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.9: cannot open shared object file: No such file or directory\n",
            "\u001b[m\n",
            "\u001b[0;93m2024-07-04 14:57:16.632539850 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:895 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto ensure all dependencies are met.\u001b[m\n",
            "Process Process-13:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 248, in run_one_test\n",
            "    session = create_session(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime/transformers/bert_perf_test.py\", line 141, in create_session\n",
            "    assert \"CUDAExecutionProvider\" in session.get_providers()\n",
            "AssertionError\n",
            "Test summary is saved to onnx/perf_results_GPU_B1_S128_20240704-145718.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m onnxruntime.transformers.machine_info --silent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJew4uDmQCa-",
        "outputId": "6345646d-fe45-4647-9d7c-6ab476fcd388"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-04 14:58:49.543511: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-04 14:58:49.543560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-04 14:58:49.545041: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-04 14:58:50.624728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-07-04 14:58:51,747 - numexpr.utils - INFO: NumExpr defaulting to 12 threads.\n",
            "{\n",
            "  \"gpu\": {\n",
            "    \"driver_version\": \"535.104.05\",\n",
            "    \"devices\": [\n",
            "      {\n",
            "        \"memory_total\": 24152899584,\n",
            "        \"memory_available\": 22178299904,\n",
            "        \"name\": \"NVIDIA L4\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"cpu\": {\n",
            "    \"brand\": \"Intel(R) Xeon(R) CPU @ 2.20GHz\",\n",
            "    \"cores\": 6,\n",
            "    \"logical_cores\": 12,\n",
            "    \"hz\": \"2200164000,0\",\n",
            "    \"l2_cache\": 6291456,\n",
            "    \"flags\": \"3dnowprefetch,abm,adx,aes,apic,arat,arch_capabilities,avx,avx2,avx512_vnni,avx512bw,avx512cd,avx512dq,avx512f,avx512vl,avx512vnni,bmi1,bmi2,clflush,clflushopt,clwb,cmov,constant_tsc,cpuid,cx16,cx8,de,erms,f16c,fma,fpu,fsgsbase,fxsr,hle,ht,hypervisor,ibpb,ibrs,ibrs_enhanced,invpcid,invpcid_single,lahf_lm,lm,mca,mce,md_clear,mmx,movbe,mpx,msr,mtrr,nonstop_tsc,nopl,nx,osxsave,pae,pat,pcid,pclmulqdq,pdpe1gb,pge,pni,popcnt,pse,pse36,rdrand,rdrnd,rdseed,rdtscp,rep_good,rtm,sep,smap,smep,ss,ssbd,sse,sse2,sse4_1,sse4_2,ssse3,stibp,syscall,tsc,tsc_adjust,tsc_known_freq,vme,x2apic,xgetbv1,xsave,xsavec,xsaveopt,xsaves,xtopology\",\n",
            "    \"processor\": \"x86_64\"\n",
            "  },\n",
            "  \"memory\": {\n",
            "    \"total\": 56866914304,\n",
            "    \"available\": 49149288448\n",
            "  },\n",
            "  \"os\": \"Linux-6.1.85+-x86_64-with-glibc2.35\",\n",
            "  \"python\": \"3.10.12.final.0 (64 bit)\",\n",
            "  \"packages\": {\n",
            "    \"flatbuffers\": \"24.3.25\",\n",
            "    \"numpy\": \"1.25.2\",\n",
            "    \"onnx\": \"1.14.1\",\n",
            "    \"onnxruntime-gpu\": \"1.18.1\",\n",
            "    \"protobuf\": \"3.20.3\",\n",
            "    \"sympy\": \"1.12.1\",\n",
            "    \"tensorflow\": \"2.15.0\",\n",
            "    \"torch\": \"2.3.0+cu121\",\n",
            "    \"transformers\": \"4.33.1\"\n",
            "  },\n",
            "  \"onnxruntime\": {\n",
            "    \"version\": \"1.18.1\",\n",
            "    \"support_gpu\": true\n",
            "  },\n",
            "  \"pytorch\": {\n",
            "    \"version\": \"2.3.0+cu121\",\n",
            "    \"support_gpu\": true,\n",
            "    \"cuda\": \"12.1\"\n",
            "  },\n",
            "  \"tensorflow\": {\n",
            "    \"version\": \"2.15.0\",\n",
            "    \"git_version\": \"v2.15.0-0-g6887368d6d4\",\n",
            "    \"support_gpu\": true\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli download microsoft/Phi-3-mini-4k-instruct-onnx --include cuda/cuda-int4-rtn-block-32/* --local-dir .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvV5dUIcR7hM",
        "outputId": "85e84bcf-1368-454f-b96c-21e40ebc520b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rFetching 10 files:   0% 0/10 [00:00<?, ?it/s]Downloading 'cuda/cuda-int4-rtn-block-32/genai_config.json' to '.huggingface/download/cuda/cuda-int4-rtn-block-32/genai_config.json.58c6c0ff8940da053ab740431526fd1abffe496f.incomplete'\n",
            "Downloading 'cuda/cuda-int4-rtn-block-32/added_tokens.json' to '.huggingface/download/cuda/cuda-int4-rtn-block-32/added_tokens.json.178968dec606c790aa335e9142f6afec37288470.incomplete'\n",
            "Downloading 'cuda/cuda-int4-rtn-block-32/phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx.data' to '.huggingface/download/cuda/cuda-int4-rtn-block-32/phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx.data.b05e2aa951df379a6c155b7bec5c3cd66e04a351c80fc25a66d0616d180e50d5.incomplete'\n",
            "Downloading 'cuda/cuda-int4-rtn-block-32/special_tokens_map.json' to '.huggingface/download/cuda/cuda-int4-rtn-block-32/special_tokens_map.json.c6a944b4d49ce5d79030250ed6bdcbb1a65dfda1.incomplete'\n",
            "Downloading 'cuda/cuda-int4-rtn-block-32/config.json' to '.huggingface/download/cuda/cuda-int4-rtn-block-32/config.json.b9b031fadda61a035b2e8ceb4362cbf604002b21.incomplete'\n",
            "Downloading 'cuda/cuda-int4-rtn-block-32/tokenizer.json' to '.huggingface/download/cuda/cuda-int4-rtn-block-32/tokenizer.json.88ec145f4e7684c009bc6d55df24bb82c7d3c379.incomplete'\n",
            "Downloading 'cuda/cuda-int4-rtn-block-32/configuration_phi3.py' to '.huggingface/download/cuda/cuda-int4-rtn-block-32/configuration_phi3.py.780401034b66c92c6ce76ffc541493b6eac61627.incomplete'\n",
            "Downloading 'cuda/cuda-int4-rtn-block-32/phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx' to '.huggingface/download/cuda/cuda-int4-rtn-block-32/phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx.694e8a697352ea15cead99bfbc680f7237eacad08baddfc07d60c8d00f49cd43.incomplete'\n",
            "\n",
            "(…)cuda-int4-rtn-block-32/genai_config.json: 100% 1.74k/1.74k [00:00<00:00, 12.8MB/s]\n",
            "Download complete. Moving file to cuda/cuda-int4-rtn-block-32/genai_config.json\n",
            "\n",
            "(…)cuda-int4-rtn-block-32/added_tokens.json: 100% 306/306 [00:00<00:00, 2.54MB/s]\n",
            "Download complete. Moving file to cuda/cuda-int4-rtn-block-32/added_tokens.json\n",
            "Fetching 10 files:  10% 1/10 [00:00<00:06,  1.31it/s]\n",
            "(…)nt4-rtn-block-32/special_tokens_map.json: 100% 599/599 [00:00<00:00, 5.62MB/s]\n",
            "Download complete. Moving file to cuda/cuda-int4-rtn-block-32/special_tokens_map.json\n",
            "\n",
            "cuda/cuda-int4-rtn-block-32/config.json: 100% 967/967 [00:00<00:00, 8.89MB/s]\n",
            "Download complete. Moving file to cuda/cuda-int4-rtn-block-32/config.json\n",
            "\n",
            "(…)da/cuda-int4-rtn-block-32/tokenizer.json:   0% 0.00/1.94M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "(…)-int4-rtn-block-32/configuration_phi3.py: 100% 11.2k/11.2k [00:00<00:00, 62.4MB/s]\n",
            "Download complete. Moving file to cuda/cuda-int4-rtn-block-32/configuration_phi3.py\n",
            "\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:   0% 0.00/2.29G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:   1% 31.5M/2.29G [00:00<00:08, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "(…)-4k-instruct-cuda-int4-rtn-block-32.onnx:   0% 0.00/223k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'cuda/cuda-int4-rtn-block-32/tokenizer.model' to '.huggingface/download/cuda/cuda-int4-rtn-block-32/tokenizer.model.9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347.incomplete'\n",
            "(…)-4k-instruct-cuda-int4-rtn-block-32.onnx: 100% 223k/223k [00:00<00:00, 26.8MB/s]\n",
            "Download complete. Moving file to cuda/cuda-int4-rtn-block-32/phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx\n",
            "Fetching 10 files:  50% 5/10 [00:00<00:00,  6.14it/s]Downloading 'cuda/cuda-int4-rtn-block-32/tokenizer_config.json' to '.huggingface/download/cuda/cuda-int4-rtn-block-32/tokenizer_config.json.67aa82cddb4d66391ddf31ff99f059239bd2d1e7.incomplete'\n",
            "\n",
            "(…)da/cuda-int4-rtn-block-32/tokenizer.json: 100% 1.94M/1.94M [00:00<00:00, 7.97MB/s]\n",
            "Download complete. Moving file to cuda/cuda-int4-rtn-block-32/tokenizer.json\n",
            "\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:   3% 73.4M/2.29G [00:00<00:07, 312MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:   5% 115M/2.29G [00:00<00:06, 336MB/s] \u001b[A\u001b[A\n",
            "(…)-int4-rtn-block-32/tokenizer_config.json: 100% 3.44k/3.44k [00:00<00:00, 25.1MB/s]\n",
            "Download complete. Moving file to cuda/cuda-int4-rtn-block-32/tokenizer_config.json\n",
            "\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 43.4MB/s]\n",
            "Download complete. Moving file to cuda/cuda-int4-rtn-block-32/tokenizer.model\n",
            "\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:   7% 157M/2.29G [00:00<00:06, 341MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:   9% 199M/2.29G [00:00<00:05, 354MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  11% 241M/2.29G [00:00<00:05, 354MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  12% 283M/2.29G [00:00<00:06, 320MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  14% 325M/2.29G [00:01<00:06, 314MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  16% 367M/2.29G [00:01<00:06, 313MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  18% 409M/2.29G [00:01<00:05, 325MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  20% 451M/2.29G [00:01<00:05, 326MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  22% 493M/2.29G [00:01<00:05, 320MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  23% 535M/2.29G [00:01<00:05, 324MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  25% 577M/2.29G [00:01<00:05, 296MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  27% 619M/2.29G [00:01<00:05, 292MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  28% 650M/2.29G [00:02<00:05, 292MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  30% 692M/2.29G [00:02<00:05, 310MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  32% 734M/2.29G [00:02<00:04, 320MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  34% 776M/2.29G [00:02<00:04, 335MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  36% 818M/2.29G [00:02<00:04, 342MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  38% 860M/2.29G [00:02<00:04, 352MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  39% 902M/2.29G [00:02<00:03, 350MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  41% 944M/2.29G [00:02<00:03, 344MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  43% 986M/2.29G [00:03<00:03, 333MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  45% 1.03G/2.29G [00:03<00:03, 318MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  47% 1.07G/2.29G [00:06<00:29, 41.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  48% 1.10G/2.29G [00:06<00:23, 51.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  49% 1.13G/2.29G [00:06<00:17, 65.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  51% 1.17G/2.29G [00:06<00:12, 88.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  53% 1.21G/2.29G [00:06<00:09, 109MB/s] \u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  54% 1.24G/2.29G [00:06<00:08, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  56% 1.28G/2.29G [00:06<00:06, 162MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  58% 1.32G/2.29G [00:07<00:04, 197MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  59% 1.35G/2.29G [00:07<00:04, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  60% 1.38G/2.29G [00:07<00:04, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  62% 1.43G/2.29G [00:07<00:03, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  64% 1.46G/2.29G [00:07<00:03, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  65% 1.49G/2.29G [00:07<00:03, 254MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  66% 1.52G/2.29G [00:07<00:02, 264MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  68% 1.56G/2.29G [00:07<00:02, 290MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  70% 1.59G/2.29G [00:08<00:02, 271MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  71% 1.63G/2.29G [00:08<00:02, 275MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  72% 1.66G/2.29G [00:08<00:02, 276MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  74% 1.69G/2.29G [00:08<00:02, 280MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  75% 1.72G/2.29G [00:08<00:02, 284MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  77% 1.76G/2.29G [00:08<00:01, 283MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  78% 1.79G/2.29G [00:08<00:01, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  80% 1.82G/2.29G [00:08<00:01, 278MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  81% 1.86G/2.29G [00:09<00:01, 286MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  82% 1.89G/2.29G [00:09<00:01, 281MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  84% 1.92G/2.29G [00:09<00:01, 271MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  86% 1.96G/2.29G [00:09<00:01, 309MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  87% 1.99G/2.29G [00:09<00:01, 288MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  89% 2.03G/2.29G [00:09<00:00, 262MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  90% 2.07G/2.29G [00:09<00:00, 259MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  91% 2.10G/2.29G [00:09<00:00, 252MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  93% 2.13G/2.29G [00:10<00:00, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  94% 2.16G/2.29G [00:10<00:01, 121MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  95% 2.18G/2.29G [00:10<00:00, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  96% 2.20G/2.29G [00:11<00:00, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data:  97% 2.23G/2.29G [00:11<00:00, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data: 100% 2.29G/2.29G [00:11<00:00, 202MB/s]\n",
            "Download complete. Moving file to cuda/cuda-int4-rtn-block-32/phi3-mini-4k-instruct-cuda-int4-rtn-block-32.onnx.data\n",
            "Fetching 10 files: 100% 10/10 [00:12<00:00,  1.22s/it]\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install numpy\n",
        "! pip install onnxruntime-genai-cuda --pre --index-url=https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3RlkLykSUM9",
        "outputId": "03310627-d720-4659-8abe-e81d3822dd07"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Looking in indexes: https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
            "Collecting onnxruntime-genai-cuda\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/onnxruntime-genai-cuda/0.3/onnxruntime_genai_cuda-0.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (200.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.0/200.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime-genai-cuda\n",
            "Successfully installed onnxruntime-genai-cuda-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime_genai as og"
      ],
      "metadata": {
        "id": "4fPWKZSOTao6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model = og.Model(\"cuda/cuda-int4-rtn-block-32\")"
      ],
      "metadata": {
        "id": "BKFjDlsNSWsE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = og.Tokenizer(model)\n",
        "tokenizer_stream = tokenizer.create_stream()\n",
        "\n",
        "search_options = {\"max_length\": 1024,\"temperature\":0.3}"
      ],
      "metadata": {
        "id": "ZaonTCchW70R"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = og.GeneratorParams(model)\n",
        "params.try_use_cuda_graph_with_max_batch_size(1)\n",
        "params.set_search_options(**search_options)\n",
        "\n",
        "prompt = \"<|user|>Who are you not allowed to marry in the UK?<|end|><|assistant|>\"\n",
        "input_tokens = tokenizer.encode(prompt)\n",
        "params.input_ids = input_tokens\n",
        "\n",
        "generator = og.Generator(model, params)"
      ],
      "metadata": {
        "id": "6h6-y2MfW7h6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while not generator.is_done():\n",
        "                generator.compute_logits()\n",
        "                generator.generate_next_token()\n",
        "\n",
        "                new_token = generator.get_next_tokens()[0]\n",
        "                print(tokenizer_stream.decode(new_token), end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3Vh0wLQXL4t",
        "outputId": "b861f024-fdac-4e30-ca38-425b0f2c3abc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In the United Kingdom, the law does not prohibit marriage between individuals based on their nationality or citizenship. As of my knowledge cutoff in 2023, there are no restrictions on marriage between a British citizen and a foreign national. However, it is important to note that while the law does not prevent such unions, there may be practical considerations regarding visa status and residency rights that could affect a foreign national'dependent' partner's ability to live and work in the UK.\n",
            "\n",
            "It is also worth mentioning that while the UK does not have laws that prevent interracial or interethnic marriages, there have been historical instances of discrimination and prejudice. However, these societal attitudes have significantly diminished over time, and the UK is now considered to be a diverse and inclusive society.\n",
            "\n",
            "If you are considering marriage in the UK, it is advisable to consult with legal experts or immigration advisors to understand the implications for both parties, especially regarding residency rights and family reunification."
          ]
        }
      ]
    }
  ]
}