{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1wgeQj_CNGcVVGyAlywZ9oPTiNrWguneF",
      "authorship_tag": "ABX9TyNCZBbZkifXtLdHIS3V7Sc4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac9909050cc1461892a1ef81ba1752b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22bb4ae3818344f394b6f65c1bf864b6",
              "IPY_MODEL_799cc18f55b0489ea500c882145b1fd5",
              "IPY_MODEL_c122db4c600a4dfda70f4567768904b0"
            ],
            "layout": "IPY_MODEL_aacfe1f633e24cb0993e448ba8f8b3f1"
          }
        },
        "22bb4ae3818344f394b6f65c1bf864b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2bf5d06cea94e7a86b48415570f2b9b",
            "placeholder": "​",
            "style": "IPY_MODEL_02e5c59bd8024265a04f2553b2b9eb7f",
            "value": "Fetching 75 files: 100%"
          }
        },
        "799cc18f55b0489ea500c882145b1fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25896f6e844d4393bb73c5a6704426f6",
            "max": 75,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5512e46dac7b48eeb9030d138b71db99",
            "value": 75
          }
        },
        "c122db4c600a4dfda70f4567768904b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04748f800cf140bab338ce47c4040e56",
            "placeholder": "​",
            "style": "IPY_MODEL_d6916d13f85749ba91d4587b6a0678f8",
            "value": " 75/75 [00:00&lt;00:00, 150.69it/s]"
          }
        },
        "aacfe1f633e24cb0993e448ba8f8b3f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2bf5d06cea94e7a86b48415570f2b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02e5c59bd8024265a04f2553b2b9eb7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25896f6e844d4393bb73c5a6704426f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5512e46dac7b48eeb9030d138b71db99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04748f800cf140bab338ce47c4040e56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6916d13f85749ba91d4587b6a0678f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/onnx/Phi3__ONNX_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX\n",
        "\n",
        "ONNX Runtime is a cross-platform machine-learning model accelerator, with a flexible interface to integrate hardware-specific libraries. ONNX Runtime can be used with models from PyTorch, Tensorflow/Keras, TFLite, scikit-learn, and other frameworks.\n",
        "\n",
        "\n",
        "ONNX Runtime Inference powers machine learning models in key Microsoft products and services across Office, Azure, Bing, as well as dozens of community projects.\n",
        "\n",
        "Examples use cases for ONNX Runtime Inferencing include:\n",
        "\n",
        "Improve inference performance for a wide variety of ML models\n",
        "Run on different hardware and operating systems\n",
        "Train in Python but deploy into a C#/C++/Java app\n",
        "Train and perform inference with models created in different frameworks\n",
        "\n",
        "\n",
        "https://onnxruntime.ai/docs/get-started/with-python.html\n",
        "\n",
        "https://github.com/onnx/onnx/blob/main/docs/Versioning.md\n",
        "\n",
        "# Run generative AI models with ONNX Runtime\n",
        "https://onnxruntime.ai/docs/genai/\n",
        "\n",
        "# ONNXRuntime-genai-cuda\n",
        "https://onnxruntime.ai/docs/genai/howto/install\n",
        "\n",
        "```pip install numpy\n",
        "pip install onnxruntime-genai-cuda --pre --index-url=https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
        "```\n",
        "\n",
        "# CudNN\n",
        "https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html\n",
        "\n",
        "https://github.com/Hardware-Alchemy/cuDNN-sample/tree/master\n",
        "\n",
        "# Phi3\n",
        "\n",
        "https://github.com/microsoft/Phi-3CookBook/tree/main?tab=readme-ov-file\n",
        "\n",
        "https://onnxruntime.ai/docs/genai/tutorials/phi3-python.html#run-with-nvidia-cuda\n",
        "\n",
        "\n",
        "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx"
      ],
      "metadata": {
        "id": "fEwBXpzgjHPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohO-VW_Si8ip",
        "outputId": "9e3d8c0c-3645-4249-9ea2-64615ce332ae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh97TAXGjWoG",
        "outputId": "43f40581-c8f3-482f-f1c6-4d1e2d3fb417"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
            "Collecting onnxruntime-gpu\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/onnxruntime-gpu/1.18.1/onnxruntime_gpu-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (201.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.5/201.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/coloredlogs/15.0.1/coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.3.25)\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.12.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/humanfriendly/10/humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.18.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install onnx==1.14.1 transformers==4.33.1 psutil pandas py-cpuinfo py3nvml coloredlogs wget netron sympy protobuf -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URjYlcc4llYV",
        "outputId": "d0e498c9-bbc8-4e4e-c54c-f25eabd002f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install onnxruntime-genai-cuda cuda 12 Support"
      ],
      "metadata": {
        "id": "GTCw8fYNaSZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install numpy\n",
        "! pip install onnxruntime-genai-cuda --pre --index-url=https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ4i5eX2ZuLN",
        "outputId": "c705629d-9213-4bb6-b871-0dafe4f5d31f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Looking in indexes: https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
            "Collecting onnxruntime-genai-cuda\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/onnxruntime-genai-cuda/0.3/onnxruntime_genai_cuda-0.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (200.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.0/200.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime-genai-cuda\n",
            "Successfully installed onnxruntime-genai-cuda-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Cudnn 9 with cuda support 12"
      ],
      "metadata": {
        "id": "r-8PBxAraI6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo apt-get -y install zlib1g cudnn9-cuda-12 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wohf35I9Z9PU",
        "outputId": "5b3a6d16-ca84-418f-f067-12c5d3c8c523"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "zlib1g is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "The following additional packages will be installed:\n",
            "  cudnn9-cuda-12-5 libcudnn9-cuda-12 libcudnn9-dev-cuda-12\n",
            "  libcudnn9-static-cuda-12\n",
            "The following NEW packages will be installed:\n",
            "  cudnn9-cuda-12 cudnn9-cuda-12-5 libcudnn9-cuda-12 libcudnn9-dev-cuda-12\n",
            "  libcudnn9-static-cuda-12\n",
            "0 upgraded, 5 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 762 MB of archives.\n",
            "After this operation, 1,886 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn9-cuda-12 9.2.0.82-1 [380 MB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn9-dev-cuda-12 9.2.0.82-1 [34.1 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcudnn9-static-cuda-12 9.2.0.82-1 [383 MB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cudnn9-cuda-12-5 9.2.0.82-1 [12.3 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cudnn9-cuda-12 9.2.0.82-1 [12.3 kB]\n",
            "Fetched 762 MB in 8s (97.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libcudnn9-cuda-12.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn9-cuda-12_9.2.0.82-1_amd64.deb ...\n",
            "Unpacking libcudnn9-cuda-12 (9.2.0.82-1) ...\n",
            "Selecting previously unselected package libcudnn9-dev-cuda-12.\n",
            "Preparing to unpack .../libcudnn9-dev-cuda-12_9.2.0.82-1_amd64.deb ...\n",
            "Unpacking libcudnn9-dev-cuda-12 (9.2.0.82-1) ...\n",
            "Selecting previously unselected package libcudnn9-static-cuda-12.\n",
            "Preparing to unpack .../libcudnn9-static-cuda-12_9.2.0.82-1_amd64.deb ...\n",
            "Unpacking libcudnn9-static-cuda-12 (9.2.0.82-1) ...\n",
            "Selecting previously unselected package cudnn9-cuda-12-5.\n",
            "Preparing to unpack .../cudnn9-cuda-12-5_9.2.0.82-1_amd64.deb ...\n",
            "Unpacking cudnn9-cuda-12-5 (9.2.0.82-1) ...\n",
            "Selecting previously unselected package cudnn9-cuda-12.\n",
            "Preparing to unpack .../cudnn9-cuda-12_9.2.0.82-1_amd64.deb ...\n",
            "Unpacking cudnn9-cuda-12 (9.2.0.82-1) ...\n",
            "Setting up libcudnn9-cuda-12 (9.2.0.82-1) ...\n",
            "Setting up libcudnn9-dev-cuda-12 (9.2.0.82-1) ...\n",
            "update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v9.h to provide /usr/include/cudnn.h (libcudnn) in manual mode\n",
            "Setting up libcudnn9-static-cuda-12 (9.2.0.82-1) ...\n",
            "Setting up cudnn9-cuda-12-5 (9.2.0.82-1) ...\n",
            "Setting up cudnn9-cuda-12 (9.2.0.82-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import onnx\n",
        "import onnxruntime\n",
        "import transformers\n",
        "import sys\n",
        "print(\"pytorch:\", torch.__version__)\n",
        "print(\"onnxruntime:\", onnxruntime.__version__)\n",
        "print(\"onnx:\", onnx.__version__)\n",
        "print(\"transformers:\", transformers.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MykQoa6Dka5d",
        "outputId": "97358a25-0ee1-4283-ecc5-2c5431033b25"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch: 2.3.0+cu121\n",
            "onnxruntime: 1.18.1\n",
            "onnx: 1.14.1\n",
            "transformers: 4.33.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m onnxruntime.transformers.machine_info --silent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJew4uDmQCa-",
        "outputId": "1dded93b-3f32-4f75-b282-06781ac15a7a"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-04 21:04:09.891116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-04 21:04:09.891169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-04 21:04:09.892659: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-04 21:04:11.082221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-07-04 21:04:12,542 - numexpr.utils - INFO: NumExpr defaulting to 12 threads.\n",
            "{\n",
            "  \"gpu\": {\n",
            "    \"driver_version\": \"535.104.05\",\n",
            "    \"devices\": [\n",
            "      {\n",
            "        \"memory_total\": 24152899584,\n",
            "        \"memory_available\": 11734482944,\n",
            "        \"name\": \"NVIDIA L4\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"cpu\": {\n",
            "    \"brand\": \"Intel(R) Xeon(R) CPU @ 2.20GHz\",\n",
            "    \"cores\": 6,\n",
            "    \"logical_cores\": 12,\n",
            "    \"hz\": \"2200200000,0\",\n",
            "    \"l2_cache\": 6291456,\n",
            "    \"flags\": \"3dnowprefetch,abm,adx,aes,apic,arat,arch_capabilities,avx,avx2,avx512_vnni,avx512bw,avx512cd,avx512dq,avx512f,avx512vl,avx512vnni,bmi1,bmi2,clflush,clflushopt,clwb,cmov,constant_tsc,cpuid,cx16,cx8,de,erms,f16c,fma,fpu,fsgsbase,fxsr,hle,ht,hypervisor,ibpb,ibrs,ibrs_enhanced,invpcid,invpcid_single,lahf_lm,lm,mca,mce,md_clear,mmx,movbe,mpx,msr,mtrr,nonstop_tsc,nopl,nx,osxsave,pae,pat,pcid,pclmulqdq,pdpe1gb,pge,pni,popcnt,pse,pse36,rdrand,rdrnd,rdseed,rdtscp,rep_good,rtm,sep,smap,smep,ss,ssbd,sse,sse2,sse4_1,sse4_2,ssse3,stibp,syscall,tsc,tsc_adjust,tsc_known_freq,vme,x2apic,xgetbv1,xsave,xsavec,xsaveopt,xsaves,xtopology\",\n",
            "    \"processor\": \"x86_64\"\n",
            "  },\n",
            "  \"memory\": {\n",
            "    \"total\": 56866914304,\n",
            "    \"available\": 53493493760\n",
            "  },\n",
            "  \"os\": \"Linux-6.1.85+-x86_64-with-glibc2.35\",\n",
            "  \"python\": \"3.10.12.final.0 (64 bit)\",\n",
            "  \"packages\": {\n",
            "    \"flatbuffers\": \"24.3.25\",\n",
            "    \"numpy\": \"1.25.2\",\n",
            "    \"onnx\": \"1.14.1\",\n",
            "    \"onnxruntime-gpu\": \"1.18.1\",\n",
            "    \"protobuf\": \"3.20.3\",\n",
            "    \"sympy\": \"1.12.1\",\n",
            "    \"tensorflow\": \"2.15.0\",\n",
            "    \"torch\": \"2.3.0+cu121\",\n",
            "    \"transformers\": \"4.42.3\"\n",
            "  },\n",
            "  \"onnxruntime\": {\n",
            "    \"version\": \"1.18.1\",\n",
            "    \"support_gpu\": true\n",
            "  },\n",
            "  \"pytorch\": {\n",
            "    \"version\": \"2.3.0+cu121\",\n",
            "    \"support_gpu\": true,\n",
            "    \"cuda\": \"12.1\"\n",
            "  },\n",
            "  \"tensorflow\": {\n",
            "    \"version\": \"2.15.0\",\n",
            "    \"git_version\": \"v2.15.0-0-g6887368d6d4\",\n",
            "    \"support_gpu\": true\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download"
      ],
      "metadata": {
        "id": "km-D3g_66Wlu"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot_location = snapshot_download(repo_id=\"microsoft/Phi-3-mini-4k-instruct-onnx\",  local_dir=\"/content/drive/MyDrive/models/onnx_2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ac9909050cc1461892a1ef81ba1752b3",
            "22bb4ae3818344f394b6f65c1bf864b6",
            "799cc18f55b0489ea500c882145b1fd5",
            "c122db4c600a4dfda70f4567768904b0",
            "aacfe1f633e24cb0993e448ba8f8b3f1",
            "b2bf5d06cea94e7a86b48415570f2b9b",
            "02e5c59bd8024265a04f2553b2b9eb7f",
            "25896f6e844d4393bb73c5a6704426f6",
            "5512e46dac7b48eeb9030d138b71db99",
            "04748f800cf140bab338ce47c4040e56",
            "d6916d13f85749ba91d4587b6a0678f8"
          ]
        },
        "id": "p4i0_i_O6Zem",
        "outputId": "f7b59c6b-078a-4428-a571-ac3017bb2461"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 75 files:   0%|          | 0/75 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac9909050cc1461892a1ef81ba1752b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snapshot_location"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5Rr868nV_15I",
        "outputId": "e1be75a1-2cac-4e21-886e-79a5283d6dae"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/models/onnx_2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli download microsoft/Phi-3-mini-4k-instruct-onnx --include cuda/cuda-int4-rtn-block-32/* --local-dir /content/drive/MyDrive/models/onnx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvV5dUIcR7hM",
        "outputId": "b5e95a06-cf1f-4162-f532-ad761dd587d7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching 10 files: 100% 10/10 [00:04<00:00,  2.23it/s]\n",
            "/content/drive/MyDrive/models/onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime_genai as og\n",
        "import datetime"
      ],
      "metadata": {
        "id": "4fPWKZSOTao6"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model = og.Model(\"/content/drive/MyDrive/models/onnx/cuda/cuda-int4-rtn-block-32\")"
      ],
      "metadata": {
        "id": "BKFjDlsNSWsE"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = og.Tokenizer(model)\n",
        "tokenizer_stream = tokenizer.create_stream()\n",
        "\n",
        "search_options = {\"max_length\": 2048,\"temperature\":0.3}"
      ],
      "metadata": {
        "id": "ZaonTCchW70R"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = og.GeneratorParams(model)\n",
        "params.try_use_cuda_graph_with_max_batch_size(1)\n",
        "params.set_search_options(**search_options)\n",
        "\n",
        "prompt = \"<|user|>Who are you not allowed to marry in the UK?<|end|><|assistant|>\"\n",
        "input_tokens = tokenizer.encode(prompt)\n",
        "params.input_ids = input_tokens\n",
        "\n",
        "generator = og.Generator(model, params)"
      ],
      "metadata": {
        "id": "6h6-y2MfW7h6"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time1 = datetime.datetime.now()\n",
        "out = model.generate(params)\n",
        "tokenizer.decode(out[0])\n",
        "time2 = datetime.datetime.now()\n",
        "print(time2-time1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM_M0BxT09Ai",
        "outputId": "4b57340a-ad95-4e22-cb8e-51bf500b3153"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:00:02.323243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(out[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "b1B-1Scc1XTr",
        "outputId": "a9682eff-c742-4fc8-86c1-be4ff6d6e5d2"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Who are you not allowed to marry in the UK?In the United Kingdom, the law does not prohibit marriage between individuals based on their nationality or citizenship. As of my knowledge cutoff in 2023, there are no restrictions on marriage between a British citizen and a foreign national. However, it is important to note that while the law does not prevent such unions, there may be practical considerations regarding visa status and residency rights that could affect a foreign national'dependent' partner's ability to live and work in the UK.\\n\\nIt is also worth mentioning that while the UK does not have laws that prevent interracial or interethnic marriages, there have been historical instances of discrimination and prejudice. However, these societal attitudes have significantly diminished over time, and the UK is now considered to be a diverse and inclusive society.\\n\\nIf you are considering marriage in the UK, it is advisable to consult with legal experts or immigration advisors to understand the implications for both parties, especially regarding residency rights and family reunification.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out =\"\"\n",
        "while not generator.is_done():\n",
        "                generator.compute_logits()\n",
        "                generator.generate_next_token()\n",
        "\n",
        "                new_token = generator.get_next_tokens()[0]\n",
        "                t= tokenizer_stream.decode(new_token)\n",
        "                out = out +t"
      ],
      "metadata": {
        "id": "F3Vh0wLQXL4t"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "8Wq2V9LUeN0R",
        "outputId": "761533c7-2cb9-473e-b15e-d6e6f9038140"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" In the United Kingdom, the law does not prohibit marriage between individuals based on their nationality or citizenship. As of my knowledge cutoff in 2023, there are no restrictions on marriage between a British citizen and a foreign national. However, it is important to note that while the law does not prevent such unions, there may be practical considerations regarding visa status and residency rights that could affect a foreign national'dependent' partner's ability to live and work in the UK.\\n\\nIt is also worth mentioning that while the UK does not have laws that prevent interracial or interethnic marriages, there have been historical instances of discrimination and prejudice. However, these societal attitudes have significantly diminished over time, and the UK is now considered to be a diverse and inclusive society.\\n\\nIf you are considering marriage in the UK, it is advisable to consult with legal experts or immigration advisors to understand the implications for both parties, especially regarding residency rights and family reunification.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnxruntime.get_available_providers()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gJA4J3Bc9u6",
        "outputId": "1044a346-bd5f-4848-bd4d-27b25b9d999c"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m onnxruntime_genai.models.builder --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5RR1iU8J0Dm",
        "outputId": "fcea651d-98ea-48c3-c610-9faaf379c8d3"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: builder.py [-h] [-m MODEL_NAME] [-i INPUT] -o OUTPUT -p {int4,fp16,fp32} -e\n",
            "                  {cpu,cuda,dml,web} [-c CACHE_DIR] [--extra_options KEY=VALUE [KEY=VALUE ...]]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -m MODEL_NAME, --model_name MODEL_NAME\n",
            "                        Model name in Hugging Face. Do not use if providing an input path to a Hugging Face directory in -i/--input.\n",
            "  -i INPUT, --input INPUT\n",
            "                        Input model source. Currently supported options are:\n",
            "                            hf_path: Path to folder on disk containing the Hugging Face config, model, tokenizer, etc.\n",
            "                            gguf_path: Path to float16/float32 GGUF file on disk containing the GGUF model\n",
            "  -o OUTPUT, --output OUTPUT\n",
            "                        Path to folder to store ONNX model and additional files (e.g. GenAI config, external data files, etc.)\n",
            "  -p {int4,fp16,fp32}, --precision {int4,fp16,fp32}\n",
            "                        Precision of model\n",
            "  -e {cpu,cuda,dml,web}, --execution_provider {cpu,cuda,dml,web}\n",
            "                        Execution provider to target with precision of model (e.g. FP16 CUDA, INT4 CPU, INT4 WEB)\n",
            "  -c CACHE_DIR, --cache_dir CACHE_DIR\n",
            "                        Cache directory for Hugging Face files and temporary ONNX external data files\n",
            "  --extra_options KEY=VALUE [KEY=VALUE ...]\n",
            "                        Key value pairs for various options. Currently supports:\n",
            "                            int4_block_size = 16/32/64/128/256: Specify the block_size for int4 quantization.\n",
            "                            int4_accuracy_level = 1/2/3/4: Specify the minimum accuracy level for activation of MatMul in int4 quantization.\n",
            "                                4 is int8, which means input A of int4 quantized MatMul is quantized to int8 and input B is upcasted to int8 for computation.\n",
            "                                3 is bf16.\n",
            "                                2 is fp16.\n",
            "                                1 is fp32.\n",
            "                            num_hidden_layers = Manually specify the number of layers in your ONNX model (for unit testing purposes).\n",
            "                            filename = Filename for ONNX model (default is 'model.onnx').\n",
            "                                For models with multiple components, each component is exported to its own ONNX model.\n",
            "                                The filename for each component will be '<filename>_<component-name>.onnx' (ex: '<filename>_encoder.onnx', '<filename>_decoder.onnx').\n",
            "                            config_only = Generate config and pre/post processing files only.\n",
            "                                Use this option when you already have your optimized and/or quantized ONNX model.\n",
            "                            exclude_embeds = Remove embedding layer from your ONNX model.\n",
            "                                Use this option when you want to remove the embedding layer from within your ONNX model.\n",
            "                                Instead of `input_ids`, you will have `inputs_embeds` as the input to your ONNX model.\n",
            "                            exclude_lm_head = Remove language modeling head from your ONNX model.\n",
            "                                Use this option when you want to remove the language modeling head from within your ONNX model.\n",
            "                                Instead of `logits`, you will have `hidden_states` as the output to your ONNX model.\n",
            "                            enable_cuda_graph = 1 : The model can use CUDA graph capture for CUDA execution provider. If enabled, all nodes being placed on the CUDA EP\n",
            "                                is the prerequisite for the CUDA graph to be used correctly. It is not guaranteed that cuda graph be enabled as it depends on the model\n",
            "                                and the graph structure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install flash_attn einops timm mlflow pyngrok transformers accelerate --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sTJALV3Lhg0",
        "outputId": "53d579b4-5d72-4344-8b75-21ef15b968fb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U transformers accelerate --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVLbnjbDMCkS",
        "outputId": "638d7cb2-3f0c-45ad-cd46-9eaef8382e37"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# microsoft/Phi-3-mini-128k-instruct"
      ],
      "metadata": {
        "id": "UWZLTn0NKEci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qpz4MS9K13J",
        "outputId": "e112515a-b266-4a38-a584-9d5a98cec898"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m onnxruntime_genai.models.builder -m 'microsoft/Phi-3-mini-128k-instruct' -o '/content/drive/MyDrive/models/onnx/Phi-3-mini-128k-instruct' -p int4 -e cuda -c '/tmp'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJviKAkLKKLE",
        "outputId": "0dd43592-8c3a-41a3-b060-1edf9efae17b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid precision + execution provider combinations are: FP32 CPU, FP32 CUDA, FP16 CUDA, FP16 DML, INT4 CPU, INT4 CUDA, INT4 DML\n",
            "Extra options: {}\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py:950: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "GroupQueryAttention (GQA) is used in this model.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "model.safetensors.index.json: 100% 16.3k/16.3k [00:00<00:00, 63.8MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/4.97G [00:00<00:16, 297MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.97G [00:00<00:13, 360MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.97G [00:00<00:14, 346MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.97G [00:00<00:13, 357MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 220M/4.97G [00:00<00:12, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 273M/4.97G [00:00<00:11, 401MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.97G [00:00<00:12, 384MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 357M/4.97G [00:00<00:12, 385MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.97G [00:01<00:12, 370MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 451M/4.97G [00:01<00:11, 379MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.97G [00:01<00:12, 356MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.97G [00:01<00:12, 352MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.97G [00:01<00:12, 358MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.97G [00:01<00:11, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.97G [00:01<00:11, 369MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.97G [00:01<00:12, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.97G [00:02<00:12, 348MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 786M/4.97G [00:02<00:12, 340MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 828M/4.97G [00:02<00:12, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.97G [00:02<00:11, 346MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 912M/4.97G [00:02<00:11, 359MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/4.97G [00:02<00:11, 359MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.97G [00:02<00:11, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/4.97G [00:02<00:11, 353MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/4.97G [00:03<00:11, 344MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.12G/4.97G [00:03<00:11, 341MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/4.97G [00:03<00:10, 354MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:03<00:10, 371MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.97G [00:03<00:09, 383MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.97G [00:03<00:09, 400MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.97G [00:03<00:08, 410MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/4.97G [00:03<00:09, 394MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.97G [00:03<00:09, 377MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.97G [00:04<00:09, 367MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.97G [00:04<00:08, 391MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.97G [00:04<00:08, 414MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.97G [00:04<00:08, 395MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.67G/4.97G [00:04<00:08, 376MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/4.97G [00:04<00:09, 356MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.97G [00:04<00:09, 344MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.97G [00:04<00:09, 339MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.97G [00:05<00:09, 334MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.97G [00:05<00:08, 349MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.97G [00:05<00:08, 372MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.97G [00:05<00:07, 380MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.01G/4.97G [00:05<00:07, 389MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.97G [00:05<00:07, 395MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.97G [00:05<00:07, 373MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.97G [00:05<00:07, 356MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.97G [00:05<00:07, 358MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.97G [00:06<00:07, 363MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.26G/4.97G [00:06<00:07, 373MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.97G [00:06<00:07, 347MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.97G [00:06<00:07, 354MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/4.97G [00:06<00:06, 385MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.97G [00:06<00:06, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.97G [00:06<00:06, 401MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.55G/4.97G [00:06<00:05, 409MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.60G/4.97G [00:07<00:05, 406MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.97G [00:07<00:05, 413MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.97G [00:07<00:05, 416MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.97G [00:07<00:05, 411MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.97G [00:07<00:05, 410MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.97G [00:07<00:05, 414MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.97G [00:07<00:05, 397MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.97G [00:07<00:05, 402MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [00:07<00:04, 400MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.97G [00:08<00:04, 396MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.97G [00:08<00:04, 405MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.12G/4.97G [00:08<00:04, 413MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.97G [00:08<00:04, 417MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.97G [00:08<00:04, 421MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.97G [00:08<00:03, 426MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/4.97G [00:08<00:03, 430MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [00:08<00:03, 435MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.44G/4.97G [00:09<00:03, 425MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.97G [00:09<00:03, 427MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.97G [00:09<00:03, 410MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/4.97G [00:09<00:03, 420MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.97G [00:09<00:03, 422MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.70G/4.97G [00:09<00:03, 414MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.97G [00:09<00:03, 405MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.79G/4.97G [00:09<00:02, 402MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.97G [00:10<00:02, 421MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/4.97G [00:10<00:04, 250MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.97G [00:10<00:04, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.97G [00:10<00:05, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.01G/4.97G [00:11<00:05, 175MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.97G [00:11<00:05, 157MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.06G/4.97G [00:11<00:06, 147MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.97G [00:11<00:06, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.97G [00:11<00:06, 128MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.97G [00:12<00:06, 122MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.97G [00:12<00:06, 124MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.16G/4.97G [00:12<00:06, 120MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.97G [00:12<00:06, 118MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.20G/4.97G [00:12<00:06, 110MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.97G [00:13<00:06, 108MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.97G [00:13<00:06, 109MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.97G [00:13<00:06, 110MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.97G [00:13<00:05, 115MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.31G/4.97G [00:13<00:05, 114MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.97G [00:13<00:05, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.97G [00:14<00:02, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.97G [00:14<00:02, 241MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.97G [00:14<00:01, 271MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.51G/4.97G [00:14<00:01, 291MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.97G [00:14<00:01, 316MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.97G [00:14<00:01, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.63G/4.97G [00:14<00:01, 302MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/4.97G [00:14<00:00, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/4.97G [00:15<00:00, 361MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.97G [00:15<00:00, 356MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.81G/4.97G [00:15<00:00, 360MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.85G/4.97G [00:15<00:00, 363MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.91G/4.97G [00:15<00:00, 379MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [00:15<00:00, 315MB/s]\n",
            "Downloading shards:  50% 1/2 [00:16<00:16, 16.29s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 41.9M/2.67G [00:00<00:06, 393MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 94.4M/2.67G [00:00<00:06, 415MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 147M/2.67G [00:00<00:05, 437MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 199M/2.67G [00:00<00:06, 407MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 252M/2.67G [00:00<00:05, 429MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 304M/2.67G [00:00<00:05, 444MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 357M/2.67G [00:00<00:05, 446MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 409M/2.67G [00:00<00:05, 429MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 461M/2.67G [00:01<00:05, 422MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 514M/2.67G [00:01<00:04, 442MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 566M/2.67G [00:01<00:04, 460MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 619M/2.67G [00:01<00:06, 299MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 671M/2.67G [00:01<00:05, 336MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 724M/2.67G [00:01<00:05, 372MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 776M/2.67G [00:01<00:04, 403MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 828M/2.67G [00:02<00:04, 430MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 881M/2.67G [00:02<00:04, 417MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 933M/2.67G [00:02<00:04, 393MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 975M/2.67G [00:02<00:04, 392MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.03G/2.67G [00:02<00:04, 409MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.08G/2.67G [00:02<00:03, 427MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.13G/2.67G [00:02<00:03, 436MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.18G/2.67G [00:02<00:03, 444MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.24G/2.67G [00:02<00:03, 449MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.29G/2.67G [00:03<00:03, 459MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.34G/2.67G [00:03<00:02, 468MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.39G/2.67G [00:03<00:02, 472MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.45G/2.67G [00:03<00:02, 476MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.50G/2.67G [00:03<00:02, 481MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.55G/2.67G [00:03<00:02, 484MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.60G/2.67G [00:03<00:02, 484MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.66G/2.67G [00:03<00:02, 381MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.70G/2.67G [00:04<00:02, 351MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.74G/2.67G [00:04<00:02, 331MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.78G/2.67G [00:04<00:02, 318MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.82G/2.67G [00:04<00:02, 312MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.87G/2.67G [00:04<00:02, 301MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.90G/2.67G [00:04<00:02, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.93G/2.67G [00:04<00:02, 280MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.96G/2.67G [00:05<00:02, 268MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.99G/2.67G [00:05<00:02, 263MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.02G/2.67G [00:05<00:02, 250MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.06G/2.67G [00:05<00:02, 227MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.09G/2.67G [00:05<00:02, 230MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.12G/2.67G [00:05<00:02, 239MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.15G/2.67G [00:05<00:02, 256MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.19G/2.67G [00:05<00:01, 280MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.23G/2.67G [00:06<00:01, 304MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.28G/2.67G [00:06<00:01, 319MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.32G/2.67G [00:06<00:01, 341MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.36G/2.67G [00:06<00:00, 352MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.40G/2.67G [00:11<00:10, 26.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.43G/2.67G [00:11<00:07, 33.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.46G/2.67G [00:11<00:05, 40.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.52G/2.67G [00:11<00:02, 62.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.57G/2.67G [00:12<00:01, 89.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.62G/2.67G [00:12<00:00, 124MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.67G/2.67G [00:12<00:00, 217MB/s]\n",
            "Downloading shards: 100% 2/2 [00:29<00:00, 14.53s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.68s/it]\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.19MB/s]\n",
            "Reading embedding layer\n",
            "Reading decoder layer 0\n",
            "Reading decoder layer 1\n",
            "Reading decoder layer 2\n",
            "Reading decoder layer 3\n",
            "Reading decoder layer 4\n",
            "Reading decoder layer 5\n",
            "Reading decoder layer 6\n",
            "Reading decoder layer 7\n",
            "Reading decoder layer 8\n",
            "Reading decoder layer 9\n",
            "Reading decoder layer 10\n",
            "Reading decoder layer 11\n",
            "Reading decoder layer 12\n",
            "Reading decoder layer 13\n",
            "Reading decoder layer 14\n",
            "Reading decoder layer 15\n",
            "Reading decoder layer 16\n",
            "Reading decoder layer 17\n",
            "Reading decoder layer 18\n",
            "Reading decoder layer 19\n",
            "Reading decoder layer 20\n",
            "Reading decoder layer 21\n",
            "Reading decoder layer 22\n",
            "Reading decoder layer 23\n",
            "Reading decoder layer 24\n",
            "Reading decoder layer 25\n",
            "Reading decoder layer 26\n",
            "Reading decoder layer 27\n",
            "Reading decoder layer 28\n",
            "Reading decoder layer 29\n",
            "Reading decoder layer 30\n",
            "Reading decoder layer 31\n",
            "Reading final norm\n",
            "Reading LM head\n",
            "Saving ONNX model in /content/drive/MyDrive/models/onnx/Phi-3-mini-128k-instruct\n",
            "2024-07-04 19:23:08,772 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:08,924 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:08,929 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:08,999 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:08,999 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:09,135 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:09,140 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:09,280 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:09,285 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:09,421 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:09,425 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:09,583 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:09,588 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:09,637 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:09,637 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:09,769 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:09,774 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:09,909 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:09,913 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:10,079 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:10,083 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:10,233 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:10,239 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:10,288 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:10,288 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:10,426 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:10,431 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:10,571 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:10,576 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:10,712 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:10,716 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:10,874 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:10,880 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:10,928 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:10,928 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:11,068 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:11,073 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:11,209 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:11,214 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:11,350 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:11,354 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:11,507 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:11,513 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:11,561 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:11,561 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:11,700 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:11,704 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:11,845 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:11,850 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:11,987 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:11,992 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:12,154 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:12,159 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:12,208 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:12,208 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:12,343 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:12,348 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:12,485 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:12,490 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:12,625 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:12,630 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:12,781 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:12,787 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:12,836 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:12,836 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:12,974 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:12,979 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:13,127 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:13,132 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:13,267 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:13,272 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:13,432 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:13,437 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:13,487 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:13,487 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:13,623 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:13,627 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:13,779 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:13,784 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:13,919 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:13,924 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:14,079 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:14,084 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:14,134 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:14,134 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:14,272 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:14,276 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:14,417 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:14,422 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:14,558 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:14,562 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:14,720 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:14,725 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:14,774 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:14,774 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:14,909 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:14,913 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:15,049 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:15,053 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:15,204 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:15,208 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:15,385 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:15,389 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:15,438 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:15,438 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:15,574 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:15,579 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:15,716 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:15,720 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:15,856 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:15,860 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:16,013 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:16,017 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:16,066 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:16,067 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:16,219 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:16,224 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:16,366 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:16,371 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:16,505 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:16,509 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:16,666 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:16,671 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:16,720 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:16,720 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:16,869 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:16,874 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:17,027 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:17,031 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:17,169 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:17,173 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:17,324 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:17,328 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:17,377 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:17,378 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:17,527 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:17,531 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:17,684 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:17,688 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:17,823 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:17,827 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:17,985 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:17,990 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:18,039 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:18,040 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:18,231 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:18,235 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:18,430 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:18,434 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:18,628 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:18,633 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:18,850 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:18,854 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:18,925 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:18,925 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:19,135 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:19,139 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:19,338 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:19,342 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:19,538 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:19,542 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:19,767 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:19,772 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:19,834 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:19,835 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:20,050 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:20,054 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:20,255 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:20,259 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:20,394 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:20,398 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:20,548 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:20,553 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:20,601 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:20,601 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:20,765 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:20,768 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:20,941 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:20,945 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:21,078 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:21,082 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:21,238 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:21,242 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:21,290 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:21,290 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:21,463 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:21,466 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:21,647 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:21,651 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:21,783 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:21,787 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:21,943 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:21,947 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:21,997 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:21,997 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:22,157 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:22,161 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:22,332 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:22,336 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:22,471 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:22,474 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:22,623 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:22,627 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:22,675 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:22,675 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:22,842 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:22,846 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:23,015 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:23,019 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:23,152 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:23,156 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:23,315 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:23,319 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:23,367 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:23,368 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:23,532 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:23,536 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:23,699 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:23,703 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:23,835 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:23,838 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:23,988 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:23,992 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:24,040 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:24,040 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:24,222 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:24,226 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:24,401 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:24,405 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:24,539 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:24,542 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:24,698 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:24,702 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:24,750 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:24,750 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:24,911 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:24,915 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:25,086 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:25,090 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:25,224 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:25,228 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:25,382 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:25,386 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:25,435 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:25,435 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:25,610 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:25,613 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:25,792 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:25,795 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:25,928 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:25,932 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:26,090 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:26,094 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:26,143 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:26,143 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:26,320 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:26,324 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:26,507 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:26,511 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:26,644 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:26,648 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:26,798 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:26,802 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:26,851 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:26,851 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:27,022 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:27,025 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:27,209 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:27,213 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:27,347 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:27,350 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:27,542 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:27,547 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:27,596 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:27,596 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:27,767 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:27,771 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:27,941 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:27,945 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:28,086 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:28,090 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:28,239 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:28,243 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:28,291 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:28,291 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:28,484 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:28,488 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:28,667 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:28,671 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:28,805 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:28,809 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:28,965 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:28,969 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:29,019 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:29,019 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:29,198 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:29,201 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:29,379 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:29,382 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:29,519 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:29,522 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:29,673 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:29,677 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:29,725 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:29,725 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:29,906 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:29,909 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:30,093 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:30,096 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:30,239 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:30,242 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:30,426 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/attn/qkv_proj/MatMul ...\n",
            "2024-07-04 19:23:30,430 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:30,501 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/attn/o_proj/MatMul ...\n",
            "2024-07-04 19:23:30,501 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:30,756 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/mlp/gate_proj/MatMul ...\n",
            "2024-07-04 19:23:30,760 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:30,984 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/mlp/up_proj/MatMul ...\n",
            "2024-07-04 19:23:30,987 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:31,181 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/mlp/down_proj/MatMul ...\n",
            "2024-07-04 19:23:31,184 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /lm_head/MatMul ...\n",
            "2024-07-04 19:23:31,859 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /lm_head/MatMul ...\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:886: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "Saving GenAI config in /content/drive/MyDrive/models/onnx/Phi-3-mini-128k-instruct\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 3.44k/3.44k [00:00<00:00, 25.2MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 439MB/s]\n",
            "tokenizer.json: 100% 1.94M/1.94M [00:00<00:00, 7.90MB/s]\n",
            "added_tokens.json: 100% 306/306 [00:00<00:00, 2.27MB/s]\n",
            "special_tokens_map.json: 100% 599/599 [00:00<00:00, 4.16MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Saving processing files in /content/drive/MyDrive/models/onnx/Phi-3-mini-128k-instruct for GenAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "ofqak2PVNVjB"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "n7uQAwj2NXqB"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model2 = og.Model(\"/content/drive/MyDrive/models/onnx/Phi-3-mini-128k-instruct\")"
      ],
      "metadata": {
        "id": "540JXV8-NP-b"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2 = og.Tokenizer(model2)\n",
        "tokenizer_stream2= tokenizer2.create_stream()\n",
        "\n",
        "search_options = {\"max_length\": 512,\"temperature\":0.9}"
      ],
      "metadata": {
        "id": "8UKiqhdGN6ci"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = og.GeneratorParams(model2)\n",
        "params.try_use_cuda_graph_with_max_batch_size(1)\n",
        "params.set_search_options(**search_options)\n",
        "\n",
        "prompt = \"<|user|>Tell me a joke about taxidrivers<|end|><|assistant|>\"\n",
        "input_tokens = tokenizer2.encode(prompt)\n",
        "params.input_ids = input_tokens\n",
        "\n",
        "generator2 = og.Generator(model2, params)"
      ],
      "metadata": {
        "id": "2eCJqMC5OGUA"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out2 = model2.generate(params)"
      ],
      "metadata": {
        "id": "bnJMq_bFOQVX"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer2.decode(out2[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayvozf-lOV2f",
        "outputId": "c3404382-a1e4-47f7-b652-c66d9fdef297"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a joke about taxidriversCon-tribute Answer: The question is- ising with the knowledge with--(outside of-know- 3 to respond about a new-\n",
            "\n",
            "Think: (al: the taxi:\n",
            "comet', for--tell-erici, in Italian,oss,i ions of not) and this  peuvent- oislock—with theft\n",
            "\n",
            "\\\"\\'\\ custom\\ rice\\t\\tpic\\r-lo the, <<:teh) Call-  (6)$\\fword tori\\tagas) the has exceed?\n",
            "using  ]ersersophentrees.  for 8\n",
            "\n",
            " \"The Osbubba_Rushland_R_ and - and Reali, &_ andre.Ter_ and t is of the course of day has, kt::\n"
          ]
        }
      ]
    }
  ]
}