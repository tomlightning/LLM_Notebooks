{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Inference on ONNX Runtime on Azure Kubernetes Service\r",
    "#### Video \n",
    "Vision Transformer converted to ONNXs\r\n",
    "\r\n",
    "This example shows how to deploy an image classification neural network using ONNX Runtime on GPU compute SKUs in Azur In this example we useVideo  a Vision Transformer fine tuned with a custom datased to detect images non-safe for workrme\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prerequisites to install A\n",
    "Please restart kernel after pip installs to sync environment with new modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install matplotlib onnx opencv-python\n",
    "# pip install azure-ai-ml azure-identity datasets azure-cli mlflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### 2. Connect to Azure Machine Learning workspace\n",
    "\n",
    "Before we dive in the code, you'll need to connect to your workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
    "\n",
    "For this lab, we've already setup an AzureML Workspace for you. If you'd like to learn more about `Workspace`s, please reference [`AzureML's documentation`](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?view=azureml-api-2&tabs=azure-portal).\n",
    "\n",
    "We are using the `DefaultAzureCredential` to get access to workspace. `DefaultAzureCredential` should be capable of handling most scenarios. If you want to learn more about other available credentials, go to [`Set up authentication`](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk&view=azureml-api-2) for more available credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from dotenv import dotenv_values\n",
    "from dotenv import load_dotenv\n",
    "from utils.login import get_ws_client\n",
    "from utils.datasets import get_labels_dataset, create_datasets\n",
    "from utils.computer import create_gpu_cluster\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .env file to use your service principal account\n",
    "\n",
    "```md\n",
    "RESOURCE_GROUP=\n",
    "SUBSCRIPTION_ID=\n",
    "AZUREML_WORKSPACE_NAME=\n",
    "TENANTID=\n",
    "AZURE_CLIENT_ID=\n",
    "AZURE_TENANT_ID=\n",
    "AZURE_CLIENT_SECRET=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env and login to Workspace\n",
    "load_dotenv(\".env\")\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "\n",
    "# Enter details of your Azure Machine Learning workspace\n",
    "subscription_id = config.get(\"SUBSCRIPTION_ID\")\n",
    "resource_group = config.get(\"RESOURCE_GROUP\")\n",
    "workspace = 'azure-ml-2'#config.get(\"AZUREML_WORKSPACE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x0000016AC0BAB790>,\n",
      "         subscription_id=5a8ec57c-47f9-4bc3-aee5-9e4db1b89345,\n",
      "         resource_group_name=olonok-ml,\n",
      "         workspace_name=azure-ml-2)\n"
     ]
    }
   ],
   "source": [
    "credential = DefaultAzureCredential()\n",
    "# Check if given credential can get token successfully.\n",
    "credential.get_token(\"https://management.azure.com/.default\")\n",
    "\n",
    "\n",
    "ml_client = get_ws_client(\n",
    "    credential, subscription_id, resource_group, workspace\n",
    ")\n",
    "print(ml_client)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Model Converted to ONNX\n",
    "#### Video\n",
    "LLMOps: Convert Video Classifier (ViViT ) to ONNX, Inference on a CPU  --> https://youtu.be/-vjr0IjH4Nc\n",
    "#### code\n",
    "https://github.com/olonok69/LLM_Notebooks/tree/main/video/convert_onnx\n",
    "\n",
    "\n",
    "#### Model Base \n",
    "https://huggingface.co/google/vivit-b-16x2-kinetics400\n",
    "\n",
    "### Fine Tuning with own Dataset\n",
    "#### Code\n",
    "https://github.com/olonok69/LLM_Notebooks/tree/main/video/fine_tune_ViViT\n",
    "\n",
    "#### Video\n",
    "LLMOps: Fine Tune Video Classifier (ViViT ) with your own data --> https://youtu.be/XNMU_bm0Xwc\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1684438712706
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "path_model =\"onnx_vivit/vivit.onnx\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Load Azure ML workspace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1684438754226
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.57.0\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5a8ec57c-47f9-4bc3-aee5-9e4db1b89345\n",
      "uksouth\n",
      "olonok-ml\n",
      "azure-ml-2\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# read existing workspace from config.json\n",
    "ws = Workspace(subscription_id=subscription_id, resource_group=resource_group, workspace_name=workspace)\n",
    "\n",
    "print(ws.subscription_id, ws.location, ws.resource_group, ws.name, sep = '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register your ONNX model with Azure ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1684438763102
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading vivit.onnx\u001b[32m (< 1 MB): 273MB [09:36, 473kB/s]                                                                                                                              \u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model({'job_name': None, 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'vivit_classifier', 'description': 'Video classification', 'tags': {'type': 'google/vivit-b-16x2-kinetics400', 'format': 'onnx', 'fine_tuned': '10 classes from https://www.crcv.ucf.edu/research/data-sets/ucf101/'}, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/5a8ec57c-47f9-4bc3-aee5-9e4db1b89345/resourceGroups/olonok-ml/providers/Microsoft.MachineLearningServices/workspaces/azure-ml-2/models/vivit_classifier/versions/1', 'Resource__source_path': None, 'base_path': 'D:\\\\repos\\\\onnx\\\\azureml', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000016AABF03F10>, 'serialize': <msrest.serialization.Serializer object at 0x0000016AABF41FD0>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/5a8ec57c-47f9-4bc3-aee5-9e4db1b89345/resourceGroups/olonok-ml/workspaces/azure-ml-2/datastores/workspaceblobstore/paths/LocalUpload/e57c1eca97ee89da07fb639177213969/vivit.onnx', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model', 'stage': 'Development'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"onnx_vitit/vivit.onnx\" # replace this with the location of your model files\n",
    "\n",
    "\n",
    "file_model = Model(\n",
    "    path= path_model,\n",
    "    type=AssetTypes.CUSTOM_MODEL,\n",
    "    name=\"vivit_classifier\",\n",
    "    description= \"Video classification\",\n",
    "    tags={\"type\": \"google/vivit-b-16x2-kinetics400\", \"format\":\"onnx\", \"fine_tuned\": \"10 classes from https://www.crcv.ucf.edu/research/data-sets/ucf101/\"}\n",
    ")\n",
    "ml_client.models.create_or_update(file_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1684438864620
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: vivit_classifier \tVersion: 1 \tDescription: Video classification {'type': 'google/vivit-b-16x2-kinetics400', 'format': 'onnx', 'fine_tuned': '10 classes from https://www.crcv.ucf.edu/research/data-sets/ucf101/'}\n"
     ]
    }
   ],
   "source": [
    "models = ws.models\n",
    "for name, m in models.items():\n",
    "    print(\"Name:\", name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = ml_client.models.download(\"onnx_nsfw\", version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            reformatted_frame = frame.reformat(width=224,height=224)\n",
    "            frames.append(reformatted_frame)\n",
    "    new=np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n",
    "\n",
    "def get_key(dict, value):\n",
    "    \"\"\"\n",
    "    return key given a value. From a dictionary\n",
    "    \"\"\"\n",
    "    for key, val in dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return \"Value not found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate a request to build the score.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./image/6540601-uhd_2560_1440_25fps.mp4\"\n",
    "with open(file_name, \"rb\") as f:\n",
    "    video = f.read()\n",
    "    f.close()\n",
    "    im_b64 = base64.b64encode(video).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = json.dumps({'data': im_b64})\n",
    "requests_json = json.loads(input_data.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = base64.b64decode(requests_json.get(\"data\").encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "iov = io.BytesIO(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_io.BytesIO"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "container= av.open(io.BytesIO(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container.streams.video[0].frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = sample_frame_indices(clip_len=10, frame_sample_rate=2,seg_len=container.streams.video[0].frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44, 46, 48, 50, 52, 55, 57, 59, 61, 63], dtype=int64)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = read_video_pyav(container=container, indices=indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 224, 224, 3)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VivitImageProcessor\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess = onnxruntime.InferenceSession(\n",
    "    \"./onnx_vivit/vivit.onnx\", providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "\n",
    "input_name = ort_sess.get_inputs()[0].name\n",
    "output_name = ort_sess.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {'ApplyEyeMakeup':0, 'ApplyLipstick':1, 'Archery':2, 'BabyCrawling':3, 'BalanceBeam':4, 'BandMarching':5, \n",
    "             'BaseballPitch':6, 'Basketball':7,'BasketballDunk':8, 'BenchPress':9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_t = np.array(image_processor(list(video), return_tensors=\"pt\")['pixel_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 3, 224, 224)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ort_sess.run([output_name], {input_name: inputs_t})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Logits\n",
    "logits = np.array(outputs)\n",
    "# Get Probabilities\n",
    "probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "# Get Pedicted Class\n",
    "predicted_class = np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: 2, label: Archery\n",
      "\n",
      "\n",
      "All Probabilities:\n",
      "{'ApplyEyeMakeup': 0.0025050577241927385, 'ApplyLipstick': 0.0018224065424874425, 'Archery': 0.9428067803382874, 'BabyCrawling': 0.002071560360491276, 'BalanceBeam': 0.004101442638784647, 'BandMarching': 0.009591354057192802, 'BaseballPitch': 0.0006394461379386485, 'Basketball': 0.007728622294962406, 'BasketballDunk': 0.0010978842619806528, 'BenchPress': 0.02763550356030464}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted classes: {predicted_class[0]}, label: { get_key(label_dic, predicted_class[0])}\")\n",
    "print(\"\\n\")\n",
    "output_probs = {}\n",
    "print(\"All Probabilities:\")\n",
    "for prob, key in zip(probabilities[0], range(0, len(probabilities[0]))):\n",
    "    label = get_key(label_dic, key)\n",
    "    output_probs[label] = float(prob)\n",
    "\n",
    "print(output_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Scoring file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting onnx_vivit/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile onnx_vivit/score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from transformers import VivitImageProcessor\n",
    "import sys\n",
    "import os\n",
    "from azureml.core.model import Model\n",
    "import time\n",
    "import av\n",
    "import base64\n",
    "import io\n",
    "\n",
    "\n",
    "def init():\n",
    "    global session, input_name, output_name, image_processor, label_dic\n",
    "    model = \"./vivit.onnx\"\n",
    "    # Load the model in onnx runtime to start the session    \n",
    "    session = onnxruntime.InferenceSession(model, providers=[\"CPUExecutionProvider\"])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name \n",
    "    image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "    label_dic = {'ApplyEyeMakeup':0, 'ApplyLipstick':1, 'Archery':2, 'BabyCrawling':3, 'BalanceBeam':4, 'BandMarching':5, 'BaseballPitch':6, 'Basketball':7,'BasketballDunk':8, 'BenchPress':9}\n",
    "    \n",
    "def run(input_data):\n",
    "    '''Purpose: evaluate test input in Azure Cloud using onnxruntime.\n",
    "        We will call the run function later from our Jupyter Notebook \n",
    "        so our azure service can evaluate our model input in the cloud. '''\n",
    "\n",
    "    try:\n",
    "        # We expect a video im base64 format in the attribute data. Load the data and decode the base64\n",
    "        requests_json = json.loads(input_data.encode())\n",
    "        v = base64.b64decode(requests_json.get(\"data\").encode(\"utf-8\"))\n",
    "        #create container from bytes request\n",
    "        container= av.open(io.BytesIO(v))\n",
    "        #Sample Frames\n",
    "        indices = sample_frame_indices(clip_len=10, frame_sample_rate=1,seg_len=container.streams.video[0].frames)\n",
    "        # create video with sampled frames\n",
    "        video = read_video_pyav(container=container, indices=indices)\n",
    "        # tokenize video\n",
    "        data = np.array(image_processor(list(video), return_tensors=\"pt\")['pixel_values'])\n",
    "        #### INFERENCE ONNX #####\n",
    "        # pass input data to do model inference with ONNX Runtime\n",
    "        start = time.time()\n",
    "        r = session.run([output_name], {input_name : data})\n",
    "        end = time.time()\n",
    "        probabilities, predicted_class = postprocess(r[0])\n",
    "        # predicted class and label\n",
    "        class_label = predicted_class[0]\n",
    "        label = get_key(label_dic, class_label)\n",
    "        \n",
    "        result = label_map(probabilities)\n",
    "        result['predicted_label'] = label\n",
    "        result['predicted_calss'] = int(class_label)\n",
    "        \n",
    "        result_dict = {\"result\": result,\n",
    "                      \"time_in_sec\": [end - start]}\n",
    "    except Exception as e:\n",
    "        result_dict = {\"error\": str(e)}\n",
    "    \n",
    "    return json.dumps(result_dict)\n",
    "\n",
    "\n",
    "\n",
    "def label_map(probs, threshold=.5):\n",
    "    \"\"\"Take the most probable labels (output of postprocess) and returns the \n",
    "    probs of each label.\"\"\"\n",
    "    # labels and dictionary to \n",
    "    label_dic = {'ApplyEyeMakeup':0, 'ApplyLipstick':1, 'Archery':2, 'BabyCrawling':3, 'BalanceBeam':4, 'BandMarching':5, \n",
    "             'BaseballPitch':6, 'Basketball':7,'BasketballDunk':8, 'BenchPress':9}\n",
    "    output_probs = {}\n",
    "    image_preds = {}\n",
    "    for prob, key in zip(probs[0], range(0, len(probs[0]))):\n",
    "        label = get_key(label_dic, key)\n",
    "        output_probs[label] = float(prob)\n",
    "\n",
    "    image_preds[\"class_probabilities\"] = output_probs\n",
    "    return image_preds\n",
    "\n",
    "def get_key(dict, value):\n",
    "    \"\"\"\n",
    "    return key given a value. From a dictionary\n",
    "    \"\"\"\n",
    "    for key, val in dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return \"Value not found\"\n",
    "\n",
    "def postprocess(scores):\n",
    "    \"\"\"This function takes the scores generated by the network and \n",
    "    returns the class IDs in decreasing order of probability.\"\"\"\n",
    "    logits = np.array(scores)\n",
    "    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    predicted_class = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    return probabilities, predicted_class\n",
    "\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            reformatted_frame = frame.reformat(width=224,height=224)\n",
    "            frames.append(reformatted_frame)\n",
    "    new=np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    "BuildContext\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define an endpoint name\n",
    "endpoint_name = \"endpt-vivit-inference-onnx\"\n",
    "\n",
    "# Example way to define a random name\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ManagedOnlineEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://endpt-vivit-inference-onnx.uksouth.inference.ml.azure.com/score', 'openapi_uri': 'https://endpt-vivit-inference-onnx.uksouth.inference.ml.azure.com/swagger.json', 'name': 'endpt-vivit-inference-onnx', 'description': 'this is a endpoint for onnx inference Video ViT classification model', 'tags': {}, 'properties': {'azureml.onlineendpointid': '/subscriptions/5a8ec57c-47f9-4bc3-aee5-9e4db1b89345/resourcegroups/olonok-ml/providers/microsoft.machinelearningservices/workspaces/azure-ml-2/onlineendpoints/endpt-vivit-inference-onnx', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/5a8ec57c-47f9-4bc3-aee5-9e4db1b89345/providers/Microsoft.MachineLearningServices/locations/uksouth/mfeOperationsStatus/oeidp:31ae886d-ee28-4a4f-af29-64989f2a9076:c8a58ebb-305e-479b-a21c-0366f0e399ee?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/5a8ec57c-47f9-4bc3-aee5-9e4db1b89345/resourceGroups/olonok-ml/providers/Microsoft.MachineLearningServices/workspaces/azure-ml-2/onlineEndpoints/endpt-vivit-inference-onnx', 'Resource__source_path': None, 'base_path': 'D:\\\\repos\\\\onnx\\\\azureml', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000016AC251C9D0>, 'auth_mode': 'key', 'location': 'uksouth', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x0000016AC0B84050>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#endpoint_name = \"endpt-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name = endpoint_name, \n",
    "    description=\"this is a endpoint for onnx inference Video ViT classification model\",\n",
    "    auth_mode=\"key\"\n",
    ")\n",
    "\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).wait()\n",
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = \"mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1:13\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Env from local Dockerfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(\n",
    "    build=BuildContext(path=\"docker\"),\n",
    "    name=\"docker-vivit\",\n",
    "    description=\"Environment Vivit.\",\n",
    ")\n",
    "env_log= ml_client.environments.create_or_update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Environment({'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'docker-vivit', 'description': 'Environment Vivit.', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': True, 'id': '/subscriptions/5a8ec57c-47f9-4bc3-aee5-9e4db1b89345/resourceGroups/olonok-ml/providers/Microsoft.MachineLearningServices/workspaces/azure-ml-2/environments/docker-vivit/versions/3', 'Resource__source_path': None, 'base_path': 'D:\\\\repos\\\\onnx\\\\azureml', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000016ABFDBD5D0>, 'serialize': <msrest.serialization.Serializer object at 0x0000016AC0A37E50>, 'version': '3', 'latest_version': None, 'conda_file': None, 'image': None, 'build': <azure.ai.ml.entities._assets.environment.BuildContext object at 0x0000016AC251E990>, 'inference_config': None, 'os_type': 'Linux', 'arm_type': 'environment_version', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': None})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_log "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in ml_client.models.list():\n",
    "#     print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for env1 in ml_client.environments.list():\n",
    "#     print(env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = \"mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1:13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1684439751829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# env = Environment(\n",
    "#     conda_file=\"./conda.yml\",\n",
    "#     image=image\n",
    "# )\n",
    "# env\n",
    "model = ml_client.models.get(name=\"vivit_classifier\", version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Environment({'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'docker-vivit', 'description': 'Environment Vivit.', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': 'D:\\\\repos\\\\onnx\\\\azureml', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000016AC25DDD50>, 'version': '2', 'latest_version': None, 'conda_file': None, 'image': None, 'build': <azure.ai.ml.entities._assets.environment.BuildContext object at 0x0000016ABFD16BD0>, 'inference_config': None, 'os_type': None, 'arm_type': 'environment_version', 'conda_file_path': None, 'path': WindowsPath('D:/repos/onnx/azureml/docker'), 'datastore': None, 'upload_hash': None, 'translated_conda_file': None})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ml_client.models.download(name=\"onnx_emotion\", version = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### 9.3 Deploy scoring file to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'endpt-vivit-inference-onnx'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Env\n",
    "env = ml_client.environments.get(name=\"docker-vivit\", version=\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azure.ai.ml.entities._assets.environment.Environment"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Environment({'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'docker-vivit', 'description': 'Environment Vivit.', 'tags': {}, 'properties': {'azureml.labels': ''}, 'print_as_yaml': True, 'id': '/subscriptions/5a8ec57c-47f9-4bc3-aee5-9e4db1b89345/resourceGroups/olonok-ml/providers/Microsoft.MachineLearningServices/workspaces/azure-ml-2/environments/docker-vivit/versions/2', 'Resource__source_path': None, 'base_path': 'D:\\\\repos\\\\onnx\\\\azureml', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000016AAFBA7550>, 'serialize': <msrest.serialization.Serializer object at 0x0000016AC0A3B910>, 'version': '2', 'latest_version': None, 'conda_file': None, 'image': None, 'build': <azure.ai.ml.entities._assets.environment.BuildContext object at 0x0000016AC241B350>, 'inference_config': None, 'os_type': 'Linux', 'arm_type': 'environment_version', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': None})"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1684439501278
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint endpt-vivit-inference-onnx exists\n",
      "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
      "\n",
      "Example: azcopy copy 'D:\\repos\\onnx\\azureml\\onnx_vivit' 'https://azureml27454141413.blob.core.windows.net/31ae886d-ee28-4a4f-af29-64989f2a9076-pty30yvn6u01ek8f2ipgusn14s/onnx_vivit' \n",
      "\n",
      "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
      "Uploading onnx_vivit (348.31 MBs): 100%|#########################################################################################| 348305750/348305750 [09:37<00:00, 602982.95it/s]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x16ac0ba4b10>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................................................................................................."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"vivit-onnx\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"./onnx_vivit\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    instance_type=\"Standard_NC4as_T4_v3\",\n",
    "    instance_count=1,\n",
    ")\n",
    "\n",
    "ml_client.online_deployments.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1684906894100
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "KEY = \"Z9IODQreZDexj45xfwSyQkZPvpzCpHWM\"\n",
    "# Documentation: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints\n",
    "# Troubleshooting: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./image/6540601-uhd_2560_1440_25fps.mp4\"\n",
    "with open(file_name, \"rb\") as f:\n",
    "    video = f.read()\n",
    "    f.close()\n",
    "    im_b64 = base64.b64encode(video).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13705632"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(im_b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "input_data = json.dumps({'data': im_b64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from transformers import VivitImageProcessor\n",
    "import sys\n",
    "import os\n",
    "from azureml.core.model import Model\n",
    "import time\n",
    "import json\n",
    "import av\n",
    "import base64\n",
    "\n",
    "\n",
    "def init():\n",
    "    global session, input_name, output_name, image_processor, label_dic\n",
    "    model = \"onnx_vivit/vivit.onnx\"\n",
    "    # Load the model in onnx runtime to start the session    \n",
    "    session = onnxruntime.InferenceSession(model, providers=[\"CPUExecutionProvider\"])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name \n",
    "    image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "    label_dic = {'ApplyEyeMakeup':0, 'ApplyLipstick':1, 'Archery':2, 'BabyCrawling':3, 'BalanceBeam':4, 'BandMarching':5, \n",
    "             'BaseballPitch':6, 'Basketball':7,'BasketballDunk':8, 'BenchPress':9}\n",
    "    \n",
    "def run(input_data):\n",
    "    '''Purpose: evaluate test input in Azure Cloud using onnxruntime.\n",
    "        We will call the run function later from our Jupyter Notebook \n",
    "        so our azure service can evaluate our model input in the cloud. '''\n",
    "\n",
    "    try:\n",
    "        # We expect a video im base64 format in the attribute data. Load the data and decode the base64\n",
    "        requests_json = json.loads(input_data.encode())\n",
    "        v = base64.b64decode(requests_json.get(\"data\").encode(\"utf-8\"))\n",
    "        #create container from bytes request\n",
    "        container= av.open(io.BytesIO(v))\n",
    "        #Sample Frames\n",
    "        indices = sample_frame_indices(clip_len=10, frame_sample_rate=1,seg_len=container.streams.video[0].frames)\n",
    "        # create video with sampled frames\n",
    "        video = read_video_pyav(container=container, indices=indices)\n",
    "        # tokenize video\n",
    "        data = np.array(image_processor(list(video), return_tensors=\"pt\")['pixel_values'])\n",
    "        #### INFERENCE ONNX #####\n",
    "        # pass input data to do model inference with ONNX Runtime\n",
    "        start = time.time()\n",
    "        r = session.run([output_name], {input_name : data})\n",
    "        end = time.time()\n",
    "        probabilities, predicted_class = postprocess(r[0])\n",
    "        # predicted class and label\n",
    "        class_label = predicted_class[0]\n",
    "        label = get_key(label_dic, class_label)\n",
    "        \n",
    "        result = label_map(probabilities)\n",
    "        result['predicted_label'] = label\n",
    "        result['predicted_calss'] = int(class_label)\n",
    "        \n",
    "        result_dict = {\"result\": result,\n",
    "                      \"time_in_sec\": [end - start]}\n",
    "    except Exception as e:\n",
    "        result_dict = {\"error\": str(e)}\n",
    "    \n",
    "    return json.dumps(result_dict)\n",
    "\n",
    "\n",
    "\n",
    "def label_map(probs, threshold=.5):\n",
    "    \"\"\"Take the most probable labels (output of postprocess) and returns the \n",
    "    probs of each label.\"\"\"\n",
    "    # labels and dictionary to \n",
    "    label_dic = {'ApplyEyeMakeup':0, 'ApplyLipstick':1, 'Archery':2, 'BabyCrawling':3, 'BalanceBeam':4, 'BandMarching':5, \n",
    "             'BaseballPitch':6, 'Basketball':7,'BasketballDunk':8, 'BenchPress':9}\n",
    "    output_probs = {}\n",
    "    image_preds = {}\n",
    "    for prob, key in zip(probs[0], range(0, len(probs[0]))):\n",
    "        label = get_key(label_dic, key)\n",
    "        output_probs[label] = float(prob)\n",
    "\n",
    "    image_preds[\"class_probabilities\"] = output_probs\n",
    "    return image_preds\n",
    "\n",
    "def get_key(dict, value):\n",
    "    \"\"\"\n",
    "    return key given a value. From a dictionary\n",
    "    \"\"\"\n",
    "    for key, val in dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return \"Value not found\"\n",
    "\n",
    "def postprocess(scores):\n",
    "    \"\"\"This function takes the scores generated by the network and \n",
    "    returns the class IDs in decreasing order of probability.\"\"\"\n",
    "    logits = np.array(scores)\n",
    "    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "    predicted_class = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    return probabilities, predicted_class\n",
    "\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            reformatted_frame = frame.reformat(width=224,height=224)\n",
    "            frames.append(reformatted_frame)\n",
    "    new=np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"result\": {\"class_probabilities\": {\"ApplyEyeMakeup\": 0.004351954907178879, \"ApplyLipstick\": 0.003930356819182634, \"Archery\": 0.8199613094329834, \"BabyCrawling\": 0.003192294854670763, \"BalanceBeam\": 0.01228354312479496, \"BandMarching\": 0.048293791711330414, \"BaseballPitch\": 0.0017063587438315153, \"Basketball\": 0.00959216058254242, \"BasketballDunk\": 0.002194389933720231, \"BenchPress\": 0.09449388831853867}, \"predicted_label\": \"Archery\", \"predicted_calss\": 2}, \"time_in_sec\": [1.792248249053955]}'"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(input_data)\n",
    "# body.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = str.encode(input_data)\n",
    "url = 'https://endpt-vivit-inference-onnx.uksouth.inference.ml.azure.com/score'\n",
    "# Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
    "api_key = KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\"{\\\\\"result\\\\\": {\\\\\"class_probabilities\\\\\": {\\\\\"ApplyEyeMakeup\\\\\": 0.0026463675312697887, \\\\\"ApplyLipstick\\\\\": 0.0013310174690559506, \\\\\"Archery\\\\\": 0.9078341722488403, \\\\\"BabyCrawling\\\\\": 0.001594557543285191, \\\\\"BalanceBeam\\\\\": 0.0021795297507196665, \\\\\"BandMarching\\\\\": 0.0377991758286953, \\\\\"BaseballPitch\\\\\": 0.0008338856277987361, \\\\\"Basketball\\\\\": 0.004699265118688345, \\\\\"BasketballDunk\\\\\": 0.001007644459605217, \\\\\"BenchPress\\\\\": 0.04007432982325554}, \\\\\"predicted_label\\\\\": \\\\\"Archery\\\\\", \\\\\"predicted_calss\\\\\": 2}, \\\\\"time_in_sec\\\\\": [1.5316495895385742]}\"'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'vivit-onnx' }\n",
    "start = time.time()\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.342873096466064"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"result\": {\"class_probabilities\": {\"ApplyEyeMakeup\": 0.0026463675312697887, \"ApplyLipstick\": 0.0013310174690559506, \"Archery\": 0.9078341722488403, \"BabyCrawling\": 0.001594557543285191, \"BalanceBeam\": 0.0021795297507196665, \"BandMarching\": 0.0377991758286953, \"BaseballPitch\": 0.0008338856277987361, \"Basketball\": 0.004699265118688345, \"BasketballDunk\": 0.001007644459605217, \"BenchPress\": 0.04007432982325554}, \"predicted_label\": \"Archery\", \"predicted_calss\": 2}, \"time_in_sec\": [1.5316495895385742]}'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(result.decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### 9.5: Delete the online endpoint\n",
    "Don't forget to delete the online endpoint, else you will leave the billing meter running for the compute used by the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1684306030654
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................"
     ]
    }
   ],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=endpoint_name).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python (azure_ml)",
   "language": "python",
   "name": "azure_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
