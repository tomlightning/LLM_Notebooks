{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/openai/TikToken_count_tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tiktoken\n",
        "\n",
        "https://github.com/openai/tiktoken\n",
        "\n",
        "# OpenAi Pricing\n",
        "https://openai.com/api/pricing/\n",
        "\n",
        "\n",
        "# How OpenAi Count tokens\n",
        "https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
        "\n",
        "https://platform.openai.com/tokenizer\n"
      ],
      "metadata": {
        "id": "gi-IIkAE-U3P"
      },
      "id": "gi-IIkAE-U3P"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken openai -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02LF184Dqg9H",
        "outputId": "f922ecf9-a1c3-4dd9-82f5-4b1216876e8f"
      },
      "id": "02LF184Dqg9H",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2b65de9a",
      "metadata": {
        "id": "2b65de9a"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('KEY_OPENAI')"
      ],
      "metadata": {
        "id": "RXH8QevB724l"
      },
      "id": "RXH8QevB724l",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"You are a fantastic coker\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"Puedes decirme como prepara bakalado a la Vizcaina?\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "  ],\n",
        "  temperature=1,\n",
        "  max_tokens=512,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")"
      ],
      "metadata": {
        "id": "VBXG4wCj7m8U"
      },
      "id": "VBXG4wCj7m8U",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.choices[0].message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZqVSq6OJFaB",
        "outputId": "249e5105-98c0-4f03-f723-7c0aa610d8f7"
      },
      "id": "rZqVSq6OJFaB",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content='¡Por supuesto! El bacalao a la vizcaína es un plato tradicional de la cocina vasca, muy sabroso y lleno de historia. Aquí tienes una receta básica para prepararlo:\\n\\n### Ingredientes:\\n- 1 kg de bacalao desalado\\n- 3 cebollas\\n- 3 dientes de ajo\\n- 2-3 pimientos choriceros secos\\n- 400 g tomates maduros o 1 lata de tomates triturados\\n- 1 hoja de laurel\\n- Aceite de oliva virgen extra\\n- Sal\\n- Pimienta\\n- Agua\\n\\n### Instrucciones:\\n\\n1. **Desalar el bacalao:**\\n   - Si tienes bacalao seco, deberás desalarlo antes de cocinarlo. Para ello, colócalo en un recipiente con agua fría y cámbiala cada 8 horas durante al menos 48 horas. Guarda el recipiente en el refrigerador para evitar que se dañe.\\n\\n2. **Preparar los pimientos choriceros:**\\n   - Remoja los pimientos choriceros en agua caliente durante al menos 2 horas o hasta que estén blandos. Luego, ábrelos, retira las semillas y raspa la carne con una cuchara. Reserva esta carne de pimiento.\\n\\n3. **Hacer la salsa vizcaína:**\\n   - Pica finamente las cebollas y los ajos.\\n   - En una sartén grande o cazuela, calienta un buen chorro de aceite de oliva.\\n   - Sofríe las cebollas a fuego medio-lento hasta que estén bien pochadas y caramelizadas, lo que puede llevar unos 30-40 minutos.\\n   - Añade los ajos picados y sofríe por unos minutos más.\\n   - Agrega los tomates pelados y troceados (o tomate triturado) y cocina todo junto a fuego lento hasta que se forme una salsa espesa (unos 20-30 minutos).\\n   - Añade la carne de los pimientos choriceros y mezcla bien.\\n   - Añade la hoja de laurel y ajusta de sal y pimienta.\\n   - Si la salsa queda muy espesa, puedes añadir un poco de agua.\\n\\n4. **Cocinar el bacalao:**\\n   - Mientras la salsa sigue cocinándose, corta el bacalao desalado en trozos grandes.\\n   - En otra sartén, calienta un poco de aceite de oliva', role='assistant', function_call=None, tool_calls=None)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.usage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-FL0P1X9d2C",
        "outputId": "db4d0a78-8abe-46da-a2ee-b86292560d74"
      },
      "id": "N-FL0P1X9d2C",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletionUsage(completion_tokens=512, prompt_tokens=30, total_tokens=542)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(response.usage.prompt_tokens * 5) /10**6 +   (response.usage.completion_tokens * 15) /10**6"
      ],
      "metadata": {
        "id": "61ZjKlBj-Fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f97410d-24a6-4274-acbd-48f503bdcb14"
      },
      "id": "61ZjKlBj-Fab",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00783"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "204b8a05",
      "metadata": {
        "id": "204b8a05"
      },
      "outputs": [],
      "source": [
        "# enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = \"\"\"role system text: \"You are a fantastic coker\",role user text: \"Puedes decirme como prepara bakalado a la Vizcaina?\"\n",
        "\"\"\"\n",
        "len(enc.encode(m))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOdsA_TXEaXt",
        "outputId": "fe92fdad-cf88-4a93-bef8-a3650cecc5da"
      },
      "id": "FOdsA_TXEaXt",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a86fc50",
      "metadata": {
        "id": "9a86fc50"
      },
      "source": [
        "## Counting Tokens In Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6db00f7c",
      "metadata": {
        "id": "6db00f7c"
      },
      "outputs": [],
      "source": [
        "book_text = \"\"\"\n",
        "Mrs. Darling quivered and went to the window. It was securely fastened.\n",
        "She looked out, and the night was peppered with stars. They were\n",
        "crowding round the house, as if curious to see what was to take place\n",
        "there, but she did not notice this, nor that one or two of the smaller\n",
        "ones winked at her. Yet a nameless fear clutched at her heart and made\n",
        "her cry, “Oh, how I wish that I wasn’t going to a party to-night!”\n",
        "\n",
        "Even Michael, already half asleep, knew that she was perturbed, and he\n",
        "asked, “Can anything harm us, mother, after the night-lights are lit?”\n",
        "\n",
        "“Nothing, precious,” she said; “they are the eyes a mother leaves\n",
        "behind her to guard her children.”\n",
        "\n",
        "She went from bed to bed singing enchantments over them, and little\n",
        "Michael flung his arms round her. “Mother,” he cried, “I’m glad of\n",
        "you.” They were the last words she was to hear from him for a long\n",
        "time.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.encode(book_text)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ruoaVNHDtQW",
        "outputId": "9492bd5c-9774-4692-f0e0-76f6cc0ba0a7"
      },
      "id": "7ruoaVNHDtQW",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[198, 91702, 13, 165836, 474, 62076, 326, 5981, 316, 290]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8468368c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8468368c",
        "outputId": "fd9c10e7-a1ca-48d9-c5e6-74ce6c5b44bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "222"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(enc.encode(book_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "602ac508",
      "metadata": {
        "id": "602ac508"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "541f6992",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "541f6992",
        "outputId": "055ae5a9-72f2-43a7-fc85-89464d7beb4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "num_tokens_from_string(book_text, \"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1b900348",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b900348",
        "outputId": "63ab1987-40ae-4692-f408-be8ddf6e8317"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "num_tokens_from_string(book_text, \"text-davinci-003\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b878a6ec",
      "metadata": {
        "id": "b878a6ec"
      },
      "source": [
        "## Counting Tokens in Messages (for Chat API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "95fff974",
      "metadata": {
        "id": "95fff974"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo\":\n",
        "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
        "    elif model == \"gpt-4\":\n",
        "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
        "    elif model == \"gpt-3.5-turbo-0301\":\n",
        "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
        "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
        "    elif model == \"gpt-4-0314\":\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    else:\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "26c41d20",
      "metadata": {
        "id": "26c41d20"
      },
      "outputs": [],
      "source": [
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"New synergies will help drive top-line growth.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Things working well together will increase revenue.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
        "    },\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1603d1c0",
      "metadata": {
        "id": "1603d1c0",
        "outputId": "93910395-ec6b-418e-da4f-5d5d253d782d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "num_tokens_from_messages(example_messages, \"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a20c8deb",
      "metadata": {
        "scrolled": false,
        "id": "a20c8deb",
        "outputId": "79a9a565-61de-4ec8-9d91-d5ee27434e5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "129"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "num_tokens_from_messages(example_messages, \"gpt-4\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}