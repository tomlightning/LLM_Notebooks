{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/openai/TikToken_count_tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tiktoken\n",
        "\n",
        "https://github.com/openai/tiktoken\n",
        "\n",
        "# OpenAi Pricing\n",
        "https://openai.com/api/pricing/\n",
        "\n",
        "\n",
        "# How OpenAi Count tokens\n",
        "https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
        "\n",
        "https://platform.openai.com/tokenizer\n"
      ],
      "metadata": {
        "id": "gi-IIkAE-U3P"
      },
      "id": "gi-IIkAE-U3P"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken openai -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02LF184Dqg9H",
        "outputId": "f922ecf9-a1c3-4dd9-82f5-4b1216876e8f"
      },
      "id": "02LF184Dqg9H",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2b65de9a",
      "metadata": {
        "id": "2b65de9a"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('KEY_OPENAI')"
      ],
      "metadata": {
        "id": "RXH8QevB724l"
      },
      "id": "RXH8QevB724l",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"You are a fantastic coker\"\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": [\n",
        "        {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"Puedes decirme como prepara bakalado a la Vizcaina? return a list of bulletpoints\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "  ],\n",
        "  temperature=1,\n",
        "  max_tokens=512,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")"
      ],
      "metadata": {
        "id": "VBXG4wCj7m8U"
      },
      "id": "VBXG4wCj7m8U",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.choices[0].finish_reason"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YNKyYO-YaLmQ",
        "outputId": "fff943ff-5c39-44a6-d643-4b22bf3d15da"
      },
      "id": "YNKyYO-YaLmQ",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'length'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZqVSq6OJFaB",
        "outputId": "578d7bb0-05f0-42b9-82e4-7aa897c63967"
      },
      "id": "rZqVSq6OJFaB",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Claro! Aquí tienes una lista de los pasos para preparar Bacalao a la Vizcaína:\n",
            "\n",
            "- **Ingredientes principales:**\n",
            "  - 1 kg de bacalao desalado\n",
            "  - 4 cebollas grandes\n",
            "  - 3 dientes de ajo\n",
            "  - 6 tomates maduros\n",
            "  - 2 pimientos choriceros secos\n",
            "  - 150 ml de aceite de oliva\n",
            "  - Sal y pimienta al gusto\n",
            "\n",
            "- **Procedimiento:**\n",
            "  - **Desalar el bacalao:**\n",
            "    1. **Remojo:** Coloca el bacalao en un recipiente con agua fría durante al menos 24-48 horas, cambiando el agua cada 8 horas para eliminar la sal.\n",
            "    2. **Escurrir y secar:** Una vez desalado, escurre bien y seca el bacalao con papel absorbente.\n",
            "\n",
            "  - **Preparar la salsa Vizcaína:**\n",
            "    3. **Pimientos choriceros:** Hidrata los pimientos choriceros en agua caliente por unos 20-30 minutos. Luego, retira las semillas y la piel, y extrae la pulpa con una cuchara.\n",
            "    4. **Cebollas y ajos:** Pela y pica finamente las cebollas y los ajos.\n",
            "    5. **Tomates:** Pela y corta los tomates en pequeños trozos o tritura.\n",
            "    6. **Sofreír:** Calienta el aceite de oliva en una sartén grande a fuego medio. Añade las cebollas y sofríe hasta que estén caramelizadas.\n",
            "    7. **Ajos y tomates:** Añade los ajos picados a la sartén y cocina por unos 2 minutos. Luego, incorpora los tomates y cocina hasta que se desintegren y formen una salsa espesa.\n",
            "    8. **Pulpa de pimientos:** Añade la pulpa de pimientos choriceros a la salsa y mezcla bien. Cocina por unos 10 minutos más.\n",
            "    9. **Condimentar:** Sazona la salsa con sal y pimienta al gusto.\n",
            "\n",
            "  - **Cocinar el bacalao:**\n",
            "    10. **Incorporar a la salsa:** Añade los trozos de bacalao a la sartén con la salsa Vizcaína.\n",
            "    11. **Cocción:** Cocina a fuego lento durante unos 15-20 minutos,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.usage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-FL0P1X9d2C",
        "outputId": "ecf63cd5-b157-4a52-dd92-115014dc5fd7"
      },
      "id": "N-FL0P1X9d2C",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletionUsage(completion_tokens=512, prompt_tokens=36, total_tokens=548)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(response.usage.prompt_tokens * 5) /10**6 +   (response.usage.completion_tokens * 15) /10**6"
      ],
      "metadata": {
        "id": "61ZjKlBj-Fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aeeaae9-90cd-49a2-b50e-92f75cccbdb2"
      },
      "id": "61ZjKlBj-Fab",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00786"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "204b8a05",
      "metadata": {
        "id": "204b8a05"
      },
      "outputs": [],
      "source": [
        "# enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = \"\"\"role system text: \"You are a fantastic coker\",role user text: \"Puedes decirme como prepara bakalado a la Vizcaina?\"\n",
        "\"\"\"\n",
        "len(enc.encode(m))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOdsA_TXEaXt",
        "outputId": "fe92fdad-cf88-4a93-bef8-a3650cecc5da"
      },
      "id": "FOdsA_TXEaXt",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a86fc50",
      "metadata": {
        "id": "9a86fc50"
      },
      "source": [
        "## Counting Tokens In Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6db00f7c",
      "metadata": {
        "id": "6db00f7c"
      },
      "outputs": [],
      "source": [
        "book_text = \"\"\"\n",
        "Mrs. Darling quivered and went to the window. It was securely fastened.\n",
        "She looked out, and the night was peppered with stars. They were\n",
        "crowding round the house, as if curious to see what was to take place\n",
        "there, but she did not notice this, nor that one or two of the smaller\n",
        "ones winked at her. Yet a nameless fear clutched at her heart and made\n",
        "her cry, “Oh, how I wish that I wasn’t going to a party to-night!”\n",
        "\n",
        "Even Michael, already half asleep, knew that she was perturbed, and he\n",
        "asked, “Can anything harm us, mother, after the night-lights are lit?”\n",
        "\n",
        "“Nothing, precious,” she said; “they are the eyes a mother leaves\n",
        "behind her to guard her children.”\n",
        "\n",
        "She went from bed to bed singing enchantments over them, and little\n",
        "Michael flung his arms round her. “Mother,” he cried, “I’m glad of\n",
        "you.” They were the last words she was to hear from him for a long\n",
        "time.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.encode(book_text)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ruoaVNHDtQW",
        "outputId": "9492bd5c-9774-4692-f0e0-76f6cc0ba0a7"
      },
      "id": "7ruoaVNHDtQW",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[198, 91702, 13, 165836, 474, 62076, 326, 5981, 316, 290]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8468368c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8468368c",
        "outputId": "fd9c10e7-a1ca-48d9-c5e6-74ce6c5b44bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "222"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(enc.encode(book_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "602ac508",
      "metadata": {
        "id": "602ac508"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_string(string: str, model_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "541f6992",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "541f6992",
        "outputId": "055ae5a9-72f2-43a7-fc85-89464d7beb4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "num_tokens_from_string(book_text, \"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1b900348",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b900348",
        "outputId": "63ab1987-40ae-4692-f408-be8ddf6e8317"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "num_tokens_from_string(book_text, \"text-davinci-003\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b878a6ec",
      "metadata": {
        "id": "b878a6ec"
      },
      "source": [
        "## Counting Tokens in Messages (for Chat API)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "95fff974",
      "metadata": {
        "id": "95fff974"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo\":\n",
        "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
        "    elif model == \"gpt-4\":\n",
        "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
        "    elif model == \"gpt-3.5-turbo-0301\":\n",
        "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
        "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
        "    elif model == \"gpt-4o\":\n",
        "        tokens_per_message = 3  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
        "        tokens_per_name = 1  # if there's a name, the role is omitted\n",
        "    elif model == \"gpt-4-0314\":\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    else:\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "26c41d20",
      "metadata": {
        "id": "26c41d20"
      },
      "outputs": [],
      "source": [
        "example_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"New synergies will help drive top-line growth.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Things working well together will increase revenue.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_user\",\n",
        "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"name\": \"example_assistant\",\n",
        "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
        "    },\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "1603d1c0",
      "metadata": {
        "id": "1603d1c0",
        "outputId": "02ede7ec-d9a1-4dde-e627-f28f1ab091d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "num_tokens_from_messages(example_messages, \"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "a20c8deb",
      "metadata": {
        "scrolled": false,
        "id": "a20c8deb",
        "outputId": "6b61d4ee-c937-4630-c049-82a2b902e7b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "129"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "num_tokens_from_messages(example_messages, \"gpt-4\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_from_messages(example_messages, \"gpt-4o\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ5eG20_cJPN",
        "outputId": "30fcfbfb-46a3-44d2-901f-8b7f53f0730e"
      },
      "id": "eQ5eG20_cJPN",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UmzP9E9YcJIR"
      },
      "id": "UmzP9E9YcJIR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}