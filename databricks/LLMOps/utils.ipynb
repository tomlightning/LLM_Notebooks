{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9053f2e2-37f0-47a9-84bf-5e8f544d9034",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32887361-4096-458d-ab49-6c0a28c093e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  Scrape and return the transcript of specific federal documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ee4d5aa-5ca1-4dc8-94ea-6b95512bd61c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_federal_document(url, div_class):\n",
    "    \"\"\"\n",
    "    Scrapes the transcript of the Act Establishing Yellowstone National Park from the given URL.\n",
    "\n",
    "    Args:\n",
    "    url (str): URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "    str: The transcript text of the Act.\n",
    "    \"\"\"\n",
    "    # Sending a request to the URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Parsing the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Finding the transcript section by its HTML structure\n",
    "        transcript_section = soup.find(\"div\", class_=div_class)\n",
    "        if transcript_section:\n",
    "            transcript_text = transcript_section.get_text(separator=\"\\n\", strip=True)\n",
    "            return transcript_text\n",
    "        else:\n",
    "            return \"Transcript section not found.\"\n",
    "    else:\n",
    "        return f\"Failed to retrieve the webpage. Status code: {response.status_code}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e102c0ba-4f27-4c71-94d4-4e800c097f5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Retrieve documents from specified URLs.\n",
    "\n",
    "### Creating a FAISS database from the documents saved in fetch_and_save_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c555c998-c5a4-4dd3-be15-5d485c00cd97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetch_and_save_documents(url_list, doc_path):\n",
    "    \"\"\"\n",
    "    Fetches documents from given URLs and saves them to a specified file path.\n",
    "\n",
    "    Args:\n",
    "        url_list (list): List of URLs to fetch documents from.\n",
    "        doc_path (str): Path to the file where documents will be saved.\n",
    "    \"\"\"\n",
    "    for url in url_list:\n",
    "        document = fetch_federal_document(url, \"col-sm-9\")\n",
    "        with open(doc_path, \"a\") as file:\n",
    "            file.write(document)\n",
    "\n",
    "\n",
    "def create_faiss_database(document_path, database_save_directory, chunk_size=500, chunk_overlap=10):\n",
    "    \"\"\"\n",
    "    Creates and saves a FAISS database using documents from the specified file.\n",
    "\n",
    "    Args:\n",
    "        document_path (str): Path to the file containing documents.\n",
    "        database_save_directory (str): Directory where the FAISS database will be saved.\n",
    "        chunk_size (int, optional): Size of each document chunk. Default is 500.\n",
    "        chunk_overlap (int, optional): Overlap between consecutive chunks. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        FAISS database instance.\n",
    "    \"\"\"\n",
    "    # Load documents from the specified file\n",
    "    document_loader = TextLoader(document_path)\n",
    "    raw_documents = document_loader.load()\n",
    "\n",
    "    # Split documents into smaller chunks with specified size and overlap\n",
    "    document_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    document_chunks = document_splitter.split_documents(raw_documents)\n",
    "\n",
    "    # Generate embeddings for each document chunk\n",
    "    embedding_generator = OpenAIEmbeddings()\n",
    "    faiss_database = FAISS.from_documents(document_chunks, embedding_generator)\n",
    "\n",
    "    # Save the FAISS database to the specified directory\n",
    "    faiss_database.save_local(database_save_directory)\n",
    "\n",
    "    return faiss_database\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
