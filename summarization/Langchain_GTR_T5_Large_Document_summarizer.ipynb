{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1eZ5X_ZyoqE3tO1WRAGZkIoojBLchKWar",
      "authorship_tag": "ABX9TyMizxwy2T5BwVLKA5Q0OGwQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f88848b3dfb049dfa5018b4f4773972e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b8a272f36904afbae4a3f273870e57b",
              "IPY_MODEL_01db4283a4444469a424140d7c040fff",
              "IPY_MODEL_c86623eac8b94feb9d9c52f7c2cee4cf"
            ],
            "layout": "IPY_MODEL_21b7a30d2d16413a99a545c9e4633101"
          }
        },
        "1b8a272f36904afbae4a3f273870e57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b6024f755084993a5c22b6769d81b3a",
            "placeholder": "​",
            "style": "IPY_MODEL_b02bf6e580504d13abcd60eafe17ef81",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "01db4283a4444469a424140d7c040fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_258d6b6592ac4c3883bc49cc5997cee2",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5688b7ce530f4d0a93a0592adb77aa80",
            "value": 3
          }
        },
        "c86623eac8b94feb9d9c52f7c2cee4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfb6e04d18b9421fbd0f5bdb895c3b8e",
            "placeholder": "​",
            "style": "IPY_MODEL_71f13753fcb244c28e4a21ee70614d72",
            "value": " 3/3 [00:53&lt;00:00, 17.75s/it]"
          }
        },
        "21b7a30d2d16413a99a545c9e4633101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b6024f755084993a5c22b6769d81b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b02bf6e580504d13abcd60eafe17ef81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "258d6b6592ac4c3883bc49cc5997cee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5688b7ce530f4d0a93a0592adb77aa80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfb6e04d18b9421fbd0f5bdb895c3b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f13753fcb244c28e4a21ee70614d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/summarization/Langchain_GTR_T5_Large_Document_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3LFaHkF9AgK",
        "outputId": "f98c7b1b-57d6-4e73-80b8-55fc0b31aff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.34.0 --progress-bar off\n",
        "!pip install -qqq accelerate==0.23.0 --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.41.1 --progress-bar off\n",
        "!pip install sentence-transformers spacy langchain trl datasets pypdf -qqq --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "\n",
        "# Loaders\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Model\n",
        "\n",
        "\n",
        "# Embedding Support\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Summarizer we'll use for Map Reduce\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "#splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=30, tokens_per_chunk=512, model_name= \"sentence-transformers/gtr-t5-large\")"
      ],
      "metadata": {
        "id": "ip_uhu9L9kof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6rUNtCzKsyR",
        "outputId": "42ce0f05-63b1-4ce0-aa47-a48afebc7f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.get(\"https://www.gutenberg.org/cache/epub/64317/pg64317.txt\")"
      ],
      "metadata": {
        "id": "qDSb12ivAsnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_complete_text = response.text"
      ],
      "metadata": {
        "id": "1X7_iELMA24V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_complete_text = book_complete_text[5:]"
      ],
      "metadata": {
        "id": "tVp1hwM7yGRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(book_complete_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKgdWQqCA4fl",
        "outputId": "9a1e0bd0-8eb8-4958-c0cd-052217312c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "296884"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/data/book.txt\""
      ],
      "metadata": {
        "id": "COC8KgjlA-IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(book_complete_text)"
      ],
      "metadata": {
        "id": "MB1lTJ1RA7e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(file_path, \"r\",  encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "HxPVavEf9klX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.replace('\\t', ' ')"
      ],
      "metadata": {
        "id": "ZX6MHp-ky-_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHMtl9KgzVU0",
        "outputId": "94892aa9-3f4a-457d-b347-54abf17d1f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "290101"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\t\"], chunk_size=12000, chunk_overlap=3000)\n",
        "\n",
        "docs = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "KKaIuJi9BKON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_documents = len(docs)\n",
        "\n",
        "print (f\"Now our book is split up into {num_documents} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaSxJgZWBKK7",
        "outputId": "3529bba3-2e0c-4359-ed13-6885b0dd8ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now our book is split up into 32 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vectors = embeddings.embed_documents([x.page_content for x in docs])"
      ],
      "metadata": {
        "id": "Kj0a9QspBKHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectors[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnjkLByMBKDF",
        "outputId": "d31c0d51-4e27-4c31-a74a-1cb263c849d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = int(len(vectors) // 4)\n",
        "num_clusters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9WqZHJGEhJQ",
        "outputId": "4ef4c235-1e05-4a05-a4bd-ac195c2d32bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'embeddings' is a list or array of 768-dimensional embeddings\n",
        "\n",
        "# Choose the number of clusters, this can be adjusted based on the book's content.\n",
        "# I played around and found ~10 was the best.\n",
        "# Usually if you have 10 passages from a book you can tell what it's about\n",
        "num_clusters = 5 if num_clusters <=5 else num_clusters\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)"
      ],
      "metadata": {
        "id": "A82raZvaB2NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.labels_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk5VRB78B2KG",
        "outputId": "80a20a2f-81a3-4707-a6f8-95dbbf4eedd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 3, 1, 1, 1, 1, 1, 3, 4, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1,\n",
              "       2, 4, 3, 3, 1, 3, 3, 5, 0, 7], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(kmeans.labels_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8VSz9TIB2HZ",
        "outputId": "b9109af7-3e2a-4d20-83e4-e8e4ce4544f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Taking out the warnings\n",
        "import warnings\n",
        "from warnings import simplefilter\n",
        "\n",
        "# Filter out FutureWarnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Perform t-SNE and reduce to 2 dimensions\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_data_tsne = tsne.fit_transform(np.array(vectors))\n",
        "\n",
        "# Plot the reduced data\n",
        "plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=kmeans.labels_)\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('Book Embeddings Clustered')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "kjl4n0fHB2En",
        "outputId": "dc4205da-3886-4dcb-91b5-a35ae7757f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHHCAYAAABTMjf2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABe2UlEQVR4nO3deVhUZf8G8PvMAMMmq8iioAgK7msmuOKGS5qpae6Wlr6Z5pKplamvlq+/LK3M9nK33LdyzT13w0JTFMQNZJFtkJ2Z5/eHMYkww6DMDMPcn+uaq+bMc875zhxkbp7znOdIQggBIiIiIgskM3UBRERERKbCIEREREQWi0GIiIiILBaDEBEREVksBiEiIiKyWAxCREREZLEYhIiIiMhiMQgRERGRxWIQIiIiIovFIERkZurUqYPnnnvO1GXodOTIEUiShM2bNxt8X/PmzYMkSXq1lSQJ8+bN0zxfuXIlJEnCzZs3DVOcERV95keOHDF1KSbx+LEl0heDEFE5FX15PvqoUaMGwsLCsGfPHlOXp1Pnzp1L1F70CA4ONnV5pMW2bdvQq1cvVK9eHTY2NvDx8cHgwYNx6NAho9Vw8uRJzJs3D+np6UbbJ5ExWJm6ACJz9d///hf+/v4QQiAxMRErV65E7969sWvXrkrdY1OrVi0sWrSoxHJnZ2cTVGNaI0eOxEsvvQSFQmHqUkolhMArr7yClStXokWLFpg2bRq8vLxw7949bNu2DV27dsXvv/+O0NBQg9dy8uRJzJ8/H2PGjIGLi4vB90dkLAxCRE+oV69eaN26teb52LFj4enpiQ0bNlTqIOTs7IwRI0aYuoxKQS6XQy6Xm7oMrT7++GOsXLkSU6ZMwSeffFLsFOC7776LNWvWwMrKvH+NZ2dnw97e3tRlkAXjqTGiCuLi4gI7O7sSX0xZWVmYPn06fH19oVAoEBQUhCVLlkAIUaxdYWEhFixYgICAACgUCtSpUwfvvPMO8vLyytz3qlWrYGVlhRkzZlTIeykad3Pt2jWMGDECzs7O8PDwwJw5cyCEwJ07d/D888/DyckJXl5e+Pjjj0vdjkqlwjvvvAMvLy84ODigX79+uHPnTol2Z86cQc+ePeHs7Ax7e3t06tQJv//+e4l2J06cwDPPPANbW1sEBATg66+/LnW/eXl5mDp1Kjw8PFCtWjX069cPd+/eLdGutDFCRWOwTpw4gTZt2sDW1hZ169bF6tWrS6z/119/oVOnTrCzs0OtWrWwcOFC/PjjjyW2ef78eYSHh6N69eqws7ODv78/XnnllVJrL5KTk4NFixYhODgYS5YsKXUc1MiRI9GmTRut26hTpw7GjBlTYnnnzp3RuXPnYss+//xzNGrUCPb29nB1dUXr1q2xfv16AA9/Hop+tvz9/TWnUx99j2vXrkWrVq1gZ2cHNzc3vPTSSyWOdefOndG4cWNcuHABHTt2hL29Pd555x0AD4/Z3LlzERgYCIVCAV9fX7z99tslfv71PbZE+jLvPyWITCgjIwP379+HEAJJSUn4/PPP8eDBg2K9LUII9OvXD4cPH8bYsWPRvHlz7Nu3DzNmzEBcXByWLl2qaTtu3DisWrUKgwYNwvTp03HmzBksWrQIV65cwbZt27TW8c0332DChAl45513sHDhwjLrVqlUuH//fonldnZ2cHBwKLZsyJAhaNCgAf73v//hl19+wcKFC+Hm5oavv/4aXbp0weLFi7Fu3Tq89dZbeOaZZ9CxY8di63/wwQeQJAkzZ85EUlISli1bhm7duuHixYuws7MDABw6dAi9evVCq1atMHfuXMhkMvz444/o0qULjh8/rvmij4yMRI8ePeDh4YF58+ahsLAQc+fOhaenZ4n3Mm7cOKxduxbDhg1DaGgoDh06hD59+pT52RSJjo7GoEGDMHbsWIwePRo//PADxowZg1atWqFRo0YAgLi4OISFhUGSJMyePRsODg747rvvSpxmS0pK0tQ9a9YsuLi44ObNm9i6davOGk6cOIHU1FRMmTLF4L1W3377LSZPnoxBgwbhzTffRG5uLv766y+cOXMGw4YNw4ABA3Dt2jVs2LABS5cuRfXq1QEAHh4eAB4e5zlz5mDw4MEYN24ckpOT8fnnn6Njx46IiIgodiotJSUFvXr1wksvvYQRI0bA09MTarUa/fr1w4kTJ/Daa6+hQYMGiIyMxNKlS3Ht2jVs375ds/7THluiEgQRlcuPP/4oAJR4KBQKsXLlymJtt2/fLgCIhQsXFls+aNAgIUmSiI6OFkIIcfHiRQFAjBs3rli7t956SwAQhw4d0iyrXbu26NOnjxBCiE8//VRIkiQWLFigV+2dOnUqtXYAYvz48Zp2c+fOFQDEa6+9pllWWFgoatWqJSRJEv/73/80y9PS0oSdnZ0YPXq0Ztnhw4cFAFGzZk2hVCo1yzdu3CgAiE8//VQIIYRarRb16tUT4eHhQq1Wa9plZ2cLf39/0b17d82y/v37C1tbW3Hr1i3Nsr///lvI5XLx6K+yos/y9ddfL/behw0bJgCIuXPnapYVHcvY2FjNstq1awsA4tixY5plSUlJQqFQiOnTp2uWTZo0SUiSJCIiIjTLUlJShJubW7Ftbtu2TQAQ586dE+Xx6aefCgBi27ZterUv+swPHz5c7L08elyKdOrUSXTq1Enz/PnnnxeNGjXSuf2PPvqoxGclhBA3b94UcrlcfPDBB8WWR0ZGCisrq2LLi37+vvrqq2Jt16xZI2QymTh+/Hix5V999ZUAIH7//XchRPmOLZG+eGqM6Al98cUXOHDgAA4cOIC1a9ciLCwM48aNK/aX/q+//gq5XI7JkycXW3f69OkQQmiuMvv1118BANOmTSvRDgB++eWXEvv/v//7P7z55ptYvHgx3nvvPb3rrlOnjqbuRx9Tpkwp0XbcuHGa/5fL5WjdujWEEBg7dqxmuYuLC4KCgnDjxo0S648aNQrVqlXTPB80aBC8vb017/fixYu4fv06hg0bhpSUFNy/fx/3799HVlYWunbtimPHjkGtVkOlUmHfvn3o378//Pz8NNtr0KABwsPDi+2zaNuPf+alvT9tGjZsiA4dOmiee3h4lHiPe/fuRUhICJo3b65Z5ubmhuHDhxfbVlFvyO7du1FQUKB3DUqlEgCKfX6G4uLigrt37+LcuXPlXnfr1q1Qq9UYPHiw5vjdv38fXl5eqFevHg4fPlysvUKhwMsvv1xs2aZNm9CgQQMEBwcX20aXLl0AQLONiji2RI/jqTGiJ9SmTZtig6WHDh2KFi1a4I033sBzzz0HGxsb3Lp1Cz4+PiW+zBo0aAAAuHXrlua/MpkMgYGBxdp5eXnBxcVF067I0aNH8csvv2DmzJnlHhfk4OCAbt266dX20dABPBxobWtrqzk18ujylJSUEuvXq1ev2HNJkhAYGKgZW3L9+nUAwOjRo7XWkJGRgby8POTk5JTYHgAEBQVpviCBfz/LgICAEu309fj7BgBXV1ekpaUV209ISEiJdo8fw06dOmHgwIGYP38+li5dis6dO6N///4YNmyYzqvVnJycAACZmZl61/2kZs6ciYMHD6JNmzYIDAxEjx49MGzYMLRr167Mda9fvw4hRKnHBgCsra2LPa9ZsyZsbGxKbOPKlSuaU22PS0pKAlAxx5bocQxCRBVEJpMhLCwMn376Ka5fv64ZS1Ie+k4M2KhRI6Snp2PNmjUYP348/P39y70vfZQ2NkXbeBXx2OBvfajVagDARx99VKxn5VGOjo56DRivSBX5Hosmljx9+jR27dqFffv24ZVXXsHHH3+M06dPw9HRsdT1iuZ1ioyMRP/+/cu936J9l0alUhV7jw0aNEBUVBR2796NvXv3YsuWLVixYgXef/99zJ8/X+c+1Go1JEnCnj17Sv3cHn9/RWPDHt9GkyZN8Mknn5S6D19fX501ED0NBiGiClRYWAgAePDgAQCgdu3aOHjwIDIzM4v1Cl29elXzetF/1Wo1rl+/ruktAoDExESkp6dr2hWpXr06Nm/ejPbt26Nr1644ceIEfHx8DPrenkRRj08RIQSio6PRtGlTAND8Ze/k5KSzl8rDwwN2dnYltgcAUVFRxZ4XfZYxMTHFegoeb/e0ateujejo6BLLS1sGAG3btkXbtm3xwQcfYP369Rg+fDh++umnYqcfH9W+fXu4urpiw4YNeOedd55owLSrq2upEyDeunULdevWLbbMwcEBQ4YMwZAhQ5Cfn48BAwbggw8+wOzZs2Fra6s1VAUEBEAIAX9/f9SvX7/cNRZt488//0TXrl11/jFgrGNLloVjhIgqSEFBAfbv3w8bGxtNmOnduzdUKhWWL19erO3SpUshSRJ69eqlaQcAy5YtK9au6C/k0q6KqVWrFg4ePIicnBx079691FNTprZ69epip3Y2b96Me/fuad53q1atEBAQgCVLlmjC46OSk5MBPOyhCQ8Px/bt23H79m3N61euXMG+ffuKrVO07c8++6zY8sc/26cVHh6OU6dO4eLFi5plqampWLduXbF2aWlpJXqSinq/dPV02dvbY+bMmbhy5QpmzpxZam/U2rVrcfbsWa3bCAgIwOnTp5Gfn69Ztnv37hKXtT/+s2NjY4OGDRtCCKEZ11R0ReHjwWrAgAGQy+WYP39+iRqFEHr9XA4ePBhxcXH49ttvS7yWk5ODrKwsAMY7tmRZ2CNE9IT27Nmj6dlJSkrC+vXrcf36dcyaNUszvqNv374ICwvDu+++i5s3b6JZs2bYv38/duzYgSlTpmh6RJo1a4bRo0fjm2++QXp6Ojp16oSzZ89i1apV6N+/P8LCwkqtITAwEPv370fnzp0RHh6OQ4cOafatTUZGBtauXVvqaxU90aKbmxvat2+Pl19+GYmJiVi2bBkCAwPx6quvAnh4OvG7775Dr1690KhRI7z88suoWbMm4uLicPjwYTg5OWHXrl0AgPnz52Pv3r3o0KEDXn/9dRQWFmrmvvnrr780+2zevDmGDh2KFStWICMjA6Ghofjtt9+09tQ8qbfffhtr165F9+7dMWnSJM3l835+fkhNTdX0bKxatQorVqzACy+8gICAAGRmZuLbb7+Fk5OTJgBrM2PGDFy+fBkff/wxDh8+jEGDBsHLywsJCQnYvn07zp49i5MnT2pdf9y4cdi8eTN69uyJwYMHIyYmBmvXri0xxqZHjx7w8vJCu3bt4OnpiStXrmD58uXo06ePpiezVatWAB5O5PjSSy/B2toaffv2RUBAABYuXIjZs2fj5s2b6N+/P6pVq4bY2Fhs27YNr732Gt566y2d73PkyJHYuHEjJkyYgMOHD6Ndu3ZQqVS4evUqNm7ciH379qF169ZGO7ZkYUxzsRqR+Srt8nlbW1vRvHlz8eWXXxa7DFwIITIzM8XUqVOFj4+PsLa2FvXq1RMfffRRiXYFBQVi/vz5wt/fX1hbWwtfX18xe/ZskZubW6zdo5fPFzlz5oyoVq2a6Nixo8jOztZau67L5x/9dVB0+XxycnKx9UePHi0cHBxK3e6jl18XXcq9YcMGMXv2bFGjRg1hZ2cn+vTpU+zy9yIRERFiwIABwt3dXSgUClG7dm0xePBg8dtvvxVrd/ToUdGqVSthY2Mj6tatK7766itNrY/KyckRkydPFu7u7sLBwUH07dtX3LlzR+/L5x//fIve46OXnBfV3aFDB6FQKEStWrXEokWLxGeffSYAiISEBCGEEH/88YcYOnSo8PPzEwqFQtSoUUM899xz4vz58yX2oc3mzZtFjx49hJubm7CyshLe3t5iyJAh4siRI5o2pV0+L4QQH3/8sahZs6ZQKBSiXbt24vz58yXey9dffy06duyo+fwDAgLEjBkzREZGRrFtLViwQNSsWVPIZLISn9uWLVtE+/bthYODg3BwcBDBwcFi4sSJIioqqthnqO0y/fz8fLF48WLRqFEjoVAohKurq2jVqpWYP39+sTr0PbZE+pKEeILRf0REVKopU6bg66+/xoMHDyr17TuI6CGOESIiekI5OTnFnqekpGDNmjVo3749QxCRmeAYISKiJxQSEoLOnTujQYMGSExMxPfffw+lUok5c+aYujQi0hODEBHRE+rduzc2b96Mb775BpIkoWXLlvj+++9L3HONiCovjhEiIiIii8UxQkRERGSxGISIiIjIYpnFGKGbN29iwYIFOHToEBISEuDj44MRI0bg3XffLXHzvkclJCRgxowZOHDgADIzMxEUFIR3330XAwcO1HvfarUa8fHxqFatmt73gSIiIiLTEkIgMzMTPj4+kMm09/uYRRC6evUq1Go1vv76awQGBuLSpUt49dVXkZWVhSVLlmhdb9SoUUhPT8fOnTtRvXp1rF+/HoMHD8b58+fRokULvfYdHx/PG/4RERGZqTt37qBWrVpaXzfbwdIfffQRvvzyS9y4cUNrG0dHR3z55ZcYOXKkZpm7uzsWL16s9UaHj8vIyICLiwvu3LlT5q0LiIiIqHJQKpXw9fVFeno6nJ2dtbYzix6h0mRkZMDNzU1nm9DQUPz888/o06cPXFxcsHHjRuTm5qJz585a18nLyyt2I8SiG0Y6OTkxCBEREZmZsoa1mOVg6ejoaHz++ecYP368znYbN25EQUEB3N3doVAoMH78eGzbtg2BgYFa11m0aBGcnZ01D54WIyIiqrpMGoRmzZoFSZJ0Poru7l0kLi4OPXv2xIsvvqi5g7U2c+bMQXp6Og4ePIjz589j2rRpGDx4MCIjI7WuM3v2bGRkZGged+7cqZD3SkRERJWPSccIJScnIyUlRWebunXraq4Mi4+PR+fOndG2bVusXLlS5yjwmJgYzcDqRo0aaZZ369YNgYGB+Oqrr/SqUalUwtnZGRkZGTw1RkREZCb0/f426RghDw8PeHh46NU2Li4OYWFhaNWqFX788UedIQgAsrOzAaBEO7lcDrVa/WQFExERUZViFmOE4uLi0LlzZ/j5+WHJkiVITk5GQkICEhISirUJDg7G2bNnAQDBwcEIDAzE+PHjcfbsWcTExODjjz/GgQMH0L9/fxO9EyIiIqpMzOKqsQMHDiA6OhrR0dEl5gIoOrNXUFCAqKgoTU+QtbU1fv31V8yaNQt9+/bFgwcPEBgYiFWrVqF3795Gfw9ERERU+ZjtPELGwjFCRERE5kff72+zODVGREREZAgMQkRERGSxzGKMEBFRVSFELpC9GSLnJ0AVD8hcINkNAOyHQZLpni2fiCoegxARkZEI9QOI1FFA4eWiJYDqAcSD5UD2BsBtAyQrP5PWSGRpeGqMiMhIROZioPBvAOKfRxE1oE6FSJ9qosqILBeDEBGREQh1BpCzFYC2CV1VQGEkRIH2WwARUcVjECIiMobCKAAFZTSSgPwIY1RDRP9gECIiMgo9f91KHLpJZEwMQkRExmDdGJAcymgkAJsQo5RDRA8xCBERGYEk2QL2owFIWlrIAUVnSFb+xiyLyOIxCBERGYnk+Aag6PnPM/k///3n17BVMCTnj0xRFpFF48loIiIjkSQrwGUZkD8EImczoLoNyNwh2T4P2HaHJFmbukQii8MgRERkRJIkAYpQSIpQU5dCROCpMSIiIrJgDEJERERksRiEiIiIyGIxCBEREZHFYhAiIiIii8UgRERERBaLQYiIiIgsFoMQERERWSwGISIiIrJYDEJERERksRiEiIiIyGIxCBEREZHFYhAiIiIii8UgRERERBaLQYiIiIgsFoMQERERWSwGISIiIrJYDEJERERksRiEiIiIyGIxCBEREZHFYhAiIiIii8UgRERERBaLQYiIiIgsFoMQERERWSyzCUL9+vWDn58fbG1t4e3tjZEjRyI+Pl7nOrm5uZg4cSLc3d3h6OiIgQMHIjEx0UgVExERUWVnNkEoLCwMGzduRFRUFLZs2YKYmBgMGjRI5zpTp07Frl27sGnTJhw9ehTx8fEYMGCAkSomIiKiyk4SQghTF/Ekdu7cif79+yMvLw/W1tYlXs/IyICHhwfWr1+vCUxXr15FgwYNcOrUKbRt21av/SiVSjg7OyMjIwNOTk4V+h6IiIjIMPT9/jabHqFHpaamYt26dQgNDS01BAHAhQsXUFBQgG7dummWBQcHw8/PD6dOndK67by8PCiVymIPIiIiqprMKgjNnDkTDg4OcHd3x+3bt7Fjxw6tbRMSEmBjYwMXF5diyz09PZGQkKB1vUWLFsHZ2Vnz8PX1rajyiYiIqJIxaRCaNWsWJEnS+bh69aqm/YwZMxAREYH9+/dDLpdj1KhRqOgze7Nnz0ZGRobmcefOnQrdPhEREVUeVqbc+fTp0zFmzBidberWrav5/+rVq6N69eqoX78+GjRoAF9fX5w+fRohISEl1vPy8kJ+fj7S09OL9QolJibCy8tL6/4UCgUUCkW53wsRERGZH5MGIQ8PD3h4eDzRumq1GsDDMT2ladWqFaytrfHbb79h4MCBAICoqCjcvn271OBERERElsekQUhfZ86cwblz59C+fXu4uroiJiYGc+bMQUBAgCbUxMXFoWvXrli9ejXatGkDZ2dnjB07FtOmTYObmxucnJwwadIkhISE6H3FGBEREVVtZhGE7O3tsXXrVsydOxdZWVnw9vZGz5498d5772lOYxUUFCAqKgrZ2dma9ZYuXQqZTIaBAwciLy8P4eHhWLFihaneBhEREVUyZjuPkLFwHiEiIiLzU6XnESIiIiKqCAxCREREZLEYhIiIiMhiMQgRERGRxWIQIiIiIovFIEREREQWi0GIiIiILBaDEBEREVksBiEiIiKyWAxCREREZLEYhIiIiMhiMQgRERGRxWIQIiIiIovFIEREREQWi0GIiIiILBaDEBEREVksBiEiIiKyWAxCREREZLEYhIiIiMhiMQgRERGRxWIQIiIiIovFIEREREQWi0GIiIiILBaDEBEREVksBiEiIiKyWAxCREREZLEYhIiIiMhiMQgRERGRxWIQIiIiIovFIEREREQWi0GIiIiILBaDEBEREVksBiEiIiKyWAxCREREZLEYhIiIiMhiMQgRERGRxWIQIiIiIotlNkGoX79+8PPzg62tLby9vTFy5EjEx8drbZ+amopJkyYhKCgIdnZ28PPzw+TJk5GRkWHEqomIiKgyM5sgFBYWho0bNyIqKgpbtmxBTEwMBg0apLV9fHw84uPjsWTJEly6dAkrV67E3r17MXbsWCNWTURERJWZJIQQpi7iSezcuRP9+/dHXl4erK2t9Vpn06ZNGDFiBLKysmBlZaXXOkqlEs7OzsjIyICTk9PTlExERERGou/3t35poJJJTU3FunXrEBoaqncIAqD5MHSFoLy8POTl5WmeK5XKp6qViIiIKi+zOTUGADNnzoSDgwPc3d1x+/Zt7NixQ+9179+/jwULFuC1117T2W7RokVwdnbWPHx9fZ+2bCIiIqqkTHpqbNasWVi8eLHONleuXEFwcDCAh2EmNTUVt27dwvz58+Hs7Izdu3dDkiSd21AqlejevTvc3Nywc+dOnb1IpfUI+fr68tQYERGRGdH31JhJg1BycjJSUlJ0tqlbty5sbGxKLL979y58fX1x8uRJhISEaF0/MzMT4eHhsLe3x+7du2Fra1uuGjlGiIiIyPyYxRghDw8PeHh4PNG6arUaAIr13jxOqVQiPDwcCoUCO3fuLHcIIiIioqrNLMYInTlzBsuXL8fFixdx69YtHDp0CEOHDkVAQICmNyguLg7BwcE4e/YsgIchqEePHsjKysL3338PpVKJhIQEJCQkQKVSmfLtEBERUSVhFleN2dvbY+vWrZg7dy6ysrLg7e2Nnj174r333oNCoQAAFBQUICoqCtnZ2QCAP/74A2fOnAEABAYGFttebGws6tSpY9T3QERERJWP2c4jZCwcI0RERGR+9P3+NotTY0RERESGwCBEREREFotBiIiIiCwWgxARERFZLAYhIiIislgMQkRERGSxGISIiIjIYpnFhIpERGQccdH38MvXB3A9IhY2ttYI6fsMug5vDztHO1OXRmQQnFCxDJxQkYgsxY4v9uKLyT9AkklQq9SQJAkCAm6eLvi/g++jdkNfU5dIpDdOqEhERHr74+BfWD7pewghoFY9vKm1EAIQQHqyEjN7LEB+XoGJqySqeAxCRESEjR/tgExe+leCWqVGSnwajm8+beSqiAyPQYiIyMKpVCr88VukpieoNDK5DOf2RhixKiLjYBAiIrJwQi0g1GUMFxUChQWFxikIQH5eAbKU2eAwVjI0XjVGRGThrKyt4N/EDzcv39EaiASAoGfqGbyWSyeuYMP/tuHc3osQagE3b1c8P7EnBk7tA4WdwuD7J8vDHiEiIsKAN/to7xWSAGsbK4SP6WzQGo78/DumdZ6L8/v+1NSSei8NK9//CTN7LEBeTp5B90+WiUGIiIjQY0xndBvREQAgk0ma5XIrGeRyOd79aSqc3KsZbP+ZaQ/w0ctfFLtqrYhQC/x96ho2LdllsP2T5WIQIiIiyGQyzFg5EbPXvYmgNvWgsLOBo4sDuo3oiBXnFyO03zMG3f+B1UdRkFf48BxcKYRaYOeKvVCrtQ/oJnoSHCNEREQAHoahLkPbo8vQ9kbfd2zkbcjkMqgKVVrbpCVm4EFalkF7psjysEeIiIhMTmFno1c7awX/fqeKxSBEREQmF/r8Mzp7g2RyGZqHNeY9z6jCMQgREZHJNe/SGPVa1YXcSsvs1mo1hr4zwMhVkSVgECIiIpOTyWT4YPds+DepDQCQW8khl8sgySRYWcvx1vevo2XXJiaukqoinmwlIqJKwdXTBV+c+x/+OBiJ37edQV5OPmo39EX4y53h4uFs6vKoimIQIiKiSkMmk6F1j2Zo3aOZqUshC8FTY0RERGSxGISIiIjIYjEIERERkcViECIiIiKLxSBEREREFotBiIiIiCwWgxARERFZLAYhIiIislgMQkRERGSxGISIiIjIYjEIERERkcViECIiIiKLxSBEREREFstsglC/fv3g5+cHW1tbeHt7Y+TIkYiPj9drXSEEevXqBUmSsH37dsMWSkRERGajXEEoJycHJ06cwN9//13itdzcXKxevbrCCntcWFgYNm7ciKioKGzZsgUxMTEYNGiQXusuW7YMkiQZrDYiIiIyT5IQQujT8Nq1a+jRowdu374NSZLQvn17/PTTT/D29gYAJCYmwsfHByqVyqAFF9m5cyf69++PvLw8WFtba2138eJFPPfcczh//jy8vb2xbds29O/fX+/9KJVKODs7IyMjA05OThVQORERERmavt/fevcIzZw5E40bN0ZSUhKioqJQrVo1tGvXDrdv366QgssjNTUV69atQ2hoqM4QlJ2djWHDhuGLL76Al5eXXtvOy8uDUqks9iAiIqKqSe8gdPLkSSxatAjVq1dHYGAgdu3ahfDwcHTo0AE3btwwZI0aM2fOhIODA9zd3XH79m3s2LFDZ/upU6ciNDQUzz//vN77WLRoEZydnTUPX1/fpy3b4uWrMnElfROOJ8zHiYSFiM08CJUoMHVZRERE+gehnJwcWFlZaZ5LkoQvv/wSffv2RadOnXDt2rVy73zWrFmQJEnn4+rVq5r2M2bMQEREBPbv3w+5XI5Ro0ZB25m9nTt34tChQ1i2bFm5apo9ezYyMjI0jzt37pT7fdG/7madxKbY53E2eRliMw/gRuY+HEt4H9tuDkZG/k1Tl0dERBZO7zFCbdq0waRJkzBy5MgSr73xxhtYt24dlEplucYIJScnIyUlRWebunXrwsbGpsTyu3fvwtfXFydPnkRISEiJ16dMmYLPPvsMMtm/WU+lUkEmk6FDhw44cuSIXjVyjNCTS8uLwe7bL0MNFYDiP2YS5LCVu+KFOj/DWmZnmgKJiKjK0vf720rrK4954YUXsGHDhlKD0PLly6FWq/HVV1+Vq0gPDw94eHiUa50iarUawMMxPaWZNWsWxo0bV2xZkyZNsHTpUvTt2/eJ9knl83f6zxAQeDwEAYCACjmq+4jNPID6zv2MXxwRERHK0SNkSmfOnMG5c+fQvn17uLq6IiYmBnPmzEFiYiIuX74MhUKBuLg4dO3aFatXr0abNm1K3Y4kSbxqzIg2xIQjX52po4WEmvYh6FZzidFqIiIiy1DhV42Zkr29PbZu3YquXbsiKCgIY8eORdOmTXH06FEoFAoAQEFBAaKiopCdnW3iaqmISuSX0UJAJXKNUgsREVFp9D41ZkpNmjTBoUOHdLapU6eO1oHTRcyg86tKcbUJREreFQioS31dghxuivpGroqIiOhfZtEjROapgcsgrSEIAATUqO/c33gFERERPYZBiAzGv1p31HHs9s+zf29xIv3zY9e6+iQ42/iZoDIiIqKHzOLUGJknSZKhg9dceGW0wJX0jcgouAUAqGHXDI1dh6OWQ6iJKyQiIkv3REHo+vXrOHz4MJKSkjSXsRd5//33K6QwqhpkkhxBLi8gyOUFFKpzIUEGuazkvFBERESmUO4g9O233+I///kPqlevDi8vr2J3dZckiUGItLKS2Zq6BCIiomLKHYQWLlyIDz74ADNnzjREPURERERGU+7B0mlpaXjxxRcNUQsRERGRUZU7CL344ovYv3+/IWohIiIiMqpynxoLDAzEnDlzcPr0aTRp0gTW1tbFXp88eXKFFUdERERkSOW+15i/v7/2jUkSbty48dRFVSa81xgREZH5qfC7zxeJjY19qsKIiIiIKounmllaCMH7dxEREZHZeqIgtHr1ajRp0gR2dnaws7ND06ZNsWbNmoqujYiIiMigyn1q7JNPPsGcOXPwxhtvoF27dgCAEydOYMKECbh//z6mTp1a4UUSERERGcITDZaeP38+Ro0aVWz5qlWrMG/evCo3hoiDpYmIiMyPvt/f5T41du/ePYSGlrxZZmhoKO7du1fezRERERGZTLmDUGBgIDZu3Fhi+c8//4x69epVSFFERERExlDuMULz58/HkCFDcOzYMc0Yod9//x2//fZbqQGJiIiIqLIqd4/QwIEDcebMGVSvXh3bt2/H9u3bUb16dZw9exYvvPCCIWokIiIiMohyD5a2NBwsTUREZH4qdGZppVKp2YhSqdTZlmGBiIiIzIVeQcjV1RX37t1DjRo14OLiAkmSSrQRQkCSJKhUqgovkoiIiMgQ9ApChw4dgpubGwDg8OHDBi2IiIiIyFg4RqgMHCNERGTZHuTlY/3Zi/j5fCQSMx/AyU6BF5o1xKi2LeHp5Gjq8kgLg02ouHfvXpw4cULz/IsvvkDz5s0xbNgwpKWlPVm1RERElVBadg6GfLsBSw+dRFyGEoVqNVKzcrDy1B/o/9Va3LifauoS6SmVOwjNmDFDM2A6MjIS06ZNQ+/evREbG4tp06ZVeIFERESm8r+9R3EzJQ2PnzxRCQFlTi6mbfq1xGtkXso9oWJsbCwaNmwIANiyZQv69u2LDz/8EH/88Qd69+5d4QUSERGZQmpWNn65FAWVlqCjEgJXE5PxV1wCmtXyNnJ1VFHK3SNkY2OD7OxsAMDBgwfRo0cPAICbm1uZl9YTERGZi2uJ91GoVutsIwGIjEs0TkFkEOXuEWrfvj2mTZuGdu3a4ezZs/j5558BANeuXUOtWrUqvEAiIiJTsJbLy2wjAFjLy92nQJVIuY/e8uXLYWVlhc2bN+PLL79EzZo1AQB79uxBz549K7xAIiIiU2js44lqCoXONhKAdgG1jVMQGQQvny8DL58nIrJcXxw5jc+PnCr1NZkkoUeDQCwb/JyRqyJ9VOgtNh6nVqsRHR2NpKQkqB87f9qxY8cn2SQREVGlM6FjG9xNz8C2i39DLpOgUgvIJQkqIdDS1wcLn+9h6hLpKZW7R+j06dMYNmwYbt26VeKSwap4iw32CBERWTYhBP6KS8DmPy4hLl0JNwd79GsajPYBdSCTlbzlFFUOBusRmjBhAlq3bo1ffvkF3t7epd53jIiIqKqQJAnNannzEvkqqtxB6Pr169i8eTMCAwMNUQ8RERGR0ZT7qrFnn30W0dHRhqiFiIiIyKjKHYQmTZqE6dOnY+XKlbhw4QL++uuvYg9D6devH/z8/GBrawtvb2+MHDkS8fHxZa536tQpdOnSBQ4ODnByckLHjh2Rk5NjsDqJiIjIfJR7sLRMVjI7SZIEIYRBB0svXboUISEh8Pb2RlxcHN566y0AwMmTJ7Wuc+rUKfTs2ROzZ89G3759YWVlhT///BPPP/88FGXMDVGEg6WJiIjMj77f3+UOQrdu3dL5eu3axplYaufOnejfvz/y8vJgbW1dapu2bduie/fuWLBgwRPvh0GIiIjI/BjsqjFjBR1dUlNTsW7dOoSGhmoNQUlJSThz5gyGDx+O0NBQxMTEIDg4GB988AHat29v5IqJiIioMnqiG6SsWbMG7dq1g4+Pj6aHaNmyZdixY0eFFve4mTNnwsHBAe7u7rh9+7bO/d24cQMAMG/ePLz66qvYu3cvWrZsia5du+L69eta18vLy4NSqSz2ICIioqqp3EHoyy+/xLRp09C7d2+kp6drxgS5uLhg2bJl5drWrFmzIEmSzsfVq1c17WfMmIGIiAjs378fcrkco0aNKjGpY5GiGa/Hjx+Pl19+GS1atMDSpUsRFBSEH374QWtNixYtgrOzs+bh6+tbrvdERERE5qPcY4QaNmyIDz/8EP3790e1atXw559/om7durh06RI6d+6M+/fv672t5ORkpKSk6GxTt25d2NjYlFh+9+5d+Pr64uTJkwgJCSnxemxsLOrWrYs1a9ZgxIgRmuVDhgyBlZUV1q1bV+r+8vLykJeXp3muVCrh6+vLMUJERERmxGBjhGJjY9GiRYsSyxUKBbKyssq1LQ8PD3h4eJS3BAD/9vg8GloeVadOHfj4+CAqKqrY8mvXrqFXr15at6tQKPS+ooyIiIjMW7lPjfn7++PixYsllu/duxcNGjSoiJpKOHPmDJYvX46LFy/i1q1bOHToEIYOHYqAgABNb1BcXByCg4Nx9uxZAA8v6Z8xYwY+++wzbN68GdHR0ZgzZw6uXr2KsWPHGqROIiIiMi/l7hGaNm0aJk6ciNzcXAghcPbsWWzYsAGLFi3Cd999Z4gaYW9vj61bt2Lu3LnIysqCt7c3evbsiffee0/Te1NQUICoqChkZ2dr1psyZQpyc3MxdepUpKamolmzZjhw4AACAgIMUicRERGZl3KPEQKAdevWYd68eYiJiQEA+Pj4YP78+VWyp4XzCBEREZkfg02o+Kjs7Gw8ePAANWrUeNJNVHoMQkRUmRSq1JBJEmQyydSlEFVqBhss/Sh7e3vY29s/zSaIiKgMKrUaWyIuY/XpCEQnp0AuSWgfWAfj2rXGM3Vqmbo8IrNW7iCUkpKC999/H4cPH0ZSUpLm6q0iqampFVYcEZGlU6nVmL75V+z9+zqK+oBUQuBE9E0cux6Lhc/3wMAWjUxaI5E5K3cQGjlyJKKjozF27Fh4enpCktg9S0RkKNsv/o29fz+cDf/RcQyqf0Y1zNl5AO3q+sHLuZoJqiMyf+UOQsePH8eJEyfQrFkzQ9RDRESPWH0mAhKKh6DHbfrjEiaFlZxYtqKo1QJ30zOgUgvUdHGCjZXcYPsiMrZyB6Hg4GDk5OQYohYiInqEEALXku7rDEFqIXAlIclg+//p/F/47vfziEt/eN9FZztbDG/TDBM6tIGN1VMNMyWqFMo9oeKKFSvw7rvv4ujRo0hJSeENSomIDESSJFjLdPe+yCQJCgMFkg/3HsH8Xw4hPv3f3+0ZObn46thZTFi/AwX/3GuSyJyVOwi5uLhAqVSiS5cuqFGjBlxdXeHq6goXFxe4uroaokYiIovVtq4vZDrGYqqFQOf6/hW+3z/v3sOaMxcBlDwtpxYCJ2/cxo4/r1T4fomMrdx/RgwfPhzW1tZYv349B0sTERlIUuYDzNv9G45dv6m1jVyS4FHNAT0b1q/w/f98PhJymQSVuvQTczIJ2HDuTwxq2bjC901kTOUOQpcuXUJERASCgoIMUQ8RkcVLz87FsO9/xr2MTJ3talRzxA+jBkJhXfGnxmLup2gNQQCgFsDNlPQK3y+RsZX71Fjr1q1x584dQ9RCREQA1pyJQHxGpuYS+dK80akt9k0eA//qhhmS4Gxrq/OUHAA4KKwNsm8iYyr3nxGTJk3Cm2++iRkzZqBJkyawti7+D6Fp06YVVhwRkSXa9Eck1DpCkFwmITHzgUGv2urVuD6ORd/UXoMkoW+TYIPtn8hYyv2vaMiQIQCAV155RbNMkiQIISBJElS8ioCI6Kncf5Ct83WVWiBBqfu02dPq1SgIXx07i7tpGSV6puSSBDsba4x4toVBayAyhnIHodjYWEPUQURE/3Cxt0Vqlvb52uQyCdUdHQxag621FVaPeRH/2bADf99LglwmQYKEQrUaNao5YvlLfeFtgtmsH+TlY9dfV3Ao6gbyCwvR2McTQ1o3hZ+bi9Froarhqe4+bwl493kiMraPD57AD7+f1zlGaOXoQWjr72vwWoQQuHA7DidjbqNQrUbzWt7oVN8fclm5h5g+taiEZLy8egvSsh+GRIGHvVNqCMzt3QUvPcM7HtC/KvTu8zt37kSvXr1gbW2NnTt36mzbr1+/8lVKRETFjG7bAtsuXkZaVk6JMCSTJLQPrI1njXTXeUmS0Lp2LbSubdq73OfkF2Dsmq1Iz8kt9Z5r8345BP/qbnjWCOGQqha9eoRkMhkSEhJQo0YNyHT8FVAVxwixR4iITOFuWgbe3rYXf9yO1yyTyyS80LwR3usVBlsDXDJfmW354xLe3XlA6+tySUL7wDr4enj/CtlfXLoSmy5E4lrSfdhaW6N7cAC6BgfyPmtmpEJ7hNRqdan/T0REhlHL1RnrXxmCa4n3cSk+EdZyOULq+hp8bFBldTz6JmSSpPVqOpUQ+D3mlubCnaex9sxFfLD3MGSQoBICMknCr5eiUNvNBT+OGggfF/5RXJVY1p8URERmpr5nddT3rG7qMkxOpRYo6wSGSqghBPA0OejotVgs3HP44fb+OQlXFL7upmfg1bXbsPP1kSYZI0WGUa4jqVar8cMPP+C5555D48aN0aRJE/Tr1w+rV68u8weUiIjoSTWpqfuWTjJJQkOvGpDJnq436JsTZ7VOJKlSC8TcT8VxHfMrkfnROwgJIdCvXz+MGzcOcXFxaNKkCRo1aoRbt25hzJgxeOGFFwxZJxERWbCBLRr/cwl/6dRCYFTbp5vXKDu/ABdux+uczNJKJsPRa5xGpirR+9TYypUrcezYMfz2228ICwsr9tqhQ4fQv39/rF69GqNGjarwIomILJ1KrcaJ6Fv48+49yGQS2gXURvNa3hZz42t3R3t8PLA3pm7+BQA090ErGjc0sEUj9Gva4Kn2UaDHxT4CAvlV7KIgS6f3PEI9evRAly5dMGvWrFJf//DDD3H06FHs27evQgs0NV41RkRFVGo11ELAWm7cK4euJiRj4k87EZeuhJVMBgEBlVqgiY8nlr/UD55Ojkatx5Su3EvC6jMROHglGgUqNRr61MDINs3Rs1H9pw6FQgiELf0OCcoHWttIAN7rHYbhbZo/1b7I8PT9/tY7CHl5eWHv3r1o3rx5qa9HRESgV69eSEhIeKKCKysGISI6fv0mvj95Hmdi70AACPb0wOiQFujfrKHBe2QSlQ/Qd8VqPMjLL3HKRi6T4Ovqgh0TRhjkDvSW6Pvfz2PJgeMo7YtRwsMZt49Pfw2Otgpjl0blpO/3t95jhFJTU+Hp6an1dU9PT6SlpZWvSiKiSm7VqT/w6rptOHfzrubL8VpSMmZv34/3dh4w+IUia89ElBqCgIenh26mpGHP5WsGrcGSjHy2BdoH1IYEFBuPJJdJkMtk+HhQb4agKkbvIKRSqWCl407HcrkchYWFFVIUEVFlEJOcgv/tOwoAxWZ4/md4CrZEXMb+K9EGrWFX5FWdg3dlkoRfLkUZtAZzkl9QiPN/38aJiBuIS0ov9/o2VnKsGPY85vTuAv/qbpAAKKzk6NUoCBtfHYouQQEVXjOZlt59qUIIjBkzBgpF6Uk4Ly+vwooiIqoMfj4fCZlM0gzMfZxMkrDmTATCG9YzWA0P8vJ1vq4WAsqcXIPt31wIIbDu1wtYuesMMrP+/T5q09gPM8d0Qy1PF723ZS2XY1ibZhjWplmFTNBIlZveQWj06NFltuEVY0RUlVy6l6g1BAEPQ8iVe8kGraG2mwv+vpdU6pgV4OEpm7oebgatwRys2HgCq3efK7H8wt93MHb+Bqz673B4VS//OE+GoKpP7yD0448/GrIOIqJKx9bKChKgNYQAMPi9p/o0DsLle0laX1epBQa3amLQGiq7+KQMrCklBAEPP5/MrFys3HUWs17uZuTKyBxwjnAiIi3KGg8il0no3iDQYPu/eOcelh85pbPNS62booWvj8FqMAe/nvgbko4ZpVVqgV+OX0ZBIef/oZIYhEwgX12Ak/f/xJ7433E25TIK1fzHSVQZPd+sAVzt7Uq95cLDq4okjHr26WYz1ia3oBAT1m9Hro4v7z6Ng/Ber844HXsHO/78G8ejb+o1KWBVk5SWWeYprPwCFZRZHEtFJXHiCSP7Jf44VsbuQrbq33+QztaO+E/gi+jgYZhfqESVWV5BIfZfica1xGQorK3QNSgADbxrmLosAEA1WwV+HD0IY9dswf0H2ZBLEgQeDsxVWFth2Yt9EFjD3SD73nM5CullDIL+43Y8un36Q7EJAN0d7PB2j454vllDg9RVGbk62QNlTGMgl8vgaMfL3qkkBiEj+jX+BFZEbyqxPKPgAf535UdYSXKEVG9qgsqITONE9C1M3/ILMnLyHs6YLASWHzmN9gG18cmg3nCyszV1iQjyrI6Db47F3svXcCLmFlRqNZrX8kb/5g3hbMD6zt+Kg1zHFWsAcE+ZWeLeWylZOZi5bR/UQuCF5o0MVl9l0jO0AVbuPKv1dblMQvdn60Nhw688KomnxowkT5WPH2N36mzz/Y3tBp+cjaiyuByfiAnrt0OZ8/BS50K1WjNXz6kbt/H6Tzsrzb8HW2sr9G/eEEsG9sLSF/tgdEhLg4Yg4OFpN32uV9L2Cf1v3zHkW8iYGP+a7ujbsTFKOzsmk0mwsbbCy8+3NX5hZBYYhIzkQtqVYqfDSnMv9z6uZd42UkVEpvXN8XMQEKV+kauEwPlbcTh/K87odVUWz/rXQqGO3qCyZOTk4veYWxVYUeU265VuGNqzFaytin+t1fZ2w1fvDUYdH04xQKVjP6GRZBRov4lf8XaZBq6EyPQKVCocvBpdbLbmx1nJZNhz+RqeqVPLiJVVHuEN62HxvmNIy87R+Tnpkvwgq4Krqrys5DK8OawTxvRrg9ORt5CXVwD/Wu5oHODNuYBIJwYhI6mucNGznathCyGqBPIKCsv8chdC4IEFz1hvY2WFb0a8gJdXbUamlnuNlaVGNQcDVFa5OTvaITwk2NRlkBkxm1Nj/fr1g5+fH2xtbeHt7Y2RI0ciPj5e5zoJCQkYOXIkvLy84ODggJYtW2LLli1Gqri4lq7BcLZ21Pq6BAl1HHzg72DZ84GQZXBQ2MDV3k5nGwGgjnvV/sMg5UE2zt68i7/uJqBQpS7xekPvGtgzaQymdAlFI+8asLe21nvbbvZ2aBdQuyLLJaqSzCYIhYWFYePGjYiKisKWLVsQExODQYMG6Vxn1KhRiIqKws6dOxEZGYkBAwZg8ODBiIiIMFLV/5JLcrwe+CIAlBgAKUGCTJLwn4BB7MIliyBJEoa2blrq/DyPGtiisZEqMq7kzCxM3fQLOnz8DUat3ITB321Ap0++xerTESUGiLs52OO1Dm2wZfxwdK7vX+ZnVmR2z86wlht21muiqkASleWyjHLauXMn+vfvj7y8PFhr+SvJ0dERX375JUaOHKlZ5u7ujsWLF2PcuHF67UepVMLZ2RkZGRlwcir/fWoed+r+X/j+xnbcy72vWebv4IMJAYPQ2MVwM9QSVTYPcvMw7IeNiElOKXaarOiWFrPCO2FMSEuT1WcoaVk5GPTteiQoM0u9NP7Vdq0xvXuHUtfde/kapmz6Ref27ayt8OHzPdCrcVCF1EtkrvT9/jbLMUKpqalYt24dQkNDtYYgAAgNDcXPP/+MPn36wMXFBRs3bkRubi46d+6sdZ28vDzkPTIuQalUVmTpCKneFG3dm+D6g9tIz38AD4UL6jj4sCeILI6jrQLrXhmMzw6fxOY/LiOnoAAAEFjDHa93fLbKfpF/f/I8EjIytY6R+vb38xjYsnGppwW7Bgcg0MMNsSlpJUKUhIeTBv487iXU9/QwROlEVZJZ9QjNnDkTy5cvR3Z2Ntq2bYvdu3fD3V37rK7p6ekYMmQI9u/fDysrK9jb22PTpk3o0aOH1nXmzZuH+fPnl1heUT1CRFRSbkEh4jOUsLWygrdztSr7h4EQAs8u/hLKXO2DwOWShLHtWmNat/alvp6U+QD/Wb8Dl+8lQS57ONtQoVoNFztbfDr4OTzr72uo8onMir49QiYNQrNmzcLixYt1trly5QqCgx9eAXD//n2kpqbi1q1bmD9/PpydnbF7926tvzQnTZqEs2fP4sMPP0T16tWxfft2LF26FMePH0eTJqXfrbm0HiFfX18GISJ6all5+Wi16AudbWSShF6N6uPjQb21thH/zLN09HosClQqNPL2RHjDelBYm2UnP5FBmEUQSk5ORkpKis42devWhY2NTYnld+/eha+vL06ePImQkJASr8fExCAwMBCXLl1Co0b/TjPfrVs3BAYG4quvvtKrxooeI0RElkulVqPFB8uRr+PGqHKZDENbN8V7vcOMWBlR1WMWY4Q8PDzg4fFk57LV6oeXmuZpmWckOzsbACCTFb8wTi6Xa9YlIjImuUyG55oEYcdfV7TeQ0ylVuO5JpwHpypIyMhEUmYW3BzsUMvV2dTlkBZm0Y965swZnDt3Du3bt4erqytiYmIwZ84cBAQEaHqD4uLi0LVrV6xevRpt2rRBcHAwAgMDMX78eCxZsgTu7u7Yvn07Dhw4gN27d5v4HRGRpXq1fRvs/fs6cgsKS0ySKJMkdKpXB81qeZmoOqoIf99Lwv/tP4bTsXc0y1r4euOtbh3QqnZNE1ZGpTGLeYTs7e2xdetWdO3aFUFBQRg7diyaNm2Ko0ePQqFQAAAKCgoQFRWl6QmytrbGr7/+Cg8PD/Tt2xdNmzbF6tWrsWrVKvTurf3cOxGRIflXd8Wq0YNQy+VhV71MenhzVQlA3ybBWPric1V2sLgliIxLwNDvf8LZm3eLLf/zbgJGrdqEkxZ0/zdzYVZXjZkCxwgRkSEIIXAm9g6iEu9DYSVHp/p14e1czdRl0VMa9M16/H0vqdRbokgS4OPshAOTX4FMxrBraGYxRoiIyFJJkoS2df3Qtq6fqUuhCnIt8T4uxSdqfV0IIC5dibO37qItpzmoNMzi1BgREVFldzcto0LbkXEwCBEREVUAJzuFfu1s9WtHxsFTY0SkcTMlDRvO/YnTsXcgQUK7AD8MfaYZL/0l0kPzWj7wcHRA8oMsrW3sbazRIbCO8YqiMjEIEREAYNdfVzFz215IEjRz3FxPuo9VpyOw9MU+6N6ANwUm0kalVuNEzE3U96yuMwhN7NQWdjba75FJxscgRES4nnQfM7ftfXilyyMXu6iEgCQEpm76Bb++MRp+bi4mq5Gosoq9n4bx67fjdmo6rGQyyCRJc9WY/J//t5LLMLFTW7wS2srE1dLjGISICOvO/glJQrEQVETg4aXeG879hZnhHY1dGlGllpmbh9GrNiHlwcM57AofuXOBBMBKLsPksFAMbNEYLva2JqqSdOFgaSLC8eibWm/5ADzsGToRc9N4BRGZiW0XLyM5MwuqUuYNEgAKVGpk5eczBFViDEJUpQghkJ2bj/yCQlOXYlb0mVa1tAniiCzdnkvXSutI1VALgV8io4xWD5UfT41RlVCoUmPLwYv4eV8E4pIfztHRskEtjH6uDdo2rWPa4szAs/61sFPHjUDlMglt/TnxH9HjHuTnl9kmS482ZDrsESKzV6hSY9anO7F07RHEJ/87UdnFqDi8+dFWbD540XTFmYnhbZpDrePUmBDAsGeaGrEiIvNQr4Y75DpulyGTJNTzcDdiRVReDEJk9nYdvYTjETceDup9ZHnRF/uS1YeKBSQqqbGPJ97v0wUSUOyXulz28IagC/t1RwB/mROVMKRVU53j69RCYOgzzYxYEZUXgxCZvY0HIqDrZt2SJGHHkUjjFWSmhj7TDBtfHYrnGgfD08kRXk6O6N+sIbaOH44BLRqZujyiSqlNnVoY2vphb2lpv4Z6NaqPbsGcg6sy4xghMmtCCMTGpegc7KtWC0TfuW+8osxYk5peWDygp6nLIDIbkiTh/T5dEOTlge9/P487/9xHzMvJEaPbtsSoti14p/lKjkGIzJokSbCxskKejqvEZJIEWxv+qBORYUiShJdaN8WQVk2QlJkFIQRqVHNkADITPDVGZq9T60CdgxXVQqBjywAjVkRElkiSpIenlZ2rMQSZEQYhMnsjerfW+ppcJsHHwwlhz9QzYkVERGQuGITI7AXVqYFFk/tCYW0FSXoYfop6iLw9nPHFrBdhY81TY0REVJIkBKeL1UWpVMLZ2RkZGRlwcnIydTmkQ2ZWLn458TeuxibC2kqOds390b5FAKzkzPtERJZG3+9v/plMVUY1B1u8FN7S1GUQEZEZ4Z/KREREZLEYhIiIiMhiMQgRERGRxWIQIiIiIovFIEREREQWi0GIiIiILBaDEBEREVksBiEiIiKyWAxCREREZLEYhIiIiMhiMQgRERGRxWIQIiIiIovFIEREREQWi0GIiIiILBaDEBEREVksBiEiIiKyWAxCREREZLHMLgjl5eWhefPmkCQJFy9e1Nk2NzcXEydOhLu7OxwdHTFw4EAkJiYap1AiIiKq9MwuCL399tvw8fHRq+3UqVOxa9cubNq0CUePHkV8fDwGDBhg4AqJiIjIXJhVENqzZw/279+PJUuWlNk2IyMD33//PT755BN06dIFrVq1wo8//oiTJ0/i9OnTRqiWiIiIKjuzCUKJiYl49dVXsWbNGtjb25fZ/sKFCygoKEC3bt00y4KDg+Hn54dTp05pXS8vLw9KpbLYg4iIiKomswhCQgiMGTMGEyZMQOvWrfVaJyEhATY2NnBxcSm23NPTEwkJCVrXW7RoEZydnTUPX1/fpymdiIiIKjGTBqFZs2ZBkiSdj6tXr+Lzzz9HZmYmZs+ebfCaZs+ejYyMDM3jzp07Bt8nERERmYaVKXc+ffp0jBkzRmebunXr4tChQzh16hQUCkWx11q3bo3hw4dj1apVJdbz8vJCfn4+0tPTi/UKJSYmwsvLS+v+FApFif0QERFR1SQJIYSpiyjL7du3i43ViY+PR3h4ODZv3oxnn30WtWrVKrFORkYGPDw8sGHDBgwcOBAAEBUVheDgYJw6dQpt27bVa99KpRLOzs7IyMiAk5NTxbwhIiIiMih9v79N2iOkLz8/v2LPHR0dAQABAQGaEBQXF4euXbti9erVaNOmDZydnTF27FhMmzYNbm5ucHJywqRJkxASEqJ3CCIiIqKqzSyCkD4KCgoQFRWF7OxszbKlS5dCJpNh4MCByMvLQ3h4OFasWGHCKomIiKgyMYtTY6bEU2NERETmR9/vb7O4fJ6IiIjIEBiEiIiIyGIxCBEREZHFYhAiIiIii8UgRERERBaLQYiIiIgsFoMQERERWSwGISIiIrJYDEJERERksRiEiIiIyGIxCBEREZHFYhAiIiIii8UgRERERBbLytQFEJHpZKRl4ddtF3B4bySyHuSitr8Hnhv0DNp2CoJMxr+TiKjqYxAii5GcmIEDuy8i6V4GnJzt0KVXU9QJ9DR1WSZzOzYZb736A5TpORBCAABSkzNx4XQMOnRtiNkfDoLcSm7iKomIDEsSRb8BqVRKpRLOzs7IyMiAk5OTqcuhJyCEwIbvj2H114chAZBkEiAAlUqNsJ5NMH1ef1hbW9bfBGq1Gq+88BkS76VDrSr5K0CSgJff6IYhYzqYoDoioqen7/c3+76pytuz7QJWfXkIQi2gVguoCtVQqdQAgCP7LmHF//1q4gqN78KpGNy7m1ZqCAIAIYBt609DVagycmVERMbFIERVmkqlxtpvjmh9XQiBvdv/QEpypvGKqgQiI25BLtf9zz8t5QES7qUbpyAiIhNhEKIqLSbqXpkhR60WOHM8ykgVVQ6SpGc76NmQiMhMMQhRlZabU1BmG5lMQm5OvhGqqTyatvLXnB7Uxr1GNXj6uBinICIiE2EQoiqtpp/7w8HROqjVArUDLOvqsRZt/OFbpzpkOk6PDRweWubpMyIic8ffclSluXtUQ0jHIK1f+DKZhBrezmjRxt/IlZmWTCbD/GXD4OrmUOw0WdHn1LV3M7wwrK2JqiMiMh7LumaYLNJ/ZvTG1Ut3kZ6WDfUjp4NkchnkchlmLhholpMHKtOzcS8uDXb2NvCtUx2SvgN//lHT1x3fbJqI/bsuPpxQMTMXtet6oM/AZ9AqJKDc2yMiMkecR6gMnEeoakhJzsT6747iwK4I5OUVQiaT0K5LAwwb1wl163mZurxySUlW4pul+3Hs4GVNsKvp54aR47sgrGcTE1dHRFQ56Pv9zSBUBgahqqWgoBAPlLmws7eBrZ2Nqcspt9T7mZg08hukpjwo1rtVZML0nnhhWIgJKiMiqlw4oSJRKaytreDq7miWIQgA1n5zBKkpmaWGIAD49tP9SE99YOSq9KNWq3H10l2cO3kdd27eN3U5REQAOEaIyGzk5xXgwO6LWmeDBh5eAffbr39h4IhQI1ZWtiP7IvHD5weR+MgEjQ2b+uL1t3ujXgMf0xVGRBaPPUJEZiI9LRv5eYU628hkEu7FpRmpIv3s2/EHFr2zuVgIAoCrl+5i+tgfEBN1zzSFERGBQYjIbDg4KlDWRM9CANWq2RqnID3k5uTjyyV7Sn1NrRYoKCjEN8v2G7kqIqJ/MQgRmQkHR1s8E1oPMh0TRKpVanQKrxxXjhUWqPD1J/uQk6191m61WuDi2RtISsgwYmVERP9iECIyIyNe6wxJkkqd40eSSejUozHqBNQwQWXF5ebkY/bE1fh163m92t9PUhq4IiKi0jEIEZmR4Ma1sODT4XB2tQcAyOWyf4IR0K13M7w1r79pC/zHj1/8hksRt/RuX/R+iIiMjVeNEZmZViGBWPfrdJw+HoXbscmws7NBaFgDeHq7mLo0AEB2Vh72bLsAtbrsKcokSUK9Bt6o6etuhMqIqLJJzsmCMj8XnvaOcLRWmKQGBiEiM2RlLUf7Lg1NXUapYqMTkZdboF9jCRg7qbthCyKiSud0wm18cvE4zibdAQBYyWR4vk5DvNWiI7wdjDt5MU+NEVGF0vceZfYOCsz/ZBiat6lr4IqIqDLZf/sahu3fgPPJdzXLCtVqbI+9jL6/rELcA+NePMEgREQVKqC+F+wdyu7iXvLty3i2Q30jVERElUWeqhAzTv4KAQH1Y3f4UgmBtLxsLLpw2Kg1MQgRUYVS2Fqj7+A20NYxJJPL0LRVHQQEeRu3MCIyuQN3riMjPxfaRhCqhMCe21FIy80xWk1mF4Ty8vLQvHlzSJKEixcvam2XmpqKSZMmISgoCHZ2dvDz88PkyZORkcH5SogMbeT4zmjbMQjAw+AD/HvKrKavG95Z9KLJaiMi07mhTIWVpDt6qITAnQfpxikIZjhY+u2334aPjw/+/PNPne3i4+MRHx+PJUuWoGHDhrh16xYmTJiA+Ph4bN682UjVElkma2srvL/kJZw9cR17tp1H/N00uLg6oFufZugc3gQKW2tTl1hlCSFw+eJtxN9JhWM1W7RsG2C2NxmmqsfR2gYqUfYVpdVsjHcFmSSEHhVVEnv27MG0adOwZcsWNGrUCBEREWjevLne62/atAkjRoxAVlYWrKz0y4BKpRLOzs7IyMiAk5NxR7ITEZVHZMQtLP3vdsTdTtUss7O3wdCxHTF4dHu9B7ITGUp8lhLttqzQempMAlDPpTr29R371D+v+n5/m02PUGJiIl599VVs374d9vZPNvla0YehKwTl5eUhLy9P81yp5Iy3RFT5Xb10F7P+swqqQnWx5TnZ+fjh84MoyC/EiNfCTFQd0UM+Dk4YHNgUG6MjIUqJQwLAtOYdjBrazWKMkBACY8aMwYQJE9C6desn2sb9+/exYMECvPbaazrbLVq0CM7OzpqHr6/vE+2PiMiYflx+EGqVGto6+dd/dwwZaVlGroqopAXPhmNQQGMAgEySYC2TQQKgkFvhfyG90NMvyKj1mPTU2KxZs7B48WKdba5cuYL9+/dj48aNOHr0KORyOW7evAl/f3+9T40plUp0794dbm5u2LlzJ6yttY9PKK1HyNfXl6fGiKjSSklWYljPj3W2kSQJE9/ujb6D2xipKiLdbmWmYffNK1Dm58HX0QX9/BvAyca2wrZvFqfGpk+fjjFjxuhsU7duXRw6dAinTp2CQlF88FTr1q0xfPhwrFq1Suv6mZmZ6NmzJ6pVq4Zt27bpDEEAoFAoSuyHiKgyS08tu6dHJpeQmvLACNUQ6ad2NVdMbBJq6jJMG4Q8PDzg4eFRZrvPPvsMCxcu1DyPj49HeHg4fv75Zzz77LNa11MqlQgPD4dCocDOnTtha1txSZOIqLJwq17t4ShTHf37apUa1WtUM1pNRObCLAZL+/n5FXvu6OgIAAgICECtWrUAAHFxcejatStWr16NNm3aQKlUokePHsjOzsbatWuhVCo1A589PDwgl8uN+yaIiAzE1d0RrUPq4Y/T0VpvdmtlJUfH7o2NXBlR5WcWQUgfBQUFiIqKQnZ2NgDgjz/+wJkzZwAAgYGBxdrGxsaiTp06xi6RiMhgxk7uhsgLN1FQUFhqGBozsSuqOdmZoDKiys2s5hEyBc4jRETm4vqVeHz6wS5cvxKvWebi5oCR48Pw3KBnTFgZkfHp+/3NIFQGBiEiMjex1xMRfycVDtVs0bi5H6ysORSALI9ZXDVGREQVz7+eJ/zreZq6DCKzwCBERGThhBD48/xNnD95HQUFKgQ1qon2XRvCxoZfEVT18dRYGXhqjIiqsuTEDLw/ZT1uXEuAXC4DJEBVqIaTiz3eX/ISmrSobeoSqRTX0pPxw9/nceDOdRSoVWha3Rtjgluha61A3lPuHxwjVEEYhIioqsrPL8SEISuQEJcGlar4PcpkMgnW1nKs2PAf1Kpd3UQVUmn2376G149uh4DQ3MldLklQCYExwa0w95luDEPQ//vbLO41RkREFe/Eb38j7nZKiRAEAGq1QGGhGtvWnzZBZaTN/ZwsvHFsB1RCrQlBADT/v/LqBfx6K8pU5ZklBiEiIgt14re/Icm09xyoVGoc2RdpxIqoLD9H/4VCtVrrJOIyScL3V84ZtSZzxyBERGShsrPzILTMRF0kN7fASNWQPiKS46A9BgFqIfDn/Xhw1Iv+GISIiCxUbX8PyOTavwYkCahV292IFVFZ5DIZyhr9I5NkHCNUDgxCREQWqtcLraAuZXxQESGAfoPbGLEiKkt77zq67q0LuSShvTev9CsPBiEiIgtVJ9ATI17rDOBh78+jJElCi2frIrxfS+MXRlq9ULcRXGxsIdPS46MSAq82etbIVZk3BiEiIgs2cnwY3l4wAH7+HpplD+9P1hn/XTact+eoZBytFVjdbQiqWSuKnSKTSxIkAP9t0x2hXuwRKg/OI1QGziNERJZACIG0lAcoLFDB3aMa5FYMQJVZel4ONsdE4uCdaOSpCtHcwwfD67dAoDPHdBXhhIoVhEGIiIjI/HBCRSIiIqIyMAgRERGRxWIQIiIiIovFIEREREQWi0GIiIiILBaDEBEREVksBiEiIiKyWAxCREREZLEYhIiIiMhiMQgRERGRxbIydQGVXdEdSJRKpYkrISIiIn0VfW+XdScxBqEyZGZmAgB8fX1NXAkRERGVV2ZmJpydnbW+zpuulkGtViMqKgoNGzbEnTt3eOPVSkipVMLX15fHp5Li8anceHwqNx6fJyeEQGZmJnx8fCCTaR8JxB6hMshkMtSsWRMA4OTkxB/ESozHp3Lj8anceHwqNx6fJ6OrJ6gIB0sTERGRxWIQIiIiIovFIKQHhUKBuXPnQqFQmLoUKgWPT+XG41O58fhUbjw+hsfB0kRERGSx2CNEREREFotBiIiIiCwWgxARERFZLAYhIiIislgMQlr069cPfn5+sLW1hbe3N0aOHIn4+Hit7VNTUzFp0iQEBQXBzs4Ofn5+mDx5MjIyMoxYteUo7/EBgNzcXEycOBHu7u5wdHTEwIEDkZiYaKSKLcfNmzcxduxY+Pv7w87ODgEBAZg7dy7y8/N1rpeQkICRI0fCy8sLDg4OaNmyJbZs2WKkqi3Hkx4fADh16hS6dOkCBwcHODk5oWPHjsjJyTFC1ZbjaY4P8HA25V69ekGSJGzfvt2wxVYRDEJahIWFYePGjYiKisKWLVsQExODQYMGaW0fHx+P+Ph4LFmyBJcuXcLKlSuxd+9ejB071ohVW47yHh8AmDp1Knbt2oVNmzbh6NGjiI+Px4ABA4xUseW4evUq1Go1vv76a1y+fBlLly7FV199hXfeeUfneqNGjUJUVBR27tyJyMhIDBgwAIMHD0ZERISRKrcMT3p8Tp06hZ49e6JHjx44e/Yszp07hzfeeEPnrQuo/J70+BRZtmwZJEkycJVVjCC97NixQ0iSJPLz8/VeZ+PGjcLGxkYUFBQYsDISouzjk56eLqytrcWmTZs0y65cuSIAiFOnThmrTIv1f//3f8Lf319nGwcHB7F69epiy9zc3MS3335ryNJI6Hd8nn32WfHee+8ZqSJ6lD7HRwghIiIiRM2aNcW9e/cEALFt2zbDF1cFMMrrITU1FevWrUNoaCisra31Xi8jIwNOTk6wsuIt3QxJn+Nz4cIFFBQUoFu3bpplwcHB8PPzw6lTp4xVqsXKyMiAm5ubzjahoaH4+eefkZqaCrVajZ9++gm5ubno3LmzcYq0YGUdn6SkJJw5cwY1atRAaGgoPD090alTJ5w4ccKIVVouff79ZGdnY9iwYfjiiy/g5eVlpMqqBgYhHWbOnAkHBwe4u7vj9u3b2LFjh97r3r9/HwsWLMBrr71mwAotW3mOT0JCAmxsbODi4lJsuaenJxISEgxcqWWLjo7G559/jvHjx+tst3HjRhQUFMDd3R0KhQLjx4/Htm3bEBgYaKRKLZM+x+fGjRsAgHnz5uHVV1/F3r170bJlS3Tt2hXXr183VqkWSd9/P1OnTkVoaCief/55I1VWdVhUEJo1axYkSdL5uHr1qqb9jBkzEBERgf3790Mul2PUqFEQekzErVQq0adPHzRs2BDz5s0z4DuqWox1fOjJlPf4AEBcXBx69uyJF198Ea+++qrO7c+ZMwfp6ek4ePAgzp8/j2nTpmHw4MGIjIw05NuqMgx5fNRqNQBg/PjxePnll9GiRQssXboUQUFB+OGHHwz6vqoKQx6fnTt34tChQ1i2bJmB30XVZFG32EhOTkZKSorONnXr1oWNjU2J5Xfv3oWvry9OnjyJkJAQretnZmYiPDwc9vb22L17N2xtbZ+6bkthyONz6NAhdO3aFWlpacV6hWrXro0pU6Zg6tSpT11/VVfe4xMfH4/OnTujbdu2WLlypc5BtTExMQgMDMSlS5fQqFEjzfJu3bohMDAQX331VcW8iSrMkMcnNjYWdevWxZo1azBixAjN8iFDhsDKygrr1q2rmDdRhRny+EyZMgWfffZZsTYqlQoymQwdOnTAkSNHKuQ9VFUWNXjFw8MDHh4eT7Ru0V9EeXl5WtsolUqEh4dDoVBg586dDEHlZMjj06pVK1hbW+O3337DwIEDAQBRUVG4ffu2zmBL/yrP8YmLi0NYWBhatWqFH3/8scwri7KzswGgRDu5XK45tqSbIY9PnTp14OPjg6ioqGLLr127hl69ej1xzZbEkMdn1qxZGDduXLFlTZo0wdKlS9G3b98nrtlimHasduV0+vRp8fnnn4uIiAhx8+ZN8dtvv4nQ0FAREBAgcnNzhRBC3L17VwQFBYkzZ84IIYTIyMgQzz77rGjSpImIjo4W9+7d0zwKCwtN+XaqnCc5PkIIMWHCBOHn5ycOHTokzp8/L0JCQkRISIip3kaVdffuXREYGCi6du0q7t69W+zfwqNtHj0++fn5IjAwUHTo0EGcOXNGREdHiyVLlghJksQvv/xiqrdSJT3J8RFCiKVLlwonJyexadMmcf36dfHee+8JW1tbER0dbYq3UWU96fF5HHjVmN4YhErx119/ibCwMOHm5iYUCoWoU6eOmDBhgrh7966mTWxsrAAgDh8+LIQQ4vDhwwJAqY/Y2FjTvJEq6kmOjxBC5OTkiNdff124uroKe3t78cILLxT75UIV48cff9T6b6FIacfn2rVrYsCAAaJGjRrC3t5eNG3atMTl9PT0nvT4CCHEokWLRK1atYS9vb0ICQkRx48fN3L1Vd/THJ9HMQjpz6LGCBERERE9yqKuGiMiIiJ6FIMQERERWSwGISIiIrJYDEJERERksRiEiIiIyGIxCBEREZHFYhAiIiIii8UgREQmJUkStm/fbuoydDpy5AgkSUJ6erqpSyGiCsYgREQVbsyYMZo7altbW8PT0xPdu3fHDz/8UOLeYffu3av096sKDQ3FvXv34OzsbND9HDt2DH379oWPj49ZBESiqoBBiIgMomfPnrh37x5u3ryJPXv2ICwsDG+++Saee+45FBYWatp5eXlBoVCYsNKy2djYwMvLC5IkGXQ/WVlZaNasGb744guD7oeI/sUgREQGoVAo4OXlhZo1a6Jly5Z45513sGPHDuzZswcrV67UtHu05+PmzZuQJAkbN25Ehw4dYGdnh2eeeQbXrl3DuXPn0Lp1azg6OqJXr15ITk4utr/vvvsODRo0gK2tLYKDg7FixQrNa0Xb3bp1K8LCwmBvb49mzZrh1KlTmja3bt1C37594erqCgcHBzRq1Ai//vorgNJPjW3ZsgWNGjWCQqFAnTp18PHHHxerp06dOvjwww/xyiuvoFq1avDz88M333yj8zPr1asXFi5ciBdeeKE8HzURPQUGISIymi5duqBZs2bYunWrznZz587Fe++9hz/++ANWVlYYNmwY3n77bXz66ac4fvw4oqOj8f7772var1u3Du+//z4++OADXLlyBR9++CHmzJmDVatWFdvuu+++i7feegsXL15E/fr1MXToUE3v1MSJE5GXl4djx44hMjISixcvhqOjY6n1XbhwAYMHD8ZLL72EyMhIzJs3D3PmzCkW8ADg448/RuvWrREREYHXX38d//nPfxAVFfUEnxwRGYyp7/pKRFXP6NGjxfPPP1/qa0OGDBENGjTQPMcjd8kuuqv2d999p3l9w4YNAoD47bffNMsWLVokgoKCNM8DAgLE+vXri+1nwYIFIiQkROt2L1++LACIK1euCCGEaNKkiZg3b16pNR8+fFgAEGlpaUIIIYYNGya6d+9erM2MGTNEw4YNNc9r164tRowYoXmuVqtFjRo1xJdfflnqPh4H3j2cyCjYI0RERiWEKHOsTdOmTTX/7+npCQBo0qRJsWVJSUkAHo6riYmJwdixY+Ho6Kh5LFy4EDExMVq36+3tDQCa7UyePBkLFy5Eu3btMHfuXPz1119a67ty5QratWtXbFm7du1w/fp1qFSqUvcnSRK8vLw0+yOiyoFBiIiM6sqVK/D399fZxtraWvP/RaHp8WVFV589ePAAAPDtt9/i4sWLmselS5dw+vTpMrdbtJ1x48bhxo0bGDlyJCIjI9G6dWt8/vnnT/o2S+zv8bqJqHJgECIiozl06BAiIyMxcODACtump6cnfHx8cOPGDQQGBhZ7lBW4Hufr64sJEyZg69atmD59Or799ttS2zVo0AC///57sWW///476tevD7lc/sTvhYiMz8rUBRBR1ZSXl4eEhASoVCokJiZi7969WLRoEZ577jmMGjWqQvc1f/58TJ48Gc7OzujZsyfy8vJw/vx5pKWlYdq0aXptY8qUKejVqxfq16+PtLQ0HD58GA0aNCi17fTp0/HMM89gwYIFGDJkCE6dOoXly5cXu1LtSTx48ADR0dGa57Gxsbh48SLc3Nzg5+f3VNsmotIxCBGRQezduxfe3t6wsrKCq6srmjVrhs8++wyjR4+GTFaxndHjxo2Dvb09PvroI8yYMQMODg5o0qQJpkyZovc2VCoVJk6ciLt378LJyQk9e/bE0qVLS23bsmVLbNy4Ee+//z4WLFgAb29v/Pe//8WYMWOe6n2cP38eYWFhmudFIW706NElrkgjooohCSGEqYsgIiIiMgWOESIiIiKLxSBEREREFotBiIiIiCwWgxARERFZLAYhIiIislgMQkRERGSxGISIiIjIYjEIERERkcViECIiIiKLxSBEREREFotBiIiIiCwWgxARERFZrP8HvSXp7GOaAOkAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the closest embeddings to the centroids\n",
        "\n",
        "# Create an empty list that will hold your closest points\n",
        "closest_indices = []\n",
        "\n",
        "# Loop through the number of clusters you have\n",
        "for i in range(num_clusters):\n",
        "\n",
        "    # Get the list of distances from that particular cluster center\n",
        "    distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)\n",
        "\n",
        "    # Find the list position of the closest one (using argmin to find the smallest distance)\n",
        "    closest_index = np.argmin(distances)\n",
        "\n",
        "    # Append that position to your closest indices list\n",
        "    closest_indices.append(closest_index)"
      ],
      "metadata": {
        "id": "y4vyHnUtB2B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_indices = sorted(closest_indices)\n",
        "selected_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MltvvttCDd48",
        "outputId": "c345d35d-7048-4018-e9f7-838d673b0f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 5, 9, 11, 16, 29, 31]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import transformers\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "NOzbh9bSM4zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "um-qU1-KRO6e",
        "outputId": "27913671-004d-486b-a0d9-4e4a190d0735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", torch_dtype=torch.float16).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f88848b3dfb049dfa5018b4f4773972e",
            "1b8a272f36904afbae4a3f273870e57b",
            "01db4283a4444469a424140d7c040fff",
            "c86623eac8b94feb9d9c52f7c2cee4cf",
            "21b7a30d2d16413a99a545c9e4633101",
            "2b6024f755084993a5c22b6769d81b3a",
            "b02bf6e580504d13abcd60eafe17ef81",
            "258d6b6592ac4c3883bc49cc5997cee2",
            "5688b7ce530f4d0a93a0592adb77aa80",
            "cfb6e04d18b9421fbd0f5bdb895c3b8e",
            "71f13753fcb244c28e4a21ee70614d72"
          ]
        },
        "id": "ww7zGIvcZznr",
        "outputId": "c38f4e45-d43e-4e07-fb3b-a68cef1a8af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f88848b3dfb049dfa5018b4f4773972e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, GPT2LMHeadModel, AutoTokenizer\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M48uBi6kVgKJ",
        "outputId": "92f1e52a-6add-4f65-956f-e5407dd32653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
        "\n",
        "# dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n",
        "model = (\n",
        "    LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")#,  torch_dtype=torch.float16)\n",
        "    .to(\"cuda\")\n",
        "\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n",
        "\n",
        "\n",
        "# def generate_answers(batch):\n",
        "#     inputs_dict = tokenizer(\n",
        "#         batch[\"article\"], max_length=16384, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "#     )\n",
        "#     input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
        "#     attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
        "#     output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=512, num_beams=2)\n",
        "#     batch[\"predicted_abstract\"] = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "#     return batch\n",
        "\n",
        "\n",
        "# result = dataset.map(generate_answer, batched=True, batch_size=2)\n",
        "# rouge = evaluate.load(\"rouge\")\n",
        "# rouge.compute(predictions=result[\"predicted_abstract\"], references=result[\"abstract\"])"
      ],
      "metadata": {
        "id": "xG8htD3LKxaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\", model_max_length = 1024)\n",
        "\n",
        "# # define model\n",
        "# # model = AutoModelForSeq2SeqLM.from_pretrained(\"gpt2-medium\")\n",
        "# model = GPT2LMHeadModel.from_pretrained('gpt2-medium', pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "jwE1DMe9FaEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_summarization_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"summarization\",\n",
        "    max_length = 512,\n",
        "    do_sample=True,\n",
        "    top_k =5,\n",
        "    num_beams = 4,\n",
        "    device_map= \"auto\",\n",
        "\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=text_summarization_pipeline)"
      ],
      "metadata": {
        "id": "TcvV2EzKV7vM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7928a691-42f7-439d-c995-cf7e5fbcc648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model 'MistralForCausalLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_prompt = \"\"\"\n",
        "You will be given a single passage of a book. This section will be enclosed in triple backticks (```)\n",
        "Your goal is to give a summary of this section so that a reader will have a full understanding of what happened.\n",
        "Your response should be at least three paragraphs and fully encompass what was said in the passage.\n",
        "\n",
        "```{text}```\n",
        "FULL SUMMARY:\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "CLFVjgvIC3P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_chain = load_summarize_chain(llm=llm,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=map_prompt_template)"
      ],
      "metadata": {
        "id": "JKDpbQvUC3Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_docs = [docs[doc] for doc in selected_indices]"
      ],
      "metadata": {
        "id": "OJZAmYyADYgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make an empty list to hold your summaries\n",
        "summary_list = []\n",
        "\n",
        "# Loop through a range of the lenght of your selected docs\n",
        "for i, doc in enumerate(selected_docs):\n",
        "\n",
        "    # Go get a summary of the chunk\n",
        "    chunk_summary = map_chain.run([doc])\n",
        "\n",
        "    # Append that summary to your list\n",
        "    summary_list.append(chunk_summary)\n",
        "\n",
        "    print (f\"Summary #{i} (chunk #{selected_indices[i]}) - Preview: {chunk_summary[:500]} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izaKwo_kDYJ4",
        "outputId": "ce12208c-951d-4695-cd1d-1e3a7ce214bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [2856], which does not match the required output shape [1, 2856]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 23, 128, 1], which does not match the required output shape [1, 23, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary #0 (chunk #0) - Preview: the great gatsby is a novel novel , and it is a novel novel . it is based on a single passage of a book . this article will be given a single passage of a book . this article will be enclosed in triple backticks (  ) the goal is to give a summary of this section so that a reader will have a full understanding of what happened . in this article , we will give a single passage of a book . this article will be enclosed in triple backticks (  ) the goal is to give a summary of this section so that a \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3147], which does not match the required output shape [1, 3147]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 25, 128, 1], which does not match the required output shape [1, 25, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary #1 (chunk #1) - Preview: this article describes the history of the summer that occurred on the evening of a warm windy evening in the summer of 2010 . two young women were buoyed up as though upon an anchored balloon . they were both in white and their dresses were rippling and fluttering as if they had just been blown back in after a short flight around the house . two young women were buoyed up as though upon an anchored balloon . they were both in white and their dresses were rippling and fluttering as if they had ju \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3327], which does not match the required output shape [1, 3327]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 26, 128, 1], which does not match the required output shape [1, 26, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary #2 (chunk #5) - Preview: the authors describe the case of a young woman who was found to have an inexhaustible variety of life . the patient was found to have a family history of a man who had undergone a work in the garage . the patient was found to have a family history of a woman who had been married to her wife , and her wife was shrill , languid , and awful . the patient was found to have an inexhaustible variety of life , and she was found to have an inexhaustible variety of life . the patient was found to have an \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3083], which does not match the required output shape [1, 3083]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 25, 128, 1], which does not match the required output shape [1, 25, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary #3 (chunk #9) - Preview: this article describes a case of a man in a long duster who had dismounted from the wreck and now stood in the middle of the road , looking from the car to the tyre and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the middle of the road and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the middle of the road and from the tyre t \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3495], which does not match the required output shape [1, 3495]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 28, 128, 1], which does not match the required output shape [1, 28, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary #4 (chunk #11) - Preview: this article presents a single passage of a book in a new york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york yo \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3244], which does not match the required output shape [1, 3244]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 26, 128, 1], which does not match the required output shape [1, 26, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary #5 (chunk #16) - Preview: this article describes the history of a young man who was found to have a family history of a young man who had been travelling along the south shore of lake superior for five years . this was a halt in my association with his affairs . this was a halt in my association with his affairs . this was a halt in my association with his affairs . this was a halt in my association with his affairs , and it was a halt in my association with his affairs . \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3067], which does not match the required output shape [1, 3067]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 24, 128, 1], which does not match the required output shape [1, 24, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary #6 (chunk #29) - Preview: the project gutenberg program is a web - based program that is designed to promote the free distribution of electronic works by using or distributing it in the united states without permission and without paying copyright royalties . it is a registered trademark and may not be used if you charge for an eBook , except by following the terms of the trademark license , including paying royalties for use of the project gutenberg trademark . \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [2243], which does not match the required output shape [1, 2243]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 18, 128, 1], which does not match the required output shape [1, 18, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary #7 (chunk #31) - Preview: the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = \"\\n\".join(summary_list)\n",
        "\n",
        "# Convert it back to a document\n",
        "summaries = Document(page_content=summaries)\n",
        "\n",
        "print (f\"Your total summary has {llm.get_num_tokens(summaries.page_content)} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6tvLvS3DX8Q",
        "outputId": "aae360ab-c1a1-43ad-8985-2406880e8ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your total summary has 2923 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summaries.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4py-VHMDXY5",
        "outputId": "380bfd8a-0a88-41e5-afa6-6012cab136e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the great gatsby is a novel novel , and it is a novel novel . it is based on a single passage of a book . this article will be given a single passage of a book . this article will be enclosed in triple backticks (  ) the goal is to give a summary of this section so that a reader will have a full understanding of what happened . in this article , we will give a single passage of a book . this article will be enclosed in triple backticks (  ) the goal is to give a summary of this section so that a reader will have a full understanding of what happened . in addition , we will give a summary of this section so that a reader will have a full understanding of what happened . in addition , we will give a summary of this section so that a reader will have a full understanding of what happened . in addition , we will give a summary of this section so that a reader will have a full understanding of what happened . in addition , we will give a summary of this section so that a reader will have a full understanding of what happened . in addition , we will give a summary of this section so that a reader will have a full understanding of what happened . in addition , we will give a summary of this section so that a reader will have a full understanding of what happened . in addition , we will give a summary of this section so that a reader will have a full understanding of what happened . in addition , we will give a summary of this section so that a reader will have a full understanding of what happened .\n",
            "this article describes the history of the summer that occurred on the evening of a warm windy evening in the summer of 2010 . two young women were buoyed up as though upon an anchored balloon . they were both in white and their dresses were rippling and fluttering as if they had just been blown back in after a short flight around the house . two young women were buoyed up as though upon an anchored balloon . they were both in white and their dresses were rippling and fluttering as if they had just been blown back in after a short flight around the house . the two young women were buoyed up as though upon an anchored balloon , and they were both in white and their dresses were rippling and fluttering as if they had just been blown back in after a short flight around the house . the two young women were buoyed up as though upon an anchored balloon , and they were both in white and their dresses were rippling and fluttering as if they had just been blown back in after a short flight around the house . the two young women were buoyed up as though upon an anchored balloon , and they were both in white and their dresses were rippling and fluttering as if they had just been blown back in after a short flight around the house . the two young women were buoyed up as though upon an anchored balloon , and they were both in white and their dresses were rippling and fluttering as if they had just been blown back in after a short flight around the house . the two young women were buoyed up as though upon an anchored balloon , and they were both in white and their dresses were rippling and fluttering as if they had just been blown back in after a short flight around the house . the second girl was a stranger to me , and her lips fluttered with her nodded at she almost imperceptibly , and then quickly tipped her head back back back back . her lips fluttered with her nodded at she almost imperceptibly , and then quickly tipped her head back back back back . her lips fluttered with\n",
            "the authors describe the case of a young woman who was found to have an inexhaustible variety of life . the patient was found to have a family history of a man who had undergone a work in the garage . the patient was found to have a family history of a woman who had been married to her wife , and her wife was shrill , languid , and awful . the patient was found to have an inexhaustible variety of life , and she was found to have an inexhaustible variety of life . the patient was found to have an inexhaustible variety of life , and she was found to have an inexhaustible variety of life . the patient was found to have an inexhaustible variety of life .\n",
            "this article describes a case of a man in a long duster who had dismounted from the wreck and now stood in the middle of the road , looking from the car to the tyre and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the middle of the road and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the middle of the road and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the middle of the road with looking from the car to the tyre and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the middle of the road with looking from the car to the tyre and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the middle of the road with looking from the car to the tyre and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the middle of the road with looking from the car to the tyre and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the middle of the road with looking from the car to the tyre and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the middle of the road with looking from the car to the tyre and from the tyre to the observers in a pleasant , puzzled way . a man in a long duster had dismounted from the wreck and now stood in the\n",
            "this article presents a single passage of a book in a new york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york york\n",
            "this article describes the history of a young man who was found to have a family history of a young man who had been travelling along the south shore of lake superior for five years . this was a halt in my association with his affairs . this was a halt in my association with his affairs . this was a halt in my association with his affairs . this was a halt in my association with his affairs , and it was a halt in my association with his affairs .\n",
            "the project gutenberg program is a web - based program that is designed to promote the free distribution of electronic works by using or distributing it in the united states without permission and without paying copyright royalties . it is a registered trademark and may not be used if you charge for an eBook , except by following the terms of the trademark license , including paying royalties for use of the project gutenberg trademark .\n",
            "the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service . the project gutenberg literary archivee foundation is a non - nonprofit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the internal revenue service .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3g0xINMpC3Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loaders\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Model\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Embedding Support\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "# Summarizer we'll use for Map Reduce\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "U82Wn0PSzrnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \"\\t\"], chunk_size=5000, chunk_overlap=1000)\n",
        "\n",
        "docs = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "zRw_Q90Hzrj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_documents = len(docs)\n",
        "\n",
        "print (f\"Now our book is split up into {num_documents} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXUh2i5o0Q82",
        "outputId": "39826636-40dc-4846-ccc9-83660c22003e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now our book is split up into 74 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O7AbxHi70Q25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L24cF7PW0QzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "miFy9gBUzrf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Map\n",
        "map_template = \"\"\"The following is a set of documents\n",
        "{docs}\n",
        "Based on this list of docs, please identify the main themes\n",
        "Helpful Answer:\"\"\"\n",
        "map_prompt = PromptTemplate.from_template(map_template)\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
      ],
      "metadata": {
        "id": "T1YwAPe_WGVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce\n",
        "reduce_template = \"\"\"The following is set of summaries:\n",
        "{docs}\n",
        "Take these and distill it into a final, consolidated summary of the main themes.\n",
        "Helpful Answer:\"\"\"\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_template)"
      ],
      "metadata": {
        "id": "Pe0xpHrfWGSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ChatPromptTemplate(input_variables=['docs'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['docs'], template='The following is a set of documents:\\n{docs}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:'))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDINUCLWWGPP",
        "outputId": "7610c763-e966-43fb-b21f-1a788f0c5812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['docs'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['docs'], template='The following is a set of documents:\\n{docs}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:'))])"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run chain\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
        ")\n",
        "\n",
        "# Combines and iteratively reduces the mapped documents\n",
        "reduce_documents_chain = ReduceDocumentsChain(\n",
        "    # This is final chain that is called.\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    # If documents exceed context for `StuffDocumentsChain`\n",
        "    collapse_documents_chain=combine_documents_chain,\n",
        "    # The maximum number of tokens to group documents into.\n",
        "    token_max=1024,\n",
        ")"
      ],
      "metadata": {
        "id": "F5GGAIXjjV92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "AOLMHDF7jV6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:1300]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "gGQUP2ZOEvKx",
        "outputId": "23336373-1434-4c41-ec78-843c81047944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffThe Project Gutenberg eBook of The Great Gatsby\\n    \\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this ebook or online\\nat www.gutenberg.org. If you are not located in the United States,\\nyou will have to check the laws of the country where you are located\\nbefore using this eBook.\\n\\nTitle: The Great Gatsby\\n\\n\\nAuthor: F. Scott Fitzgerald\\n\\nRelease date: January 17, 2021 [eBook #64317]\\n                Most recently updated: February 2, 2024\\n\\nLanguage: English\\n\\nCredits: Produced by Alex Cabal for the Standard Ebooks project, based on a transcription produced for Project Gutenberg Australia.\\n\\n\\n*** START OF THE PROJECT GUTENBERG EBOOK THE GREAT GATSBY ***\\n\\n\\n\\n\\n                           The Great Gatsby\\n                                  by\\n                          F. Scott Fitzgerald\\n\\n\\n                           Table of Contents\\n\\nI\\nII\\nIII\\nIV\\nV\\nVI\\nVII\\nVIII\\nIX\\n\\n\\n                              Once again\\n                                  to\\n                                 Zelda\\n\\n\\n  Then wear the gold hat, if that will move her;\\n  If you can bounce high, bounce for her too,\\n  Til'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn0SwCEwvfAj",
        "outputId": "8e227048-d350-4118-a7f2-4a256ecf93cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "290106"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = llm.get_num_tokens(text)\n",
        "\n",
        "print (f\"This book has {num_tokens} tokens in it\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXwsYrpgctjI",
        "outputId": "03aed8a2-4a90-4261-80db-5acb0cc96e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This book has 82477 tokens in it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install --upgrade --quiet  tiktoken"
      ],
      "metadata": {
        "id": "ReNYi2UG9IUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loaders\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Splitters\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Model\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Embedding Support\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Summarizer we'll use for Map Reduce\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "GWw5tszAaidJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1200, chunk_overlap=20\n",
        ")"
      ],
      "metadata": {
        "id": "88UiAMT_8sBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\", \" \"], chunk_size=8192, chunk_overlap=30, length_function=len,)\n",
        "\n",
        "docs = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "_m2NyppqaiaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_documents = len(docs)\n",
        "\n",
        "print (f\"Now our book is split up into {num_documents} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-05Q_Up1aiWq",
        "outputId": "368b6b65-0f8f-4625-db36-00672c38328e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now our book is split up into 37 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining documents by mapping a chain over them, then combining results\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        "    verbose = True,\n",
        ")\n",
        "\n",
        "\n",
        "split_docs = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "MEjbP96SlI04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "output = map_reduce_chain.invoke(split_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O4P09jo8lIqs",
        "outputId": "205e1ba1-3d42-40a0-fa8b-1dd0696c04a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1839 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 800, but your input_length is only 452. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=226)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 709. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=354)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 677. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=338)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 367. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=183)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 307. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=153)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 620. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=310)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 763. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=381)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 730. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=365)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 695. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=347)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 681. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=340)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 653. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=326)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 611. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=305)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 639. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=319)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 667. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=333)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 625. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=312)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 709. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=354)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 681. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=340)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 695. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=347)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 653. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=326)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "Your max_length is set to 800, but your input_length is only 489. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=244)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             outputs = (\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/map_reduce.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         ]\n\u001b[0;32m--> 236\u001b[0;31m         result, extra_return_dict = self.reduce_documents_chain.combine_docs(\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0mresult_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/reduce.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0melement\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mother\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \"\"\"\n\u001b[0;32m--> 243\u001b[0;31m         result_docs, extra_return_dict = self._collapse(\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/reduce.py\u001b[0m in \u001b[0;36m_collapse\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mresult_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_result_doc_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m                 \u001b[0mnew_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollapse_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_collapse_docs_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0mresult_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mnum_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlength_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/reduce.py\u001b[0m in \u001b[0;36mcollapse_docs\u001b[0;34m(docs, combine_document_func, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0mare\u001b[0m \u001b[0mjoined\u001b[0m \u001b[0mby\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_document_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mcombined_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/reduce.py\u001b[0m in \u001b[0;36m_collapse_docs_func\u001b[0;34m(docs, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_collapse_docs_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             return self._collapse_chain.run(\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0minput_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    361\u001b[0m         }\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             outputs = (\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/stuff.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# Call predict on the LLM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     async def acombine_docs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"funny\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    361\u001b[0m         }\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             outputs = (\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    567\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m                 )\n\u001b[1;32m    740\u001b[0m             ]\n\u001b[0;32m--> 741\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             output = (\n\u001b[0;32m--> 592\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    593\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/llms/huggingface_pipeline.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# Process batch of prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpipeline_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;31m# Process each response in the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \"\"\"\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         if (\n\u001b[1;32m    169\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 )\n\u001b[0;32m-> 1143\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1066\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         )\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1526\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2623\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1744\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                 )\n\u001b[1;32m   1109\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1111\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mquery_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             cross_attention_outputs = self.layer[1](\n\u001b[0m\u001b[1;32m    725\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_copy_to_script_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ModuleList'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output['output_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "IB5yiJ743p9W",
        "outputId": "ff7e15b4-be10-45b3-9e6b-3bf56f315a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love, love'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define prompt\n",
        "prompt_template = \"\"\"Write a concise summary of the following:\n",
        "\"{text}\"\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)"
      ],
      "metadata": {
        "id": "J7qTq5p_QcnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Define StuffDocumentsChain\n",
        "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
        "\n",
        "\n",
        "print(stuff_chain.run(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PTV6lH-N4sk",
        "outputId": "fd7d8642-efe5-4d61-e7b0-10e95e285a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (91142 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "vectors = embeddings.embed_documents([x.page_content for x in docs])"
      ],
      "metadata": {
        "id": "mu1UAOQRc8ZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a80dc1-f10d-4729-e3b2-775cb2c53d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name t5-large. Creating a new one with MEAN pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vectors[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwEouf2hdCt4",
        "outputId": "5e69ae60-52e1-4657-e344-153b4366d6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO5Ublwm6XbE",
        "outputId": "d4d26551-dce4-4399-a48e-ed76c5588416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.0246950164437294,\n",
              " -0.049805961549282074,\n",
              " 0.013617407530546188,\n",
              " 0.006391843780875206,\n",
              " 0.016294404864311218,\n",
              " 0.04171421751379967,\n",
              " 0.09312186390161514,\n",
              " 0.07349852472543716,\n",
              " -0.054893434047698975,\n",
              " -0.0396709069609642,\n",
              " 0.22199657559394836,\n",
              " -0.00904484000056982,\n",
              " -0.03472462669014931,\n",
              " 0.015074074268341064,\n",
              " -0.00399515638127923,\n",
              " 0.005794224329292774,\n",
              " -0.04130904749035835,\n",
              " -0.05434015765786171,\n",
              " 0.001101319445297122,\n",
              " 0.11893370002508163,\n",
              " -0.016603508964180946,\n",
              " 0.26355719566345215,\n",
              " -0.02930924855172634,\n",
              " -0.01956118643283844,\n",
              " -0.02051701955497265,\n",
              " -0.03630324453115463,\n",
              " -0.007647854741662741,\n",
              " -0.05820630490779877,\n",
              " -0.016669228672981262,\n",
              " 0.012905185110867023,\n",
              " 0.027155708521604538,\n",
              " 0.008759928867220879,\n",
              " -0.03157742694020271,\n",
              " 0.05758325010538101,\n",
              " -0.03561120107769966,\n",
              " -0.021108321845531464,\n",
              " -0.057831332087516785,\n",
              " -0.03785552829504013,\n",
              " 0.05429385229945183,\n",
              " 0.0531940832734108,\n",
              " -0.05333776772022247,\n",
              " -0.0110468789935112,\n",
              " 0.07530857622623444,\n",
              " 0.036324236541986465,\n",
              " 0.06865672767162323,\n",
              " 0.016981186345219612,\n",
              " 0.11797283589839935,\n",
              " -0.0013715260429307818,\n",
              " -0.03843187168240547,\n",
              " -0.0030771009624004364,\n",
              " 0.05785089731216431,\n",
              " 0.04686726629734039,\n",
              " 0.005046004895120859,\n",
              " 0.00661133136600256,\n",
              " -0.035834021866321564,\n",
              " 0.03636375814676285,\n",
              " 0.018088888376951218,\n",
              " -0.0434807650744915,\n",
              " -0.0898682177066803,\n",
              " -0.004402525722980499,\n",
              " 0.0025883729103952646,\n",
              " -0.02829667553305626,\n",
              " 0.053122036159038544,\n",
              " -0.016257990151643753,\n",
              " -0.08480755239725113,\n",
              " 0.013740628026425838,\n",
              " 0.006268308497965336,\n",
              " -0.019176851958036423,\n",
              " -0.06078345328569412,\n",
              " -0.04736693575978279,\n",
              " -0.060671236366033554,\n",
              " 0.04468725994229317,\n",
              " -0.035461779683828354,\n",
              " -0.060243960469961166,\n",
              " 0.000514410377945751,\n",
              " -0.06738265603780746,\n",
              " -0.00011456071661086753,\n",
              " -0.030722495168447495,\n",
              " 0.12301869690418243,\n",
              " 0.1728605479001999,\n",
              " -0.046608202159404755,\n",
              " 0.06097196415066719,\n",
              " 0.1272667795419693,\n",
              " 0.08415111154317856,\n",
              " -0.0045204078778624535,\n",
              " -0.11224536597728729,\n",
              " 0.0067346529103815556,\n",
              " 0.06536315381526947,\n",
              " -0.009598427452147007,\n",
              " 0.08662780374288559,\n",
              " -0.02415122650563717,\n",
              " 0.03042997233569622,\n",
              " 0.12023313343524933,\n",
              " -0.013153641484677792,\n",
              " 0.0009931519161909819,\n",
              " 0.013834391720592976,\n",
              " -0.13214333355426788,\n",
              " 0.0351407527923584,\n",
              " -0.0072387754917144775,\n",
              " 0.024114059284329414,\n",
              " -0.009204312227666378,\n",
              " -0.021165095269680023,\n",
              " -0.04262945428490639,\n",
              " -0.04207076504826546,\n",
              " -0.0999024510383606,\n",
              " -0.08026812970638275,\n",
              " 0.023535622283816338,\n",
              " -0.06590136885643005,\n",
              " -0.05273259058594704,\n",
              " -0.034987714141607285,\n",
              " -0.051944781094789505,\n",
              " -0.01954546384513378,\n",
              " 0.08613528311252594,\n",
              " 0.01080892700701952,\n",
              " -0.06764525920152664,\n",
              " 0.00943798292428255,\n",
              " -0.04585491493344307,\n",
              " 0.022081606090068817,\n",
              " -0.028031963855028152,\n",
              " -0.01901162974536419,\n",
              " 0.029897237196564674,\n",
              " -0.042948223650455475,\n",
              " 0.07543490827083588,\n",
              " -0.05706995725631714,\n",
              " -0.08172710984945297,\n",
              " 0.03736284747719765,\n",
              " 0.0047891526482999325,\n",
              " 0.05514802038669586,\n",
              " 0.004436010029166937,\n",
              " -0.044842902570962906,\n",
              " -0.007414458319544792,\n",
              " -0.01619618944823742,\n",
              " -0.057151634246110916,\n",
              " 0.034626081585884094,\n",
              " -0.024787593632936478,\n",
              " -0.07474952936172485,\n",
              " 0.15630611777305603,\n",
              " -0.02516498789191246,\n",
              " 0.049036186188459396,\n",
              " 0.012307994067668915,\n",
              " -0.056083906441926956,\n",
              " 0.030414050444960594,\n",
              " 0.07100760191679001,\n",
              " -0.02241586521267891,\n",
              " 0.0729808658361435,\n",
              " -0.05364177003502846,\n",
              " -0.0306331068277359,\n",
              " -0.027843696996569633,\n",
              " -0.06314962357282639,\n",
              " -0.007912125438451767,\n",
              " 0.06655382364988327,\n",
              " 0.004607971291989088,\n",
              " 0.02458682283759117,\n",
              " -0.05633436515927315,\n",
              " -0.09981275349855423,\n",
              " -0.04327419027686119,\n",
              " 0.05942131206393242,\n",
              " 0.06400661915540695,\n",
              " -0.10371751338243484,\n",
              " 0.017650242894887924,\n",
              " 0.06243954598903656,\n",
              " -0.072100430727005,\n",
              " 0.08264952898025513,\n",
              " 0.011784212663769722,\n",
              " -0.04862949252128601,\n",
              " 0.033903732895851135,\n",
              " 0.005745504517108202,\n",
              " -0.015017143450677395,\n",
              " 0.015286795794963837,\n",
              " -0.1030157133936882,\n",
              " 0.010529021732509136,\n",
              " -0.3851558268070221,\n",
              " -0.01817234791815281,\n",
              " 0.09881822764873505,\n",
              " 0.4820769429206848,\n",
              " -0.03547274321317673,\n",
              " 0.05401751399040222,\n",
              " -0.005237391218543053,\n",
              " -0.04437560588121414,\n",
              " -0.057673271745443344,\n",
              " -0.03416147828102112,\n",
              " -0.05467694252729416,\n",
              " -0.042571261525154114,\n",
              " -0.041918203234672546,\n",
              " -0.04203055053949356,\n",
              " 0.16207915544509888,\n",
              " -0.012478949502110481,\n",
              " 0.01491357758641243,\n",
              " 0.03448210656642914,\n",
              " -0.0012906488263979554,\n",
              " 0.03825902193784714,\n",
              " -0.052567288279533386,\n",
              " -0.010724471881985664,\n",
              " 0.026902614161372185,\n",
              " 0.01783701963722706,\n",
              " 0.009752173908054829,\n",
              " 0.009614625945687294,\n",
              " 0.00034491298720240593,\n",
              " 0.00843519251793623,\n",
              " -0.025733262300491333,\n",
              " 0.06324707716703415,\n",
              " -0.008397244848310947,\n",
              " -0.06681215018033981,\n",
              " 0.07317031919956207,\n",
              " 0.024150900542736053,\n",
              " 0.02794055826961994,\n",
              " 0.016394048929214478,\n",
              " 0.1150430217385292,\n",
              " 0.034168753772974014,\n",
              " -0.003154707606881857,\n",
              " 0.03733152523636818,\n",
              " -0.006390420254319906,\n",
              " -0.030648482963442802,\n",
              " -0.034815773367881775,\n",
              " 0.04806084930896759,\n",
              " -0.0025812098756432533,\n",
              " -0.0974045917391777,\n",
              " 0.01654844358563423,\n",
              " 0.12541154026985168,\n",
              " -0.04372003301978111,\n",
              " 0.015881814062595367,\n",
              " 0.014136961661279202,\n",
              " -0.06657791882753372,\n",
              " 0.04497063532471657,\n",
              " 0.004334971308708191,\n",
              " -0.040052544325590134,\n",
              " 0.0022898295428603888,\n",
              " 0.07866788655519485,\n",
              " -0.06713391840457916,\n",
              " -0.031475480645895004,\n",
              " -0.0012315562926232815,\n",
              " -0.07091505080461502,\n",
              " 0.10861503332853317,\n",
              " 0.010651196353137493,\n",
              " -0.018459388986229897,\n",
              " -0.05480154603719711,\n",
              " 0.019650008529424667,\n",
              " -0.004216550383716822,\n",
              " -0.05684509500861168,\n",
              " 0.001978137530386448,\n",
              " -0.028184924274683,\n",
              " 0.06574422121047974,\n",
              " 0.020517706871032715,\n",
              " -0.19976653158664703,\n",
              " 0.023143528029322624,\n",
              " -0.044581249356269836,\n",
              " -0.0014259697636589408,\n",
              " -0.04277615621685982,\n",
              " -0.09009446948766708,\n",
              " 0.027626048773527145,\n",
              " -0.03293714299798012,\n",
              " -0.04994567483663559,\n",
              " 0.013415350578725338,\n",
              " 0.01922181248664856,\n",
              " -0.021654369309544563,\n",
              " -0.2538844645023346,\n",
              " -0.012047948315739632,\n",
              " -0.012213923037052155,\n",
              " -0.10384293645620346,\n",
              " -0.030425263568758965,\n",
              " 0.014368174597620964,\n",
              " -0.022294776514172554,\n",
              " 0.06628391891717911,\n",
              " -0.0204929169267416,\n",
              " -0.008137083612382412,\n",
              " -0.059487223625183105,\n",
              " -0.04143141582608223,\n",
              " -0.0007557698409073055,\n",
              " 0.007700384594500065,\n",
              " 0.16898322105407715,\n",
              " 0.02671574428677559,\n",
              " 0.03604201227426529,\n",
              " 0.059937480837106705,\n",
              " -0.0074470555409789085,\n",
              " -0.005923531949520111,\n",
              " -0.057629480957984924,\n",
              " -0.0071280160918831825,\n",
              " -0.03247825801372528,\n",
              " 0.16647405922412872,\n",
              " 0.03042905405163765,\n",
              " -0.012260353192687035,\n",
              " -0.04071103036403656,\n",
              " -0.4566592276096344,\n",
              " 0.02283850871026516,\n",
              " 0.021314291283488274,\n",
              " -0.04110220819711685,\n",
              " 0.030805181711912155,\n",
              " -0.02236628718674183,\n",
              " 0.045639824122190475,\n",
              " 0.046386297792196274,\n",
              " -0.0022758152335882187,\n",
              " 0.09421197324991226,\n",
              " 0.04359104484319687,\n",
              " -0.008900530636310577,\n",
              " 0.02006886713206768,\n",
              " 0.004885037429630756,\n",
              " 0.006136598065495491,\n",
              " 0.15830542147159576,\n",
              " -0.002517695538699627,\n",
              " -0.04690379649400711,\n",
              " 0.4056163430213928,\n",
              " -0.05449045076966286,\n",
              " -0.010627464391291142,\n",
              " -0.011379522271454334,\n",
              " -0.017275216057896614,\n",
              " -0.009995173662900925,\n",
              " 0.03736194223165512,\n",
              " 0.0490717850625515,\n",
              " -0.03852412477135658,\n",
              " -0.013122368603944778,\n",
              " -0.06395439058542252,\n",
              " 0.00462071830406785,\n",
              " 0.11319732666015625,\n",
              " -0.09077063947916031,\n",
              " -0.018326634541153908,\n",
              " 0.032723017036914825,\n",
              " 0.03187203034758568,\n",
              " 0.036769770085811615,\n",
              " 0.086918406188488,\n",
              " -0.03508267179131508,\n",
              " -0.046559445559978485,\n",
              " 0.2564016282558441,\n",
              " -0.04931267723441124,\n",
              " -0.032201897352933884,\n",
              " 0.14137348532676697,\n",
              " -0.036830805242061615,\n",
              " 0.08319494128227234,\n",
              " -0.017115386202931404,\n",
              " 0.01160433515906334,\n",
              " 0.07764219492673874,\n",
              " -0.01571490429341793,\n",
              " 0.01518641784787178,\n",
              " 0.038440823554992676,\n",
              " -0.025623055174946785,\n",
              " 0.0485992394387722,\n",
              " 0.08197341114282608,\n",
              " 0.05925813317298889,\n",
              " 0.04788777977228165,\n",
              " -0.03817156329751015,\n",
              " 0.022128332406282425,\n",
              " 0.08499520272016525,\n",
              " 0.014219710603356361,\n",
              " -0.004378796089440584,\n",
              " -0.02249155379831791,\n",
              " -0.011541974730789661,\n",
              " -0.035799745470285416,\n",
              " -0.07997199147939682,\n",
              " -0.02365889772772789,\n",
              " -0.005187212955206633,\n",
              " 0.009099946357309818,\n",
              " -0.018180906772613525,\n",
              " -0.015819557011127472,\n",
              " 0.0206531323492527,\n",
              " 0.05163935199379921,\n",
              " -0.01733984611928463,\n",
              " -0.007838256657123566,\n",
              " -0.029166843742132187,\n",
              " 0.05348367989063263,\n",
              " -0.027142029255628586,\n",
              " -0.001453016186133027,\n",
              " -0.08232706785202026,\n",
              " -0.0026980252005159855,\n",
              " -0.07499703764915466,\n",
              " 0.024583106860518456,\n",
              " -0.05027582123875618,\n",
              " 0.05313996970653534,\n",
              " -0.10530968010425568,\n",
              " -0.018443331122398376,\n",
              " 0.06320212781429291,\n",
              " -0.017594637349247932,\n",
              " 0.01973411999642849,\n",
              " -0.08112428337335587,\n",
              " -0.03643300011754036,\n",
              " -0.018265003338456154,\n",
              " -0.3472415804862976,\n",
              " -0.07118278741836548,\n",
              " 1.411661105521489e-05,\n",
              " 0.027156421914696693,\n",
              " 0.0298609621822834,\n",
              " -0.06966409832239151,\n",
              " -0.006382565014064312,\n",
              " -0.0038651260547339916,\n",
              " -0.03394324705004692,\n",
              " 0.012162446044385433,\n",
              " 0.002793984953314066,\n",
              " -0.0416744202375412,\n",
              " -0.01419744547456503,\n",
              " 0.0501270517706871,\n",
              " -0.043723318725824356,\n",
              " -0.08983724564313889,\n",
              " -0.07678070664405823,\n",
              " 0.0401461198925972,\n",
              " -0.06345484405755997,\n",
              " -0.1106865257024765,\n",
              " -0.10950672626495361,\n",
              " -0.013273649848997593,\n",
              " -0.04641256481409073,\n",
              " -0.09592290967702866,\n",
              " 0.0010221180273219943,\n",
              " -0.05444664880633354,\n",
              " 0.027884799987077713,\n",
              " 0.053048957139253616,\n",
              " -0.008500891737639904,\n",
              " 0.03209627419710159,\n",
              " -0.04894355311989784,\n",
              " -0.001764877699315548,\n",
              " -0.04406682029366493,\n",
              " -0.0304712001234293,\n",
              " 0.018297435715794563,\n",
              " -0.032444000244140625,\n",
              " -0.03254099190235138,\n",
              " -0.04905804991722107,\n",
              " 0.006765694357454777,\n",
              " 0.005956334061920643,\n",
              " 0.014467661269009113,\n",
              " -0.025464942678809166,\n",
              " 0.00752219557762146,\n",
              " -0.07695891708135605,\n",
              " -0.022670932114124298,\n",
              " -0.04938201606273651,\n",
              " 0.010793758556246758,\n",
              " 0.03254679962992668,\n",
              " 0.07317489385604858,\n",
              " -0.03063623234629631,\n",
              " 0.07582929730415344,\n",
              " 0.10534369200468063,\n",
              " -0.0027387829031795263,\n",
              " -0.04166954383254051,\n",
              " -0.028336742892861366,\n",
              " 0.04993117228150368,\n",
              " 0.05115453153848648,\n",
              " -0.024141643196344376,\n",
              " 0.10386580973863602,\n",
              " 0.12204010784626007,\n",
              " 0.03139028698205948,\n",
              " 0.009495344944298267,\n",
              " -0.059168651700019836,\n",
              " 0.027840515598654747,\n",
              " 0.0323660671710968,\n",
              " -0.03833339363336563,\n",
              " -0.013599374331533909,\n",
              " -0.12403230369091034,\n",
              " -0.03760453313589096,\n",
              " -0.004252327606081963,\n",
              " -0.015693996101617813,\n",
              " -0.08260540664196014,\n",
              " -0.09904718399047852,\n",
              " -0.016744263470172882,\n",
              " -0.0013507968978956342,\n",
              " 0.012443962506949902,\n",
              " 0.07458184659481049,\n",
              " -0.06867548078298569,\n",
              " 0.027562126517295837,\n",
              " -0.005375117063522339,\n",
              " 0.021131185814738274,\n",
              " -0.026791013777256012,\n",
              " -0.04486158862709999,\n",
              " -0.03984475135803223,\n",
              " -0.06826595962047577,\n",
              " 0.029017584398388863,\n",
              " -0.0346301831305027,\n",
              " 0.06887814402580261,\n",
              " 0.05433957278728485,\n",
              " 0.04525277763605118,\n",
              " 0.006024855654686689,\n",
              " 0.10612645745277405,\n",
              " -0.007573547773063183,\n",
              " -0.026010196655988693,\n",
              " -0.0525507852435112,\n",
              " 0.02161862514913082,\n",
              " 0.010949283838272095,\n",
              " 0.07282347232103348,\n",
              " -0.08870141953229904,\n",
              " -0.03498358279466629,\n",
              " 0.04258256405591965,\n",
              " -0.3754936456680298,\n",
              " 0.13597235083580017,\n",
              " 0.053806740790605545,\n",
              " -0.04295063391327858,\n",
              " 0.10813922435045242,\n",
              " 0.023998547345399857,\n",
              " 0.06676816940307617,\n",
              " -0.012075492180883884,\n",
              " -0.023765545338392258,\n",
              " -0.029677357524633408,\n",
              " 0.030946532264351845,\n",
              " 0.012992608360946178,\n",
              " -0.02731418050825596,\n",
              " 0.08203055709600449,\n",
              " -0.01239374466240406,\n",
              " -0.015184311196208,\n",
              " -0.007637166418135166,\n",
              " 0.011198502033948898,\n",
              " -0.009784489870071411,\n",
              " -0.022148583084344864,\n",
              " -0.10968783497810364,\n",
              " 0.01149586122483015,\n",
              " 0.03493110463023186,\n",
              " 0.0288078710436821,\n",
              " -0.03251595422625542,\n",
              " -0.05460977926850319,\n",
              " 0.020985864102840424,\n",
              " -0.07703032344579697,\n",
              " -0.04060782864689827,\n",
              " 0.008215813897550106,\n",
              " 0.09746202826499939,\n",
              " 0.10943477600812912,\n",
              " -0.025273675099015236,\n",
              " -0.04837409034371376,\n",
              " -0.009876196272671223,\n",
              " -0.030950969085097313,\n",
              " -0.030952490866184235,\n",
              " -0.047084659337997437,\n",
              " -0.012412410229444504,\n",
              " 0.03235282376408577,\n",
              " 0.08727089315652847,\n",
              " -0.019895218312740326,\n",
              " -0.017608527094125748,\n",
              " -0.08641612529754639,\n",
              " -0.012027289718389511,\n",
              " -0.04672384634613991,\n",
              " 0.04649253562092781,\n",
              " 0.03051382675766945,\n",
              " 0.001959071960300207,\n",
              " 0.008885654620826244,\n",
              " -0.3408326506614685,\n",
              " -0.08383332937955856,\n",
              " 0.0006631037103943527,\n",
              " -0.01114755216985941,\n",
              " -0.02517342008650303,\n",
              " -0.024597780779004097,\n",
              " 0.06813577562570572,\n",
              " 0.024095473811030388,\n",
              " -0.06254038214683533,\n",
              " -0.009104480035603046,\n",
              " -0.029657991603016853,\n",
              " -0.0007753184181638062,\n",
              " -0.08923967182636261,\n",
              " -0.11955397576093674,\n",
              " -0.15458594262599945,\n",
              " -0.002124263672158122,\n",
              " 0.029617536813020706,\n",
              " 0.015692144632339478,\n",
              " 0.04446115717291832,\n",
              " -0.06711016595363617,\n",
              " 0.004332373850047588,\n",
              " 0.0028108053375035524,\n",
              " -0.02796695940196514,\n",
              " -0.1183403730392456,\n",
              " 0.010950809344649315,\n",
              " 0.019177358597517014,\n",
              " 0.011788345873355865,\n",
              " -0.03229924663901329,\n",
              " 0.013979444280266762,\n",
              " 0.10331887006759644,\n",
              " 0.18610405921936035,\n",
              " 0.025329556316137314,\n",
              " -0.011596503667533398,\n",
              " 0.02095494419336319,\n",
              " -0.0387573167681694,\n",
              " -0.07646088302135468,\n",
              " 0.0965803861618042,\n",
              " -0.11237730830907822,\n",
              " -0.0645998865365982,\n",
              " -0.02278316393494606,\n",
              " -0.010286042466759682,\n",
              " 0.027951566502451897,\n",
              " 0.023956693708896637,\n",
              " 0.23355942964553833,\n",
              " 0.010716155171394348,\n",
              " 0.033834412693977356,\n",
              " -0.2093619406223297,\n",
              " -0.027383165434002876,\n",
              " -0.017783885821700096,\n",
              " 0.024636339396238327,\n",
              " 0.0431811660528183,\n",
              " -0.03679628297686577,\n",
              " 0.024466076865792274,\n",
              " -0.005647796671837568,\n",
              " 0.010577602311968803,\n",
              " -0.12783657014369965,\n",
              " 0.08906906843185425,\n",
              " -0.04720760136842728,\n",
              " 0.24072173237800598,\n",
              " -0.07431057840585709,\n",
              " -0.03343039005994797,\n",
              " 0.018390370532870293,\n",
              " -0.024599356576800346,\n",
              " 0.05502057075500488,\n",
              " 0.29483553767204285,\n",
              " 0.12065494805574417,\n",
              " -0.07375673949718475,\n",
              " -0.05344308167695999,\n",
              " -0.04663601890206337,\n",
              " -0.05564388632774353,\n",
              " -0.10960780829191208,\n",
              " -0.02456202730536461,\n",
              " 0.07250014692544937,\n",
              " 0.06467225402593613,\n",
              " 0.0013018699828535318,\n",
              " -0.20568963885307312,\n",
              " -0.011314928531646729,\n",
              " 0.1563713401556015,\n",
              " 0.0330863781273365,\n",
              " -0.08724720031023026,\n",
              " -0.04001515358686447,\n",
              " -0.03529427573084831,\n",
              " 0.023551329970359802,\n",
              " -0.02562609128654003,\n",
              " 0.004749635234475136,\n",
              " 0.005477658472955227,\n",
              " 0.0062170992605388165,\n",
              " -0.03466041013598442,\n",
              " -0.011108433827757835,\n",
              " 0.016373194754123688,\n",
              " -0.09121961891651154,\n",
              " 0.025406021624803543,\n",
              " -0.016494065523147583,\n",
              " 0.10795360058546066,\n",
              " -0.04926653206348419,\n",
              " 0.008449018932878971,\n",
              " 0.019220400601625443,\n",
              " -0.021961212158203125,\n",
              " 0.012559239752590656,\n",
              " -0.01395502034574747,\n",
              " -0.06627100706100464,\n",
              " -0.07555931806564331,\n",
              " 0.08068617433309555,\n",
              " 0.008902856148779392,\n",
              " -0.07221225649118423,\n",
              " -0.009771609678864479,\n",
              " -0.01783733442425728,\n",
              " -0.02456752210855484,\n",
              " 0.01840529404580593,\n",
              " -0.05040908232331276,\n",
              " -0.04002976417541504,\n",
              " 0.018513673916459084,\n",
              " -0.00518555473536253,\n",
              " -0.01688552089035511,\n",
              " -0.006205915939062834,\n",
              " 0.0587707944214344,\n",
              " 0.06758803129196167,\n",
              " -0.022169096395373344,\n",
              " -0.013552341610193253,\n",
              " -0.40442413091659546,\n",
              " -0.008518275804817677,\n",
              " 0.0242290161550045,\n",
              " 0.028927454724907875,\n",
              " -0.024068094789981842,\n",
              " 0.00308853923343122,\n",
              " -0.027331283316016197,\n",
              " -0.024927537888288498,\n",
              " -0.016864968463778496,\n",
              " 0.02172992005944252,\n",
              " -0.030677253380417824,\n",
              " 0.0033162636682391167,\n",
              " -0.035589493811130524,\n",
              " 0.01629037596285343,\n",
              " 0.2600798010826111,\n",
              " -0.02052239701151848,\n",
              " -0.022012632340192795,\n",
              " -0.04465678334236145,\n",
              " -0.030181415379047394,\n",
              " -0.08828859031200409,\n",
              " -0.12037229537963867,\n",
              " 0.0329824760556221,\n",
              " 0.012215960770845413,\n",
              " -0.0052146087400615215,\n",
              " 0.03271796926856041,\n",
              " -0.012478484772145748,\n",
              " -0.007479014806449413,\n",
              " 0.11909569054841995,\n",
              " 0.07662464678287506,\n",
              " -0.016433943063020706,\n",
              " 0.21504636108875275,\n",
              " 0.03699754923582077,\n",
              " -0.014011697843670845,\n",
              " -0.05237739905714989,\n",
              " -0.009971876628696918,\n",
              " 0.03362131491303444,\n",
              " -0.007692728191614151,\n",
              " 0.027195695787668228,\n",
              " -0.015216171741485596,\n",
              " -0.012120913714170456,\n",
              " 0.04149438813328743,\n",
              " -0.032551102340221405,\n",
              " 0.038067951798439026,\n",
              " 0.010801143944263458,\n",
              " -0.09468935430049896,\n",
              " 0.005225731059908867,\n",
              " 0.09111860394477844,\n",
              " -0.023513562977313995,\n",
              " 0.1657705008983612,\n",
              " 0.028813118115067482,\n",
              " -0.09146975725889206,\n",
              " 0.012539926916360855,\n",
              " 0.0413348563015461,\n",
              " 0.06867152452468872,\n",
              " 0.04074118286371231,\n",
              " -0.01363900862634182,\n",
              " -0.10077647864818573,\n",
              " -0.06162765249609947,\n",
              " -0.031881608068943024,\n",
              " -0.023262107744812965,\n",
              " -0.041241105645895004,\n",
              " -0.03608553111553192,\n",
              " 0.01876034028828144,\n",
              " 0.026649972423911095,\n",
              " -0.08123137801885605,\n",
              " -0.03503846749663353,\n",
              " 0.023319611325860023,\n",
              " -0.03701699152588844,\n",
              " 0.05991768091917038,\n",
              " -0.07854986190795898,\n",
              " 0.0631665363907814,\n",
              " 0.057210396975278854,\n",
              " -0.00409098993986845,\n",
              " -0.09895127266645432,\n",
              " 0.0840751975774765,\n",
              " 0.044248852878808975,\n",
              " 0.07837943732738495,\n",
              " 0.028060007840394974,\n",
              " -0.013824978843331337,\n",
              " 0.040776681154966354,\n",
              " -0.04499008506536484,\n",
              " 0.019278431311249733,\n",
              " 0.0589161142706871,\n",
              " 0.018669821321964264,\n",
              " -0.008713919669389725,\n",
              " 0.007909715175628662,\n",
              " 0.02830059826374054,\n",
              " 0.07264719158411026,\n",
              " 0.050704650580883026,\n",
              " -0.12084440141916275,\n",
              " -0.051905978471040726,\n",
              " 0.06370611488819122,\n",
              " -0.11695901304483414,\n",
              " -0.06745348870754242,\n",
              " -0.084255151450634,\n",
              " -0.04212844371795654,\n",
              " -0.03840132802724838,\n",
              " 0.02245696820318699,\n",
              " 0.03672238439321518,\n",
              " -0.01582566276192665,\n",
              " 0.07549136132001877,\n",
              " 0.004779938608407974,\n",
              " 0.030763400718569756,\n",
              " -0.01676543988287449,\n",
              " -0.048038821667432785,\n",
              " -0.02351347729563713,\n",
              " -0.00649662921205163,\n",
              " -0.016390880569815636,\n",
              " 0.0571066290140152,\n",
              " -0.02385912649333477,\n",
              " 0.0016613253392279148,\n",
              " 0.017835738137364388,\n",
              " 0.0007545090629719198,\n",
              " 2.895112083933782e-05,\n",
              " -0.054249025881290436,\n",
              " -0.016670728102326393,\n",
              " -0.07027792930603027,\n",
              " -0.09683447331190109,\n",
              " 0.03183017298579216,\n",
              " -0.03110983781516552,\n",
              " 0.01348193921148777,\n",
              " 0.1100914478302002,\n",
              " -0.013489272445440292,\n",
              " 0.034392304718494415,\n",
              " -0.02763913944363594,\n",
              " 0.02141285501420498,\n",
              " -0.018311651423573494,\n",
              " 0.06085413321852684,\n",
              " 0.009209413081407547,\n",
              " -0.05604928731918335,\n",
              " 0.038443226367235184,\n",
              " -0.081940658390522,\n",
              " 0.05377722531557083,\n",
              " 0.0508662648499012,\n",
              " -0.06300660967826843,\n",
              " -0.01199954841285944,\n",
              " 0.05365494266152382,\n",
              " 0.00039100999129004776,\n",
              " -0.10270170122385025,\n",
              " -0.08406198769807816,\n",
              " -0.0466415137052536,\n",
              " -0.029590409249067307,\n",
              " 0.01104687713086605,\n",
              " 0.0429086796939373,\n",
              " 0.03577300161123276,\n",
              " 0.046368058770895004,\n",
              " -0.002406308427453041,\n",
              " -0.05008396506309509,\n",
              " 0.038116008043289185,\n",
              " -0.03358450531959534,\n",
              " 0.04708188772201538,\n",
              " 0.0254586860537529,\n",
              " -0.0036188913509249687,\n",
              " -0.009438914246857166,\n",
              " -0.010688434354960918,\n",
              " -0.020436512306332588,\n",
              " -0.06310693919658661,\n",
              " 0.07994679361581802,\n",
              " 0.007657818961888552,\n",
              " -0.09704147279262543,\n",
              " -0.14763614535331726,\n",
              " -0.04485037177801132,\n",
              " -0.12413385510444641,\n",
              " 0.02339310571551323,\n",
              " -0.07797997444868088,\n",
              " -0.04275863990187645,\n",
              " -0.02576283924281597,\n",
              " -0.03555937483906746,\n",
              " -0.006710659712553024,\n",
              " -0.0520351342856884,\n",
              " -0.0221012681722641,\n",
              " -0.03977634757757187,\n",
              " -0.07160230726003647,\n",
              " -0.03175315633416176,\n",
              " 0.06060826778411865,\n",
              " -0.13541749119758606,\n",
              " 0.07306104153394699,\n",
              " 0.08623792231082916,\n",
              " 0.04905453696846962,\n",
              " 0.03503919392824173,\n",
              " -0.035703495144844055,\n",
              " -0.057604070752859116,\n",
              " -0.007743408903479576,\n",
              " -0.03548337519168854,\n",
              " 0.056455668061971664,\n",
              " 0.09859588742256165,\n",
              " 0.0308439452201128,\n",
              " 2.6352299755671993e-05,\n",
              " 0.05372202396392822,\n",
              " -0.06744945794343948,\n",
              " -0.13734889030456543,\n",
              " -0.06468485295772552,\n",
              " 0.07325388491153717,\n",
              " 0.06050116568803787,\n",
              " 0.060100339353084564,\n",
              " 0.0023564142175018787,\n",
              " 0.03809036687016487,\n",
              " -0.003733034012839198,\n",
              " -0.02710360288619995,\n",
              " 0.06791599094867706,\n",
              " 0.008514481596648693,\n",
              " 0.0031211997848004103,\n",
              " -0.019543195143342018,\n",
              " -0.02407827600836754,\n",
              " -0.07891330868005753,\n",
              " -0.031033126637339592,\n",
              " -0.07830438017845154,\n",
              " 0.02809705026447773,\n",
              " 0.01190970093011856,\n",
              " 0.01960773393511772,\n",
              " 0.06790619343519211,\n",
              " -0.055964600294828415,\n",
              " -0.06521055102348328,\n",
              " 0.07363668084144592,\n",
              " 0.12109825015068054,\n",
              " 0.3484042286872864,\n",
              " -0.04532778263092041,\n",
              " 0.00520814536139369,\n",
              " -0.025217799469828606,\n",
              " 0.03971348702907562,\n",
              " 0.017502063885331154,\n",
              " -0.06240103393793106,\n",
              " 0.06793767958879471,\n",
              " -0.0018475287361070514,\n",
              " -0.025844283401966095,\n",
              " 0.06408515572547913,\n",
              " -0.03362569212913513,\n",
              " -0.013674512505531311,\n",
              " 0.043912168592214584,\n",
              " -0.018499234691262245,\n",
              " -0.042921677231788635,\n",
              " 0.05248887091875076,\n",
              " 0.06556808948516846,\n",
              " -0.03496139124035835,\n",
              " -0.02246849425137043,\n",
              " -0.021858133375644684,\n",
              " -0.10612165182828903,\n",
              " -0.0722619816660881,\n",
              " -0.05672057345509529,\n",
              " -0.0048436978831887245,\n",
              " 0.06991785019636154,\n",
              " -0.04591067135334015,\n",
              " 0.03566421940922737,\n",
              " -0.02006814070045948,\n",
              " -0.04991843178868294,\n",
              " -0.09930470585823059,\n",
              " 0.0002606220368761569,\n",
              " -0.14647071063518524,\n",
              " -0.09455244243144989,\n",
              " -0.041280027478933334,\n",
              " 0.030058152973651886,\n",
              " 0.020264113321900368,\n",
              " -0.002101233461871743,\n",
              " -0.0524330660700798,\n",
              " 0.09931240975856781,\n",
              " 0.014402606524527073,\n",
              " 0.06367833912372589,\n",
              " 0.017025768756866455,\n",
              " -0.05515701696276665,\n",
              " -0.004276723135262728,\n",
              " 0.03180202841758728,\n",
              " 0.06515178084373474,\n",
              " -0.02481916733086109,\n",
              " 0.001274821232073009,\n",
              " 0.023806344717741013,\n",
              " -0.09496991336345673,\n",
              " 0.10218573361635208,\n",
              " 0.024534249678254128,\n",
              " 0.09562045335769653,\n",
              " -0.00833453144878149,\n",
              " 0.08216188102960587,\n",
              " -0.08752856403589249,\n",
              " -0.06607890129089355,\n",
              " -0.1328653246164322,\n",
              " 0.0032588401809334755,\n",
              " -0.10079021006822586,\n",
              " -0.06395168602466583,\n",
              " -0.033228855580091476,\n",
              " -0.042326778173446655,\n",
              " -0.12193320691585541,\n",
              " 0.14371919631958008,\n",
              " -0.01668519899249077,\n",
              " -0.0009147092932835221,\n",
              " -0.1407356858253479,\n",
              " -0.022613022476434708,\n",
              " 0.05957426875829697,\n",
              " 0.11438234150409698,\n",
              " 0.03256788104772568,\n",
              " -0.045187972486019135,\n",
              " 0.10680631548166275,\n",
              " -0.00967460684478283,\n",
              " -0.18336033821105957,\n",
              " -0.03670293092727661,\n",
              " -0.015177220106124878,\n",
              " 0.08281303942203522,\n",
              " 0.09960739314556122,\n",
              " 0.015701742842793465,\n",
              " -0.010239096358418465,\n",
              " -0.030513491481542587,\n",
              " -0.06233547627925873,\n",
              " -0.028422348201274872,\n",
              " 0.024094492197036743,\n",
              " -0.01736299693584442,\n",
              " -0.08820896595716476,\n",
              " 0.019336499273777008,\n",
              " -0.06510138511657715,\n",
              " 0.061713818460702896,\n",
              " 0.055395521223545074,\n",
              " -0.09456266462802887,\n",
              " 0.012817742303013802,\n",
              " -0.040230270475149155,\n",
              " 0.02783379703760147,\n",
              " 0.05834460258483887,\n",
              " 0.10175477713346481,\n",
              " 0.021299367770552635,\n",
              " -0.00669069355353713,\n",
              " 0.00018406032177153975,\n",
              " 0.03282908722758293,\n",
              " -0.04478637874126434,\n",
              " -0.02815367467701435,\n",
              " 0.03375723212957382,\n",
              " -0.008161804638803005,\n",
              " -0.000429562758654356,\n",
              " 0.04621770605444908,\n",
              " 0.0468440018594265,\n",
              " 0.08220041543245316,\n",
              " -0.07013349235057831,\n",
              " -0.01863967254757881,\n",
              " 0.05617169290781021,\n",
              " 0.08227088302373886,\n",
              " -0.013639802113175392,\n",
              " 0.022877246141433716,\n",
              " 0.06764132529497147,\n",
              " -0.018563097342848778,\n",
              " 0.08007284253835678,\n",
              " 0.019671928137540817,\n",
              " -0.06353460252285004,\n",
              " -0.03402867540717125,\n",
              " -0.03405323997139931,\n",
              " -0.0068474202416837215,\n",
              " 0.13429482281208038,\n",
              " 0.021028345450758934,\n",
              " -0.057957686483860016,\n",
              " -0.0025433595292270184,\n",
              " -0.04178203269839287,\n",
              " 0.00624621519818902,\n",
              " -0.10533209890127182,\n",
              " 0.015662906691432,\n",
              " 0.07872498035430908,\n",
              " -0.03447854518890381,\n",
              " -0.004084276035428047,\n",
              " -0.00823252834379673,\n",
              " -0.0033616898581385612,\n",
              " -0.09880270063877106,\n",
              " -0.033215947449207306,\n",
              " 0.03585217520594597,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'embeddings' is a list or array of 768-dimensional embeddings\n",
        "\n",
        "# Choose the number of clusters, this can be adjusted based on the book's content.\n",
        "# I played around and found ~10 was the best.\n",
        "# Usually if you have 10 passages from a book you can tell what it's about\n",
        "num_clusters = 21\n",
        "\n",
        "# Perform K-means clustering\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors)"
      ],
      "metadata": {
        "id": "gwzwlQKddCqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.labels_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgH9qi4ydCmz",
        "outputId": "13a98789-ca08-4129-ca09-9920c1fdc040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([19,  3,  9,  9,  9,  9,  9,  4,  0, 14,  3,  9,  0, 14,  0,  9, 14,\n",
              "       18, 18,  5,  5, 18, 18, 18,  6,  6, 15, 17,  7, 18,  6,  1,  1, 17,\n",
              "       17, 17,  7,  5, 15,  6,  6,  8,  7,  6,  6,  6,  9, 14,  5, 14, 14,\n",
              "       13, 13, 15, 18,  7,  6, 14,  7,  7,  7, 14, 13, 13, 18, 18, 18,  7,\n",
              "        7,  6,  6,  6,  6,  6,  6,  3,  1,  7, 17,  5,  7, 14, 14, 20, 20,\n",
              "        3, 13, 13, 15,  1, 15,  6,  6,  1, 18, 15,  6,  6, 14, 15,  6, 18,\n",
              "        6,  1, 17, 18, 15, 18, 18, 15,  1, 17, 17,  7, 17, 17,  8,  8, 13,\n",
              "       11, 20,  3, 13,  3,  6,  1,  3,  3, 16, 16, 16, 16, 16,  4, 18, 18,\n",
              "        1,  1,  9,  0,  3,  3,  1,  3, 17,  3,  8,  8,  7,  7,  6,  7,  6,\n",
              "        6,  7,  6,  6,  7,  4, 13,  1, 13,  3,  7, 13,  1,  3,  8,  1,  6,\n",
              "        1,  3,  7,  6,  6,  6, 15, 15, 15,  5,  7,  5, 15, 15,  6, 14,  5,\n",
              "       18,  7,  6,  6, 11, 14, 18, 18,  8,  8,  1,  7,  7,  7,  7,  7,  9,\n",
              "        1, 13,  3,  0,  9,  0, 11,  0,  0,  3,  3,  8, 15,  6,  6,  6,  6,\n",
              "       17,  9,  7,  7, 15, 15,  6,  1,  3,  6,  1,  3,  1,  1,  1, 11, 11,\n",
              "       17,  7,  6,  5,  7,  5,  7,  7,  7,  7,  7,  7,  1,  7,  7,  7,  7,\n",
              "        1,  6, 17,  7,  6,  1, 17,  8,  8, 17, 18, 15,  7,  1,  6,  6,  1,\n",
              "        6,  7,  6,  6,  6,  1,  1,  6,  6,  6,  6, 17,  1,  1,  3,  8, 18,\n",
              "       17,  5,  8,  5,  5,  5,  7,  7, 17,  6, 15,  8, 15, 15,  1,  6,  6,\n",
              "        6,  5, 15, 15,  5,  1,  9, 11,  9,  9,  8,  4,  8,  8, 11,  8,  8,\n",
              "        3, 11, 15,  6,  1,  8,  6,  8,  4, 13, 15, 15,  6,  1,  6, 15,  5,\n",
              "        8, 13,  8, 11,  5, 13, 18, 17,  6,  1,  3,  7,  8,  8,  6, 15,  6,\n",
              "        6,  6, 13, 17,  7,  1,  6,  6,  1,  1,  7,  6,  8,  8,  8,  4, 11,\n",
              "       11, 14, 11,  1,  8,  8,  6,  3,  8, 14, 11,  3, 12, 12, 12, 12, 12,\n",
              "       12, 12, 12, 12, 12, 12, 12, 12, 12,  2,  2, 12, 12, 10,  2, 10, 12,\n",
              "       19,  2, 19,  2, 19, 19], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(kmeans.labels_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGJ0_stQ-aVF",
        "outputId": "54dc73dc-5ecb-445c-d534-58c925bb29ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "414"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# Taking out the warnings\n",
        "import warnings\n",
        "from warnings import simplefilter\n",
        "\n",
        "# Filter out FutureWarnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Perform t-SNE and reduce to 2 dimensions\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_data_tsne = tsne.fit_transform(np.array(vectors))\n",
        "\n",
        "# Plot the reduced data\n",
        "plt.scatter(reduced_data_tsne[:, 0], reduced_data_tsne[:, 1], c=kmeans.labels_)\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('Book Embeddings Clustered')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "OZGydweGeAlv",
        "outputId": "6a9538b9-a344-48f8-bce7-646875c644f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the closest embeddings to the centroids\n",
        "\n",
        "# Create an empty list that will hold your closest points\n",
        "closest_indices = []\n",
        "\n",
        "# Loop through the number of clusters you have\n",
        "for i in range(num_clusters):\n",
        "\n",
        "    # Get the list of distances from that particular cluster center\n",
        "    distances = np.linalg.norm(vectors - kmeans.cluster_centers_[i], axis=1)\n",
        "\n",
        "    # Find the list position of the closest one (using argmin to find the smallest distance)\n",
        "    closest_index = np.argmin(distances)\n",
        "\n",
        "    # Append that position to your closest indices list\n",
        "    closest_indices.append(closest_index)"
      ],
      "metadata": {
        "id": "N6DOKSe1eRaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_indices = sorted(closest_indices)\n",
        "selected_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5KD4VvreZiQ",
        "outputId": "32e192d5-cd86-4279-e50e-b7fcb2b86a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[38,\n",
              " 44,\n",
              " 66,\n",
              " 84,\n",
              " 129,\n",
              " 133,\n",
              " 179,\n",
              " 185,\n",
              " 203,\n",
              " 212,\n",
              " 232,\n",
              " 238,\n",
              " 303,\n",
              " 328,\n",
              " 343,\n",
              " 345,\n",
              " 361,\n",
              " 391,\n",
              " 400,\n",
              " 404,\n",
              " 412]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_prompt = \"\"\"\n",
        "You will be given a single passage of a book. This section will be enclosed in triple backticks (```)\n",
        "Your response should be at least three paragraphs and fully encompass what was said in the passage.\n",
        "\n",
        "```{text}```\n",
        "FULL SUMMARY:\n",
        "\"\"\"\n",
        "map_prompt_template = PromptTemplate(template=map_prompt, input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "V7VRN1gJeZfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_chain = load_summarize_chain(llm=llm,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=map_prompt_template)"
      ],
      "metadata": {
        "id": "OyS0OUyLeZb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_docs = [docs[doc] for doc in selected_indices]"
      ],
      "metadata": {
        "id": "V0DVEitpevuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0bvP4DFIVUr",
        "outputId": "1de93c27-a80d-41e3-aefc-e5e3c0132c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='The horses, needless to say, were not mentioned again. Tom and Miss\\nBaker, with several feet of twilight between them, strolled back into\\nthe library, as if to a vigil beside a perfectly tangible body, while,\\ntrying to look pleasantly interested and a little deaf, I followed\\nDaisy around a chain of connecting verandas to the porch in front. In\\nits deep gloom we sat down side by side on a wicker settee.\\n\\nDaisy took her face in her hands as if feeling its lovely shape, and\\nher eyes moved gradually out into the velvet dusk. I saw that\\nturbulent emotions possessed her, so I asked what I thought would be\\nsome sedative questions about her little girl.\\n\\n“We don’t know each other very well, Nick,” she said suddenly. “Even\\nif we are cousins. You didn’t come to my wedding.”\\n\\n“I wasn’t back from the war.”\\n\\n“That’s true.” She hesitated. “Well, I’ve had a very bad time, Nick,\\nand I’m pretty cynical about everything.”')"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make an empty list to hold your summaries\n",
        "summary_list = []\n",
        "\n",
        "# Loop through a range of the lenght of your selected docs\n",
        "for i, doc in enumerate(selected_docs):\n",
        "\n",
        "    # Go get a summary of the chunk\n",
        "    chunk_summary = map_chain.run([doc])\n",
        "\n",
        "    # Append that summary to your list\n",
        "    summary_list.append(chunk_summary)\n",
        "\n",
        "    print (f\"Summary #{i} (chunk #{selected_indices[i]}) - Preview: {chunk_summary[-250:]} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "mJER-0Y5evrs",
        "outputId": "9ce033dd-3b4f-457b-b40c-8eb3135282ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary #0 (chunk #38) - Preview: a reader will be given a single passage of a book. your goal is to give a summary of this section . your response should be at least three paragraphs and fully encompass what was said in the passage . \n",
            "\n",
            "Summary #1 (chunk #44) - Preview: daisy and tom looked at each other for a moment in silence . “did you give Nick a little heart to heart talk on the veranda?” demanded Tom suddenly . your goal is to give a summary of this section so that a reader will have a full understanding . \n",
            "\n",
            "Summary #2 (chunk #66) - Preview: your goal is to give a summary of this section so that a reader will have a full understanding of what happened . your response should be at least three paragraphs and fully encompass what was said in the passage. \n",
            "\n",
            "Summary #3 (chunk #84) - Preview:  be enclosed in triple backticks () your goal is to give a summary of this section so that a reader will have a full understanding of what happened . your response should be at least three paragraphs and fully encompass what was said in the passage . \n",
            "\n",
            "Summary #4 (chunk #129) - Preview: l be enclosed in triple backticks (). your goal is to give a summary of this section so that a reader will have a full understanding of what happened. your response should be at least three paragraphs and fully encompass what was said in the passage. \n",
            "\n",
            "Summary #5 (chunk #133) - Preview: l be enclosed in triple backticks (). your goal is to give a summary of this section so that a reader will have a full understanding of what happened. your response should be at least three paragraphs and fully encompass what was said in the passage. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-905dab1b681e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Go get a summary of the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mchunk_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Append that summary to your list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    539\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    361\u001b[0m         }\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             outputs = (\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/stuff.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;31m# Call predict on the LLM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     async def acombine_docs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"funny\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    361\u001b[0m         }\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             outputs = (\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    567\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    739\u001b[0m                 )\n\u001b[1;32m    740\u001b[0m             ]\n\u001b[0;32m--> 741\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             output = (\n\u001b[0;32m--> 592\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    593\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/llms/huggingface_pipeline.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# Process batch of prompts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpipeline_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;31m# Process each response in the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \"\"\"\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         if (\n\u001b[1;32m    169\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 )\n\u001b[0;32m-> 1143\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1066\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         )\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;31m# 14. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m             return self.beam_sample(\n\u001b[0m\u001b[1;32m   1596\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3351\u001b[0m             )\n\u001b[1;32m   3352\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3353\u001b[0;31m                 model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n\u001b[0m\u001b[1;32m   3354\u001b[0m                     \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3355\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_temporary_reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   2725\u001b[0m         \u001b[0;31m# Exception 1: code path for models using the legacy cache format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2727\u001b[0;31m             \u001b[0mpast_key_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reorder_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2728\u001b[0m         \u001b[0;31m# Exception 2: models with different cache formats. These are limited to `DynamicCache` until their\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2729\u001b[0m         \u001b[0;31m# cache format is standardized, to avoid adding complexity to the codebase.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36m_reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   1851\u001b[0m                 \u001b[0;31m# need to set correct `past` for each of the four key / value states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m                 reordered_layer_past_states = reordered_layer_past_states + (\n\u001b[0;32m-> 1853\u001b[0;31m                     \u001b[0mlayer_past_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_past_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m                 )\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aV-1Zg39EKwW",
        "outputId": "00200262-5680-4edd-b52b-289318b8b930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"cnn's john sutter recalls his climb of mount everest in 1996 . he says the climb was hampered by low atmospheric pressure . the climbers, he writes, were unable to escape the mountain's treacherous terrain .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(summary_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc74I-PFEOCm",
        "outputId": "764af432-b485-4df0-8773-4338a5d6465c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = \"\\n\".join(summary_list)\n",
        "\n",
        "# Convert it back to a document\n",
        "summaries = Document(page_content=summaries)\n",
        "\n",
        "print (f\"Your total summary has {llm.get_num_tokens(summaries.page_content)} tokens\")"
      ],
      "metadata": {
        "id": "2jZZILejevoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6d0a933-20e4-4da5-a246-ebd36005cb90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your total summary has 1421 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_summarization_pipeline2 = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"summarization\",\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "      max_new_tokens=1000,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "\n",
        ")\n",
        "llm2 = HuggingFacePipeline(pipeline=text_summarization_pipeline2)"
      ],
      "metadata": {
        "id": "RB2eHR0khaRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combine_prompt = \"\"\"\n",
        "You will be given a series of summaries from a book. The summaries will be enclosed in triple backticks (```)\n",
        "Your goal is to give a verbose summary of what happened in the story.\n",
        "The reader should be able to grasp what happened in the book.\n",
        "\n",
        "```{text}```\n",
        "VERBOSE SUMMARY:\n",
        "\"\"\"\n",
        "combine_prompt_template = PromptTemplate(template=combine_prompt, input_variables=[\"text\"])"
      ],
      "metadata": {
        "id": "xGymo8I-evlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_chain = load_summarize_chain(llm=llm2,\n",
        "                             chain_type=\"stuff\",\n",
        "                             prompt=combine_prompt_template,\n",
        "                             verbose=True # Set this to true if you want to see the inner workings\n",
        "                                   )"
      ],
      "metadata": {
        "id": "JGygLaEphyB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = reduce_chain.run([summaries])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbc9kBYDhx95",
        "outputId": "814928db-ae5b-492e-afc6-e081f54cbcd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "You will be given a series of summaries from a book. The summaries will be enclosed in triple backticks (```)\n",
            "Your goal is to give a verbose summary of what happened in the story.\n",
            "The reader should be able to grasp what happened in the book.\n",
            "\n",
            "```cnn's john sutter recalls his climb of mount everest in 1996 . he says the climb was hampered by low atmospheric pressure . the climbers, he writes, were unable to escape the mountain's treacherous terrain .\n",
            "james moore: climbing has become the focus of his life, but he abandoned his dream of climbing everest . he says a decade ago, a pair of u.s. climbers reached the summit of the world's tallest mountain . now, he writes, climbing is a way of life for many, but it's also a money-making venture . author: if you want to climb everest, you have to take time away from your job .\n",
            "cnn's john defterios meets the man who guided him to the top of the world's tallest mountain . he recalls a time when climbing was the most important thing in his life . after a public backlash over his role in the commercialization of the mountains, he started a guiding company .\n",
            "sherpas are a mountain people, devoutly Buddhist, whose forebears migrated south from Tibet . the 1921 expedition hired a large corps of Sherpas as load bearers and camp helpers . despite the hazards, most sought-after jobs are the half dozen openings for climbing Sherpa .\n",
            "cnn's john sutter writes about his experience climbing everest . he recalls meeting the rimpoche at a clinic in Pheriche, near the summit . the clinic offers free medical care to climbers who have suffered altitude-related illnesses .\n",
            "a young sherpa fell 150 feet into a crevasse while scouting a route up the Lhotse face . john sutter: \"without the support of our sherpas none of us has any chance of climbing\" he says the rescuers were untrained and didn't know what they were doing .\n",
            "cnn's john sutter talks with a climber about his experience at everest base camp . he recalls how a commercial expedition helped clean up the mountain . in 1996, he and a friend led a group of climbers to the top of the world's highest peak . the two men stayed at the base camp compound for the next six weeks .\n",
            "bob greene asks readers to summarize a passage in a book about a climber's life . he says he was a gregarious, magnetic climber with an almost childlike enthusiasm . his climbing career was soaring, but he struggled to find a commercial sponsorship . greene: he had a vulnerable side that most people didn't see .\n",
            "dean obeidallah: woodall wanted to include a woman on the south african everest expedition . he says he lied about who he listed on the permit and who was allowed to join the team . woodall refused to relinquish leadership of the expedition or make any compromises, he writes . after the expedition fell apart, woodall apologized for his actions, and he was sacked .\n",
            "bob greene: i'm given a passage of a book; my goal is to give a full understanding of it . he says she was known as a grandstanding dilettante, a social climber, not a climber . but she says she remained oblivious to the resentment and scorn she inspired in others . greene says she's learned to live by the imposition of narrative line upon disparate images .\n",
            "bob greene: sherpas on a 1996 expedition blamed a climber for his uncle's illness . he says they believed the climber had angered the goddess of the sky, Sagarmatha . greene says the sex was ok, but he said it was bad luck for the rest of the team . Greene: he was a cocky, athletic young man with a great sense of adventure .\n",
            "a radial keratotomy is a surgical procedure to flatten the cornea . it is used to treat glaucoma and other eye conditions . the procedure can be used to save the life of a climber .\n",
            "cnn's john sutter asks readers to give a summary of a passage in a book . he recalls the day that a climber collapsed on the summit of mt. everest . the climber had been suffering from a gastrointestinal parasite at the time, he says . after the climbers reached the summit, they were told to return to base camp .\n",
            "cnn's john sutter talks with a climber who lost his group in a blizzard . the climbers had strayed too far from the ridge to reach the tents at camp four . they were dragged down the slope by a group of climbers who had run out of oxygen . despite their best efforts, the group was unable to find their way back to camp .\n",
            "bob greene: a group of 19 climbers went missing from the summit of mt. everest . he says the guide, Anatoli Boukreev, raced down the mountain ahead of his clients . greene says he was unable to find his clients, and his oxygen ran out in the storm . Greene: the rescuers were able to find the climbers, but it was too late .\n",
            "bob greene says he was given a passage of a book and asked to give a summary . he says the author's version of events was similar to that of the cipher on the south col . greene: it's hard to believe that a climber could have been killed on the ice .\n",
            "peter bergen: climbers from ladakh believed they were on top of everest in 1996 . he says they didn't; they were trapped by a blizzard 500 feet below the summit . they rescued three climbers, one of whom had died, but it was too late for them, he writes . author: it was a mistake to think that the climbers had seen the third climber .\n",
            "a record number of people died in the spring climbing season on everest in 1996 . john sutter: critics have been quick to suggest policies to prevent future tragedies . but he says it's hard to believe a tragedy of this magnitude was overdue . the only way to reduce future death rates is to ban bottled oxygen .\n",
            "aaron carroll: critics of Into thin air have questioned author's credibility . he says he's rebutted Into Thin air's numerous errors; he regrets that . but he writes that he was trying to tell the story as accurately and honestly as possible . carroll says it's time for a public debate about the accuracy of books .\n",
            "dean obeidallah: many of us who were on everest that may have made mistakes . he says a journalist's presence on the expedition may have contributed to disaster . dean: i stand by my conviction that he descended because he was cold and weary . many experienced high-altitude climbers disagree with dean's conclusion, he writes .\n",
            "bob greene: the debate over what happened on everest in 1996 took on a different light . he says the two men were stubborn and proud and loathe to back down from a fight . greene says they disagreed on many points, but agreed on almost everything else .```\n",
            "VERBOSE SUMMARY:\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdEWkrN9hx6l",
        "outputId": "160367d3-b798-4daa-87d3-692b20b96271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bob greene says he was given a passage from a book and asked to give a verbose summary of what happened in the story . john sutter recalls his climb of everest in 1996 . dean obeidallah says a journalist's presence on the expedition may have contributed to disaster .\n"
          ]
        }
      ]
    }
  ]
}