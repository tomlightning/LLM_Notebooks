{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "612a3d4d-c3c3-4b8f-8697-87f75f19223b",
   "metadata": {},
   "source": [
    "# GTP2\n",
    "\n",
    "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n",
    "\n",
    "- https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "- https://github.com/openai/gpt-2?tab=readme-ov-file\n",
    "\n",
    "- https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "  \n",
    "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset\n",
    "of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples.\n",
    "The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting\n",
    "but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\n",
    "\n",
    "\n",
    "![image.png](nano.png)\n",
    "\n",
    "https://paperswithcode.com/dataset/webtext\n",
    "\n",
    "```\n",
    "@article{radford2019language,\n",
    "  title={Language Models are Unsupervised Multitask Learners},\n",
    "  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n",
    "  year={2019}\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "# nanoGPT --> credits to Andrej karpathy, who created this excellent Repository\n",
    "\n",
    "https://github.com/karpathy/nanoGPT\n",
    "\n",
    "- https://github.com/triton-lang/triton/\n",
    "- https://triton-lang.org/main/getting-started/installation.html\n",
    "# nanoGPT - LoRA\n",
    "https://github.com/danielgrittner/nanoGPT-LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525269e6-4bc4-46a8-a685-dddff2f6216e",
   "metadata": {},
   "source": [
    "![image.png](image2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c14e7-a6be-4b72-937d-6b34e467415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch numpy transformers datasets tiktoken wandb tqdm numoy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "781c0755-bf41-4388-9e06-507e16ef1970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olonok/.local/lib/python3.11/site-packages/pydot/core.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dcf2a3063243e5b36af6cf49f4de4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75d1518406b4c2ea4e9c6d54a02ed78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3be974d4a141879e0bd4187c41fc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ab6b3c0e82477a80214284e01c76ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e241887f63114de1bbaa42a2648795d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1d12a5eb39465b99b281faa16890b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7ff430e79d402e9bead24dc65d6523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olonok/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what I'm really doing is making a human-readable document. There are other languages, but those are\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a syntax model. That's why I like it. I've done a lot of programming projects.\\n\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll do it in no time!\\n\\nOne of the things we learned from talking to my friend\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a command line tool.\\n\\nIf my code is simple enough:\\n\\nif (use (string\"},\n",
       " {'generated_text': \"Hello, I'm a language model, I've been using Language in all my work. Just a small example, let's see a simplified example.\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9eb837-1eb6-4d44-8ba9-f22b032d4f06",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0120b0d1-0a8e-48d2-8296-c5877d67a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6657cca-dc97-4f0a-836c-1d87ed3ccdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/d/repos2/nanoGPT'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75ae3de8-d135-4198-a44a-24f43ee5173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 290,101\n"
     ]
    }
   ],
   "source": [
    "input_file_path = \"data/shakespeare_char/book.txt\"\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74cf3dcf-ab79-45a5-b117-122c2250adae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the unique characters: \n",
      " !#$%()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyzçéêô —‘’“”•…™\n",
      "vocab size: 93\n"
     ]
    }
   ],
   "source": [
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa48ab82-05fd-4eaa-b075-48c6bc5c5f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 261,090 tokens\n",
      "val has 29,011 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1490082c-8e72-46a9-85aa-de6379caae0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 71, 68, 63, 58, 56, 73, 1, 32, 74, 73, 58, 67, 55, 58, 71, 60, 1, 58, 27]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3b201e2-ab64-4e89-9e6d-26b6e3eaa756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project Gutenberg eB'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7025f13a-c228-4525-a31b-5a309c2eb282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi[\"P\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "255b5368-bd3d-43b0-8e29-c92d3f91fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(ROOT_DIR, \"data\" , \"shakespeare_char\",'train.bin'))\n",
    "val_ids.tofile(os.path.join(ROOT_DIR, \"data\" , \"shakespeare_char\", 'val.bin'))\n",
    "\n",
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join(ROOT_DIR, \"data\" , \"shakespeare_char\", 'meta.pkl'), 'wb') as f:\n",
    "    pickle.dump(meta, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb284c-a1db-40f4-830c-2ca7cb419390",
   "metadata": {},
   "source": [
    "# Train_model in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41978e70-4c8e-4a63-8ac6-a88921d3599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python train.py config/train_shakespeare_char.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21118361-12f2-4520-a46e-227437e3f213",
   "metadata": {},
   "source": [
    "### If you dont have a GPU\n",
    "```\n",
    "python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95e6af-f13a-440e-921a-a9dcb3e2f1cb",
   "metadata": {},
   "source": [
    "# Generate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b41564e6-4c61-4f77-b7ef-489af91c96e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/olonok/.local/lib/python3.11/site-packages/pydot/core.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'\n",
      "  warnings.warn(\n",
      "Overriding: out_dir = out-shakespeare-char\n",
      "/mnt/d/repos2/nanoGPT/sample.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "number of parameters: 10.66M\n",
      "Loading meta from data/shakespeare_char/meta.pkl...\n",
      "\n",
      "nover to you among now me. I wasn’t think you—were could be there?”\n",
      "\n",
      "“No, you hear in truck about the house of the balls at the removies the garage mile heatte\n",
      "driver.”\n",
      "\n",
      "“What’s sound.”\n",
      "\n",
      "“That’s business?”\n",
      "\n",
      "“Jordan Baker name interestion and as ‘There’s then Baker’s\n",
      "somebody is growns.”\n",
      "\n",
      "“I’ll let here a moment that yourself many thing of the dead.”\n",
      "\n",
      "“What’s you haven’t a cigar.” He said agained much over the\n",
      "table. Finny-she didn’t knew that had never made me all affair the\n",
      "dict of the corner.”\n",
      "---------------\n",
      "\n",
      "perceived with just still beside out its with toward about the\n",
      "room.\n",
      "\n",
      "“What’s a weathory?”\n",
      "\n",
      "“Bring?”\n",
      "\n",
      "Her voice foot. Well, what he say had pointed so now of the presence\n",
      "in the coance. They’ll we’ve to get out on the enormous\n",
      "Carray.”\n",
      "\n",
      "“You see,” said the went of the next door. “I went into then the\n",
      "“man.”\n",
      "\n",
      "“All right there’s a whole is is your own.”\n",
      "\n",
      "“Don’t keep you to twink to each?”\n",
      "\n",
      "“How’s they’re enormous.”\n",
      "\n",
      "“They’re asleep around they’re book that?” objected Mr.\n",
      "McKee slowly. “In the taxi\n",
      "---------------\n",
      "\n",
      "he came to find me and smoke then inside the great, and the understand her\n",
      "told me like at his name the cond, stall wife it was an in his fawn\n",
      "bedroom. The lawn was and so longer that the girl of\n",
      "harshwere and really afternoon until we’plet’s a greys and that until of then\n",
      "tremark.”\n",
      "\n",
      "“You about the gradual time is that?”\n",
      "\n",
      "“Go back.”\n",
      "\n",
      "“Are we’ve going.”\n",
      "\n",
      "We was complaying before I told memore to there went and the sider of his\n",
      "hand.”\n",
      "\n",
      "He had et over with the porceital toducally intensity of the\n",
      "d\n",
      "---------------\n",
      "\n",
      "\n",
      "“What do you?” he inquired more groaningly leaning leanering the\n",
      "resternal full of she was the projice was going light. It was to a the small six duck\n",
      "from the puople of my one of the small the direct.\n",
      "\n",
      "Then she filled her closed out the melace the crowd car, gan to the\n",
      "foot of the dust his house while had gone to the pather sound and house of from her cried to\n",
      "know. He had say after he had knew he had the distaking for the phone man. Have all\n",
      "then it back, and I was now already of us a chance \n",
      "---------------\n",
      "\n",
      "“In there’s house and mised about here,” answered Daisy. “She’s a lot of ghost of his wife into\n",
      "the house.”\n",
      "\n",
      "“No, and this after a moment.”\n",
      "\n",
      "“We’re advant to tell these around,” said Gatsby.\n",
      "\n",
      "“I was a round man is in the girl?”\n",
      "\n",
      "“No,” she explained.\n",
      "\n",
      "“Come of here are after here he talked and at the sun.”\n",
      "\n",
      "“I thought you are any change to you liver way. Wouldn’t know, old sport of the\n",
      "back.”\n",
      "\n",
      "They had nothing the door included a personal darkness watching\n",
      "passing down and shadow her before he ba\n",
      "---------------\n",
      "\n",
      "\n",
      "Sit of a momenly as the shrrts and he suggested the terrust what he’d been\n",
      "good to what he sat down. His swoped at Gatsby, nodding as if he said ever me\n",
      "we’ve got to get us taxies in him into the lawn, glanced eagerly into a\n",
      "darkness of a whith her stupted break the army white first or\n",
      "the people car. A deward the Tall naturater of a man sugashed the circle colaps table\n",
      "through, oj have shadow been in the dress\n",
      "things of the city in the man.\n",
      "\n",
      "“Most of course,” exclaimed Daisy, with such lifered\n",
      "---------------\n",
      "\n",
      "“Why don’t know?” She left him she ded. “I’m be run of one of the piecur filler windows\n",
      "of an it teathrough plushed and can’t eat down out the car.”\n",
      "\n",
      "“Hello!” He wanded, “I know you’ve get to areat you?”\n",
      "\n",
      "Somebody, Tom looked at Mr. Wolfshiem shaking here was as a partment, don’t\n",
      "she car had asleep and I was been my wanted into humi. I\n",
      "was just away, and Miss Baker, then had been unusually the silench “or Miss\n",
      "Baker, Risy and and pologe made Tom a lot little Ciay God\n",
      "on the pamp of and Mr. Gatsb\n",
      "---------------\n",
      "\n",
      "“Well, I’m care to know the cry. All right there we’d be true.”\n",
      "\n",
      "“I’ve got to crack to another.”\n",
      "\n",
      "“What your party tontented on have to get so my head.”\n",
      "\n",
      "“Why don’t know about on the most is when Island,” he said.\n",
      "\n",
      "“Why don’t like a little college time to stank the charm?”\n",
      "\n",
      "“Rotogethering across “and you like to yourself and your\n",
      "old there wandered to come to hot?”\n",
      "\n",
      "“Can’t are him?”\n",
      "\n",
      "“No,” she insisted. “Somebody had a little dress road.”\n",
      "\n",
      "“We’re all go to an about matter’s pression.”\n",
      "\n",
      "“I’m abou\n",
      "---------------\n",
      "\n",
      "\n",
      "He came up about the wind sidewalking that her half just of the\n",
      "song grey half\n",
      "across were next driving the first dever. And it was she roted him into the\n",
      "lawn room.\n",
      "\n",
      "“Why don’t right you real two copy?”\n",
      "\n",
      "“He told took him,” he said complained his face.\n",
      "\n",
      "“What do you’ll car would into things you are.”\n",
      "\n",
      "“I haven’t go to around my comfuse to drugstores.”\n",
      "\n",
      "“You didn’t know you’re socious after notice car to lunch on the commution.”\n",
      "\n",
      "“Come on,” he assured.\n",
      "“I couldn’t came to Daisy’s house to get h\n",
      "---------------\n",
      "\n",
      "“Gatsby there right you know.”\n",
      "\n",
      "“Not that?”\n",
      "\n",
      "Daisy’s voice before there,” said Tom. “Here’s good not and people\n",
      "wife and me interesting more so looking at each it for a moment\n",
      "people who much asked at leaned he saw her of the depression. Then\n",
      "had can already to the wall\n",
      "told have in my just after might and then I told get to keep my pair in my\n",
      "large.\n",
      "\n",
      "“I’ll give you a window,” said Daisy, “I’ll never got wait do you\n",
      "hope of theor Gatsby’s somebody day’s from a front and good out with the\n",
      "drums.”\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "! python sample.py --out_dir=out-shakespeare-char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f8a19-3a00-4c06-9fd2-5ccb42938371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU\n",
    "! python sample.py --out_dir=out-shakespeare-char --device=cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265fcd0-409f-4896-a56a-311a4b2e3a01",
   "metadata": {},
   "source": [
    "# Fine Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc49e1-2a63-4b00-8978-b5a1d1c5cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python train.py config/finetune_shakespeare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ebddd-7bb3-4b26-a817-a9960249e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference from gpt2-xl model\n",
    "! python sample.py --init_from=gpt2-xl  --start=\"What is the answer to life, the universe, and everything?\"  --num_samples=5 --max_new_tokens=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295a3a6-c167-49f7-9907-f8ac2d5e9354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
