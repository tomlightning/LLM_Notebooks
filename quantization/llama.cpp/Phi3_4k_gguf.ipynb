{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1IUc9BwQI4phsaD9ChIainiYtjLXyz7zv",
      "authorship_tag": "ABX9TyNW3VHVwuBlyI6xyuypVMxT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/quantization/llama.cpp/Phi3_4k_gguf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phi3\n",
        "https://github.com/microsoft/Phi-3CookBook\n",
        "\n",
        "https://huggingface.co/professorf/phi-3-mini-128k-f16-gguf\n",
        "\n",
        "\n",
        "# Llama.cpp\n",
        "enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud\n",
        "\n",
        "https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "# gguf\n",
        "GGUF is a file format for storing models for inference with GGML and executors based on GGML. GGUF is a binary format that is designed for fast loading and saving of models, and for ease of reading. Models are traditionally developed using PyTorch or another framework, and then converted to GGUF for use in GGML.\n",
        "\n",
        "https://huggingface.co/docs/hub/gguf\n",
        "\n",
        "## Install\n",
        "pip3 install huggingface-hub hf_transfer\n",
        "\n",
        "huggingface-cli download professorf/phi-3-mini-128k-f16-gguf phi-3-mini-128k-f16.gguf --local-dir /content/drive/MyDrive/models/phi3 --local-dir-use-symlinks False\n",
        "\n",
        "\n",
        "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --quiet"
      ],
      "metadata": {
        "id": "LKCjFAOi6kqV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYc50Wqjr1uN",
        "outputId": "7c43a295-15f9-42b5-a45a-0c89820845c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip3 install huggingface-hub hf_transfer --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! huggingface-cli download professorf/phi-3-mini-128k-f16-gguf phi-3-mini-128k-f16.gguf --local-dir /content/drive/MyDrive/models/gguf/phi3 --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKyPyZtAsQEY",
        "outputId": "9d63c5ca-387c-4015-93cf-f6551ebd73b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "/content/drive/MyDrive/models/gguf/phi3/phi-3-mini-128k-f16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsenULj6tbxY",
        "outputId": "0cd6f1bf-fd53-4460-fa2d-442fd1ca3f7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "#del llm\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "IzSLKMTBb5tG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(\n",
        "  model_path=\"/content/drive/MyDrive/models/home_made/phi-3-small-4k-instruct.gguf\",  # Download the model file first\n",
        "  n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
        "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
        "  n_gpu_layers=100         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrinoLSuu0Pj",
        "outputId": "e18b8843-5d35-442d-8ee2-7c473e4a4358"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 26 key-value pairs and 195 tensors from /content/drive/MyDrive/models/home_made/phi-3-small-4k-instruct.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:  phi3.rope.scaling.original_context_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   5:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   6:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   7:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  11:                        phi3.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q8_0:  130 tensors\n",
            "llm_load_vocab: special tokens cache size = 323\n",
            "llm_load_vocab: token to piece cache size = 0.1687 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = phi3\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 3.78 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = Phi3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA L4, compute capability 8.9, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    99.81 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  3772.57 MiB\n",
            "....................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   768.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   168.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    10.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1286\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.pre': 'default', 'general.name': 'Phi3', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'phi3.embedding_length': '3072', 'phi3.rope.freq_base': '10000.000000', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.rope.scaling.original_context_length': '4096', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.file_type': '7'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Crossrail link 'to get go-ahead' The £10bn Crossrail transport plan, backed by business groups, is to get the go-ahead this month, according to The Mail on Sunday.\n",
        "It says the UK Treasury has allocated £7.5bn ($13.99bn) for the project and that talks with business groups on raising the rest will begin shortly.\n",
        "The much delayed Crossrail Link Bill would provide for a fast cross-London rail link. The paper says it will go before the House of Commons on 23 February.\n",
        "A second reading could follow on 16 or 17 March. We've always said we are going to introduce a hybrid Bill for Crossrail in the Spring and this remains the case,\n",
        "the Department for Transport said on Sunday. Jeremy de Souza, a spokesman for Crossrail, said on Sunday he could not confirm whether the Treasury was planning to invest £7.5bn\n",
        "or when the bill would go before Parliament. However, he said some impetus may have been provided by the proximity of an election.\n",
        "The new line would go out as far as Maidenhead, Berkshire, to the west of London, and link Heathrow to Canary Wharf via the City.\n",
        "Heathrow to the City would take 40 minutes, dramatically cutting journey times for business travellers, and reducing overcrowding on the tube.\n",
        "The line has the support of the Mayor of London, Ken Livingstone, business groups and the government, but there have been three years of arguments over how it should be funded.\n",
        "The Mail on Sunday's Financial Mail said the £7.5bn of Treasury money was earmarked for spending in £2.5bn instalments in 2010, 2011 and 2012.\"\"\"\n",
        "\n",
        "text2 = \"\"\"           Celeste Barrios-Cruz\n",
        "(312) 208-6505 | Celestebarrios35@gmail.com | LinkedIn | GitHub | Chicago, IL\n",
        "\n",
        "PROFESSIONAL SUMMARY\n",
        "●\n",
        "Innovative thinker with extensive knowledge of SQL, experience utilizing Python, object oriented\n",
        "programming: C++, front end knowledge of JavaScript\n",
        "●\n",
        "Excellent communication skills (English and Spanish) including teamwork and collaboration\n",
        "●\n",
        "Outstanding organization ability including problem-solving, and time management skills.\n",
        "\n",
        "SKILLS\n",
        "Programming Language: Python, C++, JavaScript, HTML5, CSS3, TypeScript\n",
        "Web Technologies/Development Frameworks: NumPy, Pandas, MATLAB, Flask, jQuery, AJAX, JASON,\n",
        "BootstrapUI, Angular7.0\n",
        "Database: SQL, PostgreSQL, SQLite\n",
        "Software: Microsoft Office (Word, Excel, PowerPoint), Google Developer Tools\n",
        "Tools/Methodologies: Data Structures, Algorithms, GitHub, GIT, Heroku, Scrum, Agile Methodology, Agile\n",
        "Software Development, Project Management, Anaconda, Jupyter Notebook, Visual Studio, Software Development,\n",
        "Data Modeler, Tableau\n",
        "Languages: Spanish (fluent conversational skills)\n",
        "\n",
        "EDUCATION\n",
        "University of Illinois at Chicago (UIC), Chicago, IL\n",
        "Bachelor of Science in Math & Computer Science                                               December 2019\n",
        "\n",
        "PROFESSIONAL EXPERIENCE\n",
        "Empower Saturday School, Chicago IL\n",
        "\n",
        "\n",
        "Co-Director of Technology\n",
        "\n",
        "          November 2020-Present\n",
        "●\n",
        "Volunteer in a Non-Profit foundation to provide tutoring for underprivileged youth in Chicago implementing\n",
        "WordPress for a student portal and website with upcoming donation features built-in.\n",
        "\n",
        "Coding Temple, Chicago IL\n",
        "Software Engineer                                                                                               May 2020-July 2020\n",
        "●\n",
        "Participated in intensive professional development experience in code production.\n",
        "●\n",
        "Collaborated with a team to utilize Flask to revamp a law firm’s website from a previous HTML/CSS draft\n",
        "and then deployed the new website on Heroku from GitHub.\n",
        "●\n",
        "Created an Entity Relationship Diagram (ERD) using lucidchart.com to create a database; also used SQL\n",
        "to export and import data between different data sources.\n",
        "●\n",
        "Utilized Object-Oriented Programming (OOP) concepts with Python to create a parking garage system.\n",
        "●\n",
        "Oversaw Full Web UI Development on 5+ projects using Angular 4 and above, AngularJS, JavaScript,\n",
        "HTML, CSS, third party Angular frameworks, JQuery and JSON.\n",
        "●\n",
        "Used 5+ Python libraries and SQL queries/subqueries to create several datasets which produced statistics,\n",
        "tables, figures, charts and graphs.\n",
        "●\n",
        "Completed case study problem sets using Python, NumPy, SciPy, Pandas packages in order to enhance\n",
        "understanding of the functionality of each program and how to get concrete results.\n",
        "\n",
        "PROJECTS\n",
        "Avengers Phone Book\n",
        "\n",
        "\n",
        "●\n",
        "Created phone numbers for Avengers Phone Book using Flask and displayed them to your front page.\n",
        "●\n",
        "Designed a project so that characters could create,read, update their phone number from the phone book;\n",
        "the project is hosted on Heroku.\n",
        "Good Send\n",
        "\n",
        "\n",
        "\n",
        "●\n",
        "Collaborated with 2+ developers to design both administrator and client web portals using Python, Flask,\n",
        "SQLite and multiple APIs; Utilized Github for version control and deployed the final product on Heroku.\n",
        "●\n",
        "Individually designed and deployed a SQLite database to allow organization to run a more secure,\n",
        "organized, automated and efficient operation resulting in higher client satisfaction.\n",
        "●\n",
        "Incorporated Flask-Admin and Flask-Login to allow the administrator to view, create, update and delete.\n",
        "MyMoviePoster\n",
        "\n",
        "●\n",
        "Linked Spotify playlist to movie poster using an API.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "classes = [\"cv\", \"non-cv\"]\n"
      ],
      "metadata": {
        "id": "z_642CS7vJQN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 =  f\"\"\"you are an expert document classifier\n",
        "Classify the following text using these {len(classes)} classes: {classes}\n",
        "Only use the labels provided: {classes}\n",
        "\n",
        "Confidence score: float of 0-1.\n",
        "for example:\n",
        "0 means you are completely sure that the document does not belongs to class x\n",
        "1 means you are completely sure that the document belong to class x\n",
        "output: only respond with a json with these 2 attrubutes 'label' = class predicted, 'score': float\n",
        "\n",
        "#begin text\n",
        "{text2}.\n",
        "#end text\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2QTqBZn_vKnO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_template = '<|user|>\\n{input} <|end|>\\n<|assistant|>'\n",
        "\n",
        "prompt = f'{chat_template.format(input=prompt_1)}'"
      ],
      "metadata": {
        "id": "1pnozF1cw5E-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import datetime"
      ],
      "metadata": {
        "id": "AqmP3di0w-B9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time1=  datetime.datetime.now()\n",
        "# Simple inference example\n",
        "output = llm(\n",
        "  f\"{prompt}\", # Prompt\n",
        "  temperature = 0.7, # Controls randomness in output\n",
        "  max_tokens=2048,  # Generate up to 512 tokens\n",
        "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
        "  echo=False        # Whether to echo the prompt\n",
        ")\n",
        "time2=  datetime.datetime.now()\n",
        "print(time2-time1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSx0ZK9tvVTO",
        "outputId": "f7f35d3b-2034-4774-daa0-2016b61f47e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   10824.32 ms\n",
            "llama_print_timings:      sample time =      18.25 ms /    33 runs   (    0.55 ms per token,  1808.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     410.03 ms /   976 tokens (    0.42 ms per token,  2380.31 tokens per second)\n",
            "llama_print_timings:        eval time =     728.03 ms /    32 runs   (   22.75 ms per token,    43.95 tokens per second)\n",
            "llama_print_timings:       total time =    1173.79 ms /  1008 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:00:01.188989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output['choices'][0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db-ulJnpvcOO",
        "outputId": "9e507dab-2bba-4e79-fc95-b59ce285832c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['text', 'index', 'logprobs', 'finish_reason'])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output['choices'][0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JGEXWcwXvwuN",
        "outputId": "8b475cdf-1a1b-4e05-995f-351a257accbd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' ```json\\n\\n{\\n\\n    \"label\": \"cv\",\\n\\n    \"score\": 0.985\\n\\n}\\n\\n```'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 =  f\"\"\"you are an expert document classifier\n",
        "Classify the following text using these {len(classes)} classes: {classes}\n",
        "Only use the labels provided: {classes}\n",
        "\n",
        "Confidence score: float of 0-1.\n",
        "for example:\n",
        "0 means you are completely sure that the document does not belongs to class x\n",
        "1 means you are completely sure that the document belong to class x\n",
        "output: only respond with a json with these 2 attrubutes 'label' = class predicted, 'score': float\n",
        "\n",
        "#begin text\n",
        "{text}.\n",
        "#end text\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sEE1dutwxlws"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_template = '<|user|>\\n{input} <|end|>\\n<|assistant|>'\n",
        "\n",
        "prompt = f'{chat_template.format(input=prompt2)}'"
      ],
      "metadata": {
        "id": "nZnnjdPBxoxG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time1=  datetime.datetime.now()\n",
        "# Simple inference example\n",
        "output = llm(\n",
        "  f\"{prompt}\", # Prompt\n",
        "  temperature = 0.7, # Controls randomness in output\n",
        "  max_tokens=2048,  # Generate up to 512 tokens\n",
        "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
        "  echo=False        # Whether to echo the prompt\n",
        ")\n",
        "time2=  datetime.datetime.now()\n",
        "print(time2-time1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwVVxkwzxrOU",
        "outputId": "1024629d-bf3f-4a29-9f13-92ea1fc707b2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =   10824.32 ms\n",
            "llama_print_timings:      sample time =      70.03 ms /   125 runs   (    0.56 ms per token,  1785.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     176.88 ms /   421 tokens (    0.42 ms per token,  2380.19 tokens per second)\n",
            "llama_print_timings:        eval time =    2492.42 ms /   124 runs   (   20.10 ms per token,    49.75 tokens per second)\n",
            "llama_print_timings:       total time =    2799.54 ms /   545 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:00:02.812223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output['choices'][0]['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "6ykm5OSdxwx8",
        "outputId": "eb447bb4-6cec-49c9-d0b5-e29d9d7aa4b9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' {\\n \"label\": \"non-cv\",\\n \"score\": 0.98\\n}\\n\\nNote: The provided text is about the Crossrail project, a transportation initiative in London. It does not directly pertain to an academic CV or professional credentials but rather discusses economic and infrastructure aspects. However, given that it\\'s a news article related to finance and urban development which might be of interest to professionals in those fields, the confidence score is high (almost certain) yet lower than 1 as we cannot definitively classify this text under \\'cv\\'.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "model_id = \"olonok/phi-3-small-4k-instruct-q8_0-gguf\"\n",
        "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"/content/drive/MyDrive/models/home_made/phi-3-small-4k-instruct.gguf\",\n",
        "    path_in_repo=\"phi-3-small-4k-instruct-q8_0-gguf\",\n",
        "    repo_id=model_id,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "WzKNVGoKg7oE",
        "outputId": "1d6218f1-092f-4f5f-ac5b-f3d7737f11ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/olonok/phi-3-small-4k-instruct-q8_0-gguf/commit/8fa9a23d717658c9a73ca94ec3a93675e9730d5c', commit_message='Upload phi-3-small-4k-instruct-q8_0-gguf with huggingface_hub', commit_description='', oid='8fa9a23d717658c9a73ca94ec3a93675e9730d5c', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}