{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "305bbc33-d7e7-4243-a11e-493663414be2",
   "metadata": {},
   "source": [
    "# OpenVino\n",
    "OpenVINO is an open-source toolkit for optimizing and deploying deep learning models from cloud to edge. It accelerates deep learning inference across various use cases, such as generative AI, video, audio, and language with models from popular frameworks like PyTorch, TensorFlow, ONNX, and more. Convert and optimize models, and deploy across a mix of Intel® hardware and environments, on-premises and on-device, in the browser or in the cloud\n",
    "\n",
    "https://github.com/openvinotoolkit/openvino_notebooks/wiki?source=post_page-----45f066b3059a--------------------------------\n",
    "\n",
    "# Neural Network Compression Framework (NNCF)\n",
    "Neural Network Compression Framework (NNCF) provides a suite of post-training and training-time algorithms for optimizing inference of neural networks in OpenVINO™ with a minimal accuracy drop.\n",
    "NNCF is designed to work with models from PyTorch, TensorFlow, ONNX and OpenVINO™.\n",
    "\n",
    "https://github.com/openvinotoolkit/nncf\n",
    "\n",
    "# LLama 3.1-8B Instruct\n",
    "\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n",
    "\n",
    "# Optimum\n",
    "\n",
    "Optimum is an extension of Transformers that provides a set of performance optimization tools to train and run models on targeted hardware with maximum efficiency.\n",
    "\n",
    "https://huggingface.co/docs/optimum/index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c67a9-91b9-4f71-9e9b-b1d88540cd37",
   "metadata": {},
   "source": [
    "![title](Llama3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76c535-8846-4c52-b3c6-7cc5d65c8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# python -m pip install --upgrade pip\n",
    "# pip install -U --pre openvino-genai openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly \n",
    "# pip install --extra-index-url https://download.pytorch.org/whl/cpu \"git+https://github.com/huggingface/optimum-intel.git\" \"git+https://github.com/openvinotoolkit/nncf.git\" \"onnx<=1.16.1\" \n",
    "\n",
    "\n",
    " # accelerate-0.34.2 safetensors-0.4.5  transformers-4.43.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba365c9-06d7-4204-b817-261c6fc9555e",
   "metadata": {},
   "source": [
    "# Tranform the model to Format IR\n",
    "```\n",
    "optimum-cli export openvino --model meta-llama/Meta-Llama-3.1-8B-Instruct --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 1.0 --sym llama-3.1-8b-instruct/INT4_compressed_weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac70253-31bf-49d2-8fe4-e670b4faed9f",
   "metadata": {},
   "source": [
    "# Replace the variable chat_template with tokenizer_config.json\n",
    "\n",
    "\"chat_template\": \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96135fd4-0c92-452a-bb6a-e439cd97da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino_genai \n",
    "def streamer(subword): \n",
    "    print(subword, end='', flush=True) \n",
    "    # Return flag corresponds to whether generation should be stopped. \n",
    "    # False means continue generation. \n",
    "    return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af993560-e8f3-4997-b521-6101401a6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./llama-3.1-8b-instruct/INT4_compressed_weights/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4a073d-461c-4c38-b887-b5190f362589",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'  # GPU can be used as well \n",
    "pipe = openvino_genai.LLMPipeline(model_dir, device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea62f11-ff1a-4e41-9726-7c691cad9d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "question:\n",
      " what it is a llm?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Large Language Model. It's a type of artificial intelligence (AI) model that is trained on a massive corpus of text data to generate human-like language responses. LLMs are designed to understand and generate human language, including nuances, idioms, and context-dependent expressions.\n",
      "\n",
      "LLMs are typically trained on a large dataset of text, which allows them to learn patterns, relationships, and structures of language. This training enables them to generate text that is coherent, contextually relevant,\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "config = openvino_genai.GenerationConfig() \n",
    "config.max_new_tokens = 100 \n",
    "pipe.start_chat() \n",
    "while True: \n",
    "    prompt = input('question:\\n') \n",
    "    if 'Stop!' == prompt: \n",
    "        break \n",
    "    pipe.generate(prompt, config, streamer) \n",
    "    print('\\n----------') \n",
    "pipe.finish_chat() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1797376-561c-4b05-b607-a855f6103170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenVino",
   "language": "python",
   "name": "openvino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
