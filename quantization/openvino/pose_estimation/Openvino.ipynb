{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b608aa3-fc62-497f-945e-39854e2fca48",
   "metadata": {},
   "source": [
    "# Pose Estimation\n",
    "Pose estimation is an area of computer vision that involves detecting and tracking the position and orientation of objects, typically human body parts, in images or videos. This technique is widely used in various applications, from augmented reality and virtual reality to healthcare and sports analytics.\n",
    "\n",
    "There are two main types of pose estimation:\n",
    "\n",
    "- 2D Pose Estimation: This involves identifying key points on a 2D image, such as the head, shoulders, elbows, and knees. A popular tool for this is OpenPose, which can detect multiple people in real-time1.\n",
    "- 3D Pose Estimation: This goes a step further by predicting the 3D coordinates of key points, providing a more detailed understanding of the object’s spatial orientation. This is particularly useful in robotics and animation\n",
    "\n",
    "The goal of pose estimation is to detect the location and orientation of a person’s body parts, such as joints and limbs(keypoints), in an image or video. \n",
    "\n",
    "![image.png](image.png)\n",
    "\n",
    "There are two main approaches to pose estimation: single-person and multi-person. Single-person pose estimation finds the pose of one person in an image. It knows where the person is and how many keypoints to look for, making it a regression problem. Multi-person pose estimation is different. It tries to solve a harder problem where the number of people and their positions in the image are unknown.\n",
    "\n",
    "Single-person pose estimation can be further divided into two frameworks: direct regression-based and heatmap-based. Direct regression-based frameworks predict keypoints from feature map. Heatmap-based frameworks generate heatmaps of all keypoints within the image and then use additional methods to construct the final stick figure.\n",
    "\n",
    "Multi-person pose estimation problem can usually be approached in two ways. The first one, called top-down, applies a person detector and then runs a pose estimation algorithm per every detected person. So pose estimation problem is decoupled into two subproblems, and the state-of-the-art\n",
    "achievements from both areas can be utilized. The inference speed of this approach strongly depends on number of detected people inside the image.\n",
    "The second one, called bottom-up, more robust to the number of people. At first all keypoints are detected in a given image, then they are grouped by human instances. Such approach usually faster than the previous, since it finds keypoints once and does not rerun pose estimation for each person.\n",
    "\n",
    "The task is to predict a pose skeleton for every person in an image. The skeleton consists of keypoints (or joints): ankles, knees, hips, elbows, etc.\n",
    "- Inference of Neural Network to provide two tensors: keypoint heatmaps and their pairwise relations (part affinity fields, pafs). \n",
    "- Grouping keypoints by person instances. It includes upsampling tensors to original image size, keypoints extraction at the heatmaps peaks and their grouping by instances.\n",
    "  \n",
    "# Models\n",
    "- https://github.com/bmartacho/OmniPose\n",
    "- https://github.com/open-mmlab/mmpose\n",
    "- https://www.tensorflow.org/hub/tutorials/movenet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdede1d4-9108-4215-8f54-86dadacd1fcf",
   "metadata": {},
   "source": [
    "![image.png](image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ae1c7-81b6-4614-8e82-8c5c29be2e03",
   "metadata": {},
   "source": [
    "- pip install --upgrade pip && pip install -r requirements.txt\n",
    "- jupyter labextension install --no-build @jupyter-widgets/jupyterlab-manager\n",
    "- jupyter labextension install --no-build jupyter-datawidgets/extension\n",
    "- jupyter labextension install jupyter-threejs\n",
    "- jupyter labextension list\n",
    "\n",
    "- https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/model_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1c1a6-db03-45be-be50-64e465fa0683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pythreejs \"openvino-dev>=2024.0.0\" \"opencv-python\" \"torch\" \"onnx<1.16.2\" --extra-index-url https://download.pytorch.org/whl/cpu \n",
    "\n",
    "# pip install openvino-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f4c8ed-6fb0-48f8-b161-ca7beaa55b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import openvino as ov\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "with open(\"notebook_utils.py\", \"w\") as f:\n",
    "    f.write(r.text)\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/engine3js.py\",\n",
    ")\n",
    "with open(\"engine3js.py\", \"w\") as f:\n",
    "    f.write(r.text)\n",
    "\n",
    "import notebook_utils as utils\n",
    "import engine3js as engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0bbd50-fa8f-4ccf-bd14-e2be72e3211b",
   "metadata": {},
   "source": [
    "### Model\n",
    "https://docs.openvino.ai/2022.3/omz_models_model_human_pose_estimation_3d_0001.html\n",
    "\n",
    "The task is to predict a pose skeleton for every person in an image. The skeleton consists of keypoints (or joints): ankles, knees, hips, elbows, etc.\n",
    "- Inference of Neural Network to provide two tensors: keypoint heatmaps and their pairwise relations (part affinity fields, pafs). \n",
    "- Grouping keypoints by person instances. It includes upsampling tensors to original image size, keypoints extraction at the heatmaps peaks and their grouping by instances.\n",
    "\n",
    "The network first extracts features, then performs initial estimation of heatmaps and pafs, after that 5 refinement stages are performed. It is able to find 18 types of keypoints. Then grouping procedure searches the best pair (by affinity) for each keypoint, from the predefined list of keypoint pairs, e.g.\n",
    "left elbow and left wrist, right hip and right knee, left eye and left ear, and so on, 19 pairs overall.\n",
    "\n",
    "https://arxiv.org/pdf/1811.12004\n",
    "\n",
    "https://arxiv.org/pdf/1712.03453\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5724b7-b5ed-4680-a565-032bfd49246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where model will be downloaded\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# model name as named in Open Model Zoo\n",
    "model_name = \"human-pose-estimation-3d-0001\"\n",
    "# selected precision (FP32, FP16)\n",
    "precision = \"FP32\"\n",
    "\n",
    "BASE_MODEL_NAME = f\"{base_model_dir}/public/{model_name}/{model_name}\"\n",
    "model_path = Path(BASE_MODEL_NAME).with_suffix(\".pth\")\n",
    "onnx_path = Path(BASE_MODEL_NAME).with_suffix(\".onnx\")\n",
    "\n",
    "ir_model_path = Path(f\"model/public/{model_name}/{precision}/{model_name}.xml\")\n",
    "model_weights_path = Path(f\"model/public/{model_name}/{precision}/{model_name}.bin\")\n",
    "\n",
    "if not model_path.exists():\n",
    "    download_command = f\"omz_downloader \" f\"--name {model_name} \" f\"--output_dir {base_model_dir}\"\n",
    "    ! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f844ab6-4910-4c4e-91c7-23951a58e9db",
   "metadata": {},
   "source": [
    "### Model Conversion\n",
    "\n",
    "##### omz_converter --name human-pose-estimation-3d-0001 --precisions FP32 --download_dir model --output_dir model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dcf8ed6-be2c-4ceb-9050-1cfe4f7ca63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not onnx_path.exists():\n",
    "    convert_command = (\n",
    "        f\"omz_converter \" f\"--name {model_name} \" f\"--precisions {precision} \" f\"--download_dir {base_model_dir} \" f\"--output_dir {base_model_dir}\"\n",
    "    )\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c08fa-e229-405f-8aa8-3b41ced25b7c",
   "metadata": {},
   "source": [
    "### Select inference device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14db61d2-b0cc-45c4-b375-1a9aa1a46c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394b93144162435a885124647ffe528e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = utils.device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de9b95-cf1b-4519-a64a-13f05ecffcc6",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025a887a-e9ef-4d46-9401-bb973035d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize inference engine\n",
    "core = ov.Core()\n",
    "# read the network and corresponding weights from file\n",
    "model = core.read_model(model=ir_model_path, weights=model_weights_path)\n",
    "# load the model on the specified device\n",
    "compiled_model = core.compile_model(model=model, device_name=device.value)\n",
    "infer_request = compiled_model.create_infer_request()\n",
    "input_tensor_name = model.inputs[0].get_any_name()\n",
    "\n",
    "# get input and output names of nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layers = list(compiled_model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41693298-21fb-47c2-8d6b-6db0249c7806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<ConstOutput: names[data] shape[1,3,256,448] type: f32>,\n",
       " [<ConstOutput: names[features] shape[1,57,32,56] type: f32>,\n",
       "  <ConstOutput: names[heatmaps] shape[1,19,32,56] type: f32>,\n",
       "  <ConstOutput: names[pafs] shape[1,38,32,56] type: f32>])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer, output_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4586873-9311-417c-97db-1eb01c218488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data', ['features', 'heatmaps', 'pafs'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer.any_name, [o.any_name for o in output_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb06c5a-857c-4dc8-9801-5439a4dbddf1",
   "metadata": {},
   "source": [
    "## Draw 2D Pose Overlays\n",
    "We need to define some connections between the joints in advance, so that we can draw the structure of the human body in the resulting image after obtaining the inference results. Joints are drawn as circles and limbs are drawn as lines\n",
    "\n",
    "https://github.com/openvinotoolkit/open_model_zoo/tree/master/demos/human_pose_estimation_3d_demo/python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f4b78af-dda1-407a-97cb-094712507ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pose_utis import draw_poses,  body_edges, body_edges_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2ec7cc9-93ac-421d-9a0d-a94d920226e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_infer(scaled_img, stride,infer_request):\n",
    "    \"\"\"\n",
    "    Run model inference on the input image\n",
    "\n",
    "    Parameters:\n",
    "        scaled_img: resized image according to the input size of the model\n",
    "        stride: int, the stride of the window\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove excess space from the picture\n",
    "    img = scaled_img[\n",
    "        0 : scaled_img.shape[0] - (scaled_img.shape[0] % stride),\n",
    "        0 : scaled_img.shape[1] - (scaled_img.shape[1] % stride),\n",
    "    ]\n",
    "\n",
    "    img = np.transpose(img, (2, 0, 1))[None,]\n",
    "    infer_request.infer({input_tensor_name: img})\n",
    "    # A set of three inference results is obtained\n",
    "    results = {name: infer_request.get_tensor(name).data[:] for name in {\"features\", \"heatmaps\", \"pafs\"}}\n",
    "    # Get the results\n",
    "    results = (results[\"features\"][0], results[\"heatmaps\"][0], results[\"pafs\"][0])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8544caea-469b-48d8-a874-6c9edcddffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pose_estimation(source=0, flip=False, use_popup=False, skip_frames=0,infer_request=None):\n",
    "    \"\"\"\n",
    "    2D image as input, using OpenVINO as inference backend,\n",
    "    get joints 3D coordinates, and draw 3D human skeleton in the scene\n",
    "\n",
    "    :param source:      The webcam number to feed the video stream with primary webcam set to \"0\", or the video path.\n",
    "    :param flip:        To be used by VideoPlayer function for flipping capture image.\n",
    "    :param use_popup:   False for showing encoded frames over this notebook, True for creating a popup window.\n",
    "    :param skip_frames: Number of frames to skip at the beginning of the video.\n",
    "    \"\"\"\n",
    "\n",
    "    focal_length = -1  # default\n",
    "    stride = 8\n",
    "    player = None\n",
    "    skeleton_set = None\n",
    "\n",
    "    try:\n",
    "        # create video player to play with target fps  video_path\n",
    "        # get the frame from camera\n",
    "        # You can skip first N frames to fast forward video. change 'skip_first_frames'\n",
    "        player = utils.VideoPlayer(source, flip=flip, fps=30, skip_first_frames=skip_frames)\n",
    "        # start capturing\n",
    "        player.start()\n",
    "\n",
    "        input_image = player.next()\n",
    "        # set the window size\n",
    "        resize_scale = 450 / input_image.shape[1]\n",
    "        windows_width = int(input_image.shape[1] * resize_scale)\n",
    "        windows_height = int(input_image.shape[0] * resize_scale)\n",
    "\n",
    "        # use visualization library\n",
    "        engine3D = engine.Engine3js(grid=True, axis=True, view_width=windows_width, view_height=windows_height)\n",
    "\n",
    "        if use_popup:\n",
    "            # display the 3D human pose in this notebook, and origin frame in popup window\n",
    "            display(engine3D.renderer)\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(title, cv2.WINDOW_KEEPRATIO | cv2.WINDOW_AUTOSIZE)\n",
    "        else:\n",
    "            # set the 2D image box, show both human pose and image in the notebook\n",
    "            imgbox = widgets.Image(format=\"jpg\", height=windows_height, width=windows_width)\n",
    "            display(widgets.HBox([engine3D.renderer, imgbox]))\n",
    "\n",
    "        skeleton = engine.Skeleton(body_edges=body_edges)\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "\n",
    "        while True:\n",
    "            # grab the frame\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "\n",
    "            # resize image and change dims to fit neural network input\n",
    "            # (see https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/human-pose-estimation-3d-0001)\n",
    "            scaled_img = cv2.resize(frame, dsize=(model.inputs[0].shape[3], model.inputs[0].shape[2]))\n",
    "\n",
    "            if focal_length < 0:  # Focal length is unknown\n",
    "                focal_length = np.float32(0.8 * scaled_img.shape[1])\n",
    "\n",
    "            # inference start\n",
    "            start_time = time.time()\n",
    "            # get results\n",
    "            inference_result = model_infer(scaled_img, stride,infer_request)\n",
    "\n",
    "            # inference stop\n",
    "            stop_time = time.time()\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # Process the point to point coordinates of the data\n",
    "            poses_3d, poses_2d = engine.parse_poses(inference_result, 1, stride, focal_length, True)\n",
    "\n",
    "            # use processing times from last 200 frames\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "\n",
    "            if len(poses_3d) > 0:\n",
    "                # From here, you can rotate the 3D point positions using the function \"draw_poses\",\n",
    "                # or you can directly make the correct mapping below to properly display the object image on the screen\n",
    "                poses_3d_copy = poses_3d.copy()\n",
    "                x = poses_3d_copy[:, 0::4]\n",
    "                y = poses_3d_copy[:, 1::4]\n",
    "                z = poses_3d_copy[:, 2::4]\n",
    "                poses_3d[:, 0::4], poses_3d[:, 1::4], poses_3d[:, 2::4] = (\n",
    "                    -z + np.ones(poses_3d[:, 2::4].shape) * 200,\n",
    "                    -y + np.ones(poses_3d[:, 2::4].shape) * 100,\n",
    "                    -x,\n",
    "                )\n",
    "\n",
    "                poses_3d = poses_3d.reshape(poses_3d.shape[0], 19, -1)[:, :, 0:3]\n",
    "                people = skeleton(poses_3d=poses_3d)\n",
    "\n",
    "                try:\n",
    "                    engine3D.scene_remove(skeleton_set)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                engine3D.scene_add(people)\n",
    "                skeleton_set = people\n",
    "\n",
    "                # draw 2D\n",
    "                frame = draw_poses(frame, poses_2d, scaled_img, use_popup)\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    engine3D.scene_remove(skeleton_set)\n",
    "                    skeleton_set = None\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                (10, 30),\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                0.7,\n",
    "                (0, 0, 255),\n",
    "                1,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "            if use_popup:\n",
    "                cv2.imshow(title, frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27, use ESC to exit\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # encode numpy array to jpg\n",
    "                imgbox.value = cv2.imencode(\n",
    "                    \".jpg\",\n",
    "                    frame,\n",
    "                    params=[cv2.IMWRITE_JPEG_QUALITY, 90],\n",
    "                )[1].tobytes()\n",
    "\n",
    "            engine3D.renderer.render(engine3D.scene, engine3D.cam)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        clear_output()\n",
    "        if player is not None:\n",
    "            # stop capturing\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()\n",
    "        if skeleton_set:\n",
    "            engine3D.scene_remove(skeleton_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7df8241e-a5e7-415a-9d54-6c88b8dde3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WEBCAM = False\n",
    "\n",
    "cam_id = 0\n",
    "video_path = \"https://storage.openvinotoolkit.org/data/test_data/videos/face-demographics-walking.mp4\"\n",
    "video_path = \"https://github.com/tensorflow/tfjs-models/raw/master/pose-detection/assets/dance_input.gif\"\n",
    "video_path = \"file://D:/repos/openvino/pose/data/1585619-sd_960_540_30fps.mp4\"\n",
    "source = cam_id if USE_WEBCAM else video_path\n",
    "\n",
    "run_pose_estimation(source=source, flip=isinstance(source, int), use_popup=False, infer_request=infer_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626f2fb-2d6b-44c2-abc1-d0f9fa0e519e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenVino",
   "language": "python",
   "name": "openvino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
