{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN+LPlQFfXt9IYL6eW8kTYw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/quantization/openvino/TensorRT-LLM/TensorRT_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NVIDIA TensorRT\n",
        "NVIDIA® TensorRT™ is an SDK for high-performance deep learning inference. It is designed to work in a complementary fashion with training frameworks such as TensorFlow, PyTorch, and MXNet. It focuses specifically on running an already-trained network quickly and efficiently on NVIDIA hardware.\n",
        "\n",
        "TensorRT includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. The core of NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA GPUs. TensorRT takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine that performs inference for that network\n",
        "\n",
        "NVIDIA® TensorRT™ is an ecosystem of APIs for high-performance deep learning inference. TensorRT includes an inference runtime and model optimizations that deliver low latency and high throughput for production applications. The TensorRT ecosystem includes TensorRT, TensorRT-LLM, TensorRT Model Optimizer, and TensorRT Cloud.\n",
        "\n",
        "https://github.com/NVIDIA/TensorRT\n",
        "\n",
        "# TensorRT-LLM\n",
        "https://nvidia.github.io/TensorRT-LLM/quick-start-guide.html\n",
        "\n",
        "A TensorRT Toolbox for Optimized Large Language Model Inference\n",
        "\n",
        "#### Install\n",
        "TensorRT-LLM is a toolkit to assemble optimized solutions to perform Large Language Model (LLM) inference. It offers a Model Definition API to define models and compile efficient TensorRT engines for NVIDIA GPUs. It also contains Python and C++ components to build runtimes to execute those engines as well as backends for the Triton Inference Server to easily create web-based services for LLMs. TensorRT-LLM supports multi-GPU and multi-node configurations (through MPI).\n",
        "\n",
        "https://nvidia.github.io/TensorRT-LLM/installation/linux.html\n",
        "\n",
        "https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html#support-matrix-software\n",
        "\n",
        "https://github.com/NVIDIA/TensorRT-LLM\n",
        "\n",
        "Install dependencies, TensorRT-LLM requires Python 3.10\n",
        "apt-get update && apt-get -y install python3.10 python3-pip openmpi-bin libopenmpi-dev git git-lfs\n",
        "\n",
        "Install the latest preview version (corresponding to the main branch) of TensorRT-LLM. If you want to install the stable version (corresponding to the release branch), please remove the --pre option.\n",
        "\n",
        "pip3 install tensorrt_llm -U --extra-index-url https://pypi.nvidia.com"
      ],
      "metadata": {
        "id": "KGMhvVw5nIHw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1-qHdgvSQIz",
        "outputId": "b62440c4-bbac-4093-d016-f87da7026e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.8/464.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.9/346.9 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.1/374.1 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.7/453.7 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorrt-cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip3 install tensorrt_llm  tensorrt --extra-index-url https://pypi.nvidia.com -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorrt_llm import LLM, SamplingParams\n",
        "\n",
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The president of the United States is\",\n",
        "    \"The capital of France is\",\n",
        "    \"The future of AI is\",\n",
        "]\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_new_tokens=256)\n",
        "\n",
        "llm = LLM(model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RorwNa--UTY8",
        "outputId": "330137d3-7405-40c9-cfa0-5122e87b2356"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading Model: \u001b[1;32m[1/3]\t\u001b[0mDownloading HF model\n",
            "\u001b[38;20mDownloaded model to /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6\n",
            "\u001b[0m\u001b[38;20mTime: 0.260s\n",
            "\u001b[0mLoading Model: \u001b[1;32m[2/3]\t\u001b[0mLoading HF model to memory\n",
            "160it [00:00, 2036.85it/s]\n",
            "\u001b[38;20mTime: 0.105s\n",
            "\u001b[0mLoading Model: \u001b[1;32m[3/3]\t\u001b[0mBuilding TRT-LLM engine\n",
            "\u001b[38;20mTime: 28.062s\n",
            "\u001b[0m\u001b[1;32mLoading model done.\n",
            "\u001b[0m\u001b[38;20mTotal latency: 28.427s\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import torch\n",
        "import gc"
      ],
      "metadata": {
        "id": "jmgaDPSNINA7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time1 = datetime.datetime.now()\n",
        "\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "# Print the outputs.\n",
        "for p in prompts:\n",
        "  outputs = llm.generate(p, sampling_params)\n",
        "\n",
        "  generated_text = outputs.outputs[0].text\n",
        "  print(f\"Prompt: {p!r},\\n Generated text: {generated_text!r}\")\n",
        "  print(\"-\"*25)\n",
        "\n",
        "\n",
        "time2 = datetime.datetime.now()\n",
        "print(f\"Time {str(time2-time1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uf5Juy5Unta",
        "outputId": "0fb886a9-1712-4e61-f271-f6a489aa6f00"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Hello, my name is',\n",
            " Generated text: 'Sarah. I am a professional writer and editor with more than six years of experience in writing articles and blogs for a variety of industries. I am passionate about learning and sharing knowledge, and I enjoy helping others learn and grow through my writing. In my free time, I enjoy traveling, reading, and spending time with my family and friends. You can find me on LinkedIn, Facebook, and Twitter.'\n",
            "-------------------------\n",
            "Prompt: 'The president of the United States is',\n",
            " Generated text: 'required to submit a daily report to Congress on the coronavirus pandemic. What is the recommended format for the daily report?'\n",
            "-------------------------\n",
            "Prompt: 'The capital of France is',\n",
            " Generated text: \"located in which region?\\n\\n2. What is the second-largest city in France by population and what is its name?\\n\\n3. What is the currency used in France and where does it come from?\\n\\n4. What is the official language spoken in France and which countries does it belong to?\\n\\n5. What is the most commonly spoken language in France and which country is it a part of?\\n\\n6. What is the capital of Provence-Alpes-Côte d'Azur and what is its name?\\n\\n7. What is the capital of Corsica and what is its name?\\n\\n8. What is the capital of Brittany and what is its name?\\n\\n9. What is the capital of Lorraine and what is its name?\\n\\n10. What is the capital of Alsace and what is its name?\\n\\n11. What is the capital of Burgundy and what is its name?\\n\\n12. What is the capital of Provence and what is its name?\\n\\n13. What is the capital of Auvergne and what is its name?\\n\\n14. What is the capital of the Cô\"\n",
            "-------------------------\n",
            "Prompt: 'The future of AI is',\n",
            " Generated text: 'an exciting time for us. We are constantly researching, developing, and improving our platform to create the most advanced and efficient technologies available. Our team is composed of some of the brightest minds in the AI industry, and we are always looking for ways to push the boundaries of what is possible with machine learning. We are committed to building a diverse and inclusive workplace where everyone can thrive and contribute their unique perspectives and experiences. We strive to create a culture that values collaboration, innovation, and creativity, and we are committed to supporting our team members in achieving their personal and professional goals. Our goal is to create a world where AI is a fundamental part of our lives, and we are committed to making that a reality.'\n",
            "-------------------------\n",
            "Time 0:00:07.398469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del llm"
      ],
      "metadata": {
        "id": "pEUvKozqKNex"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_0Ra60YJeEe",
        "outputId": "fd5f7300-47a2-47c4-b5ff-fa2bbe353d63"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "705"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"cuda\")"
      ],
      "metadata": {
        "id": "IE3rRVUWq_Lb"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "time1 = datetime.datetime.now()\n",
        "for p in prompts:\n",
        "  messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": p},\n",
        "]\n",
        "\n",
        "  prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "  outputs = pipe(p, max_new_tokens=256, do_sample=True, temperature=0.8,  top_p=0.95)\n",
        "  generated_text = outputs[0][\"generated_text\"]\n",
        "  print(f\"Prompt: {p!r},\\n Generated text: {generated_text!r}\")\n",
        "  print(\"-\"*25)\n",
        "\n",
        "\n",
        "time2 = datetime.datetime.now()\n",
        "print(f\"Time {str(time2-time1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sPNxkPJrBgd",
        "outputId": "01c13485-83de-4326-c55a-c21a6584e420"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Hello, my name is',\n",
            " Generated text: \"Hello, my name is [Your Name] and I am a [Job Title] at [Company Name]. I have been working for [Company Name] for [Number of Years] years and I am currently responsible for managing [Number of Employees] employees. I have experience in managing [Number of Employees] at other companies, which gives me a good understanding of the challenges and opportunities that come with managing a large team. I am very familiar with [Company Name]'s mission and vision, and I am confident in my ability to ensure that our team is aligned with these goals.\\n\\nI am committed to providing the highest level of service to [Company Name]'s employees, as well as to ensuring that our team is fully engaged and motivated to achieve our company's goals. I am excited to join [Company Name] and contribute to its continued success.\\n\\nIn my previous role as [Previous Position], I was responsible for [Responsibilities], which included [Responsibilities]. In this role, I worked closely with [Supervisor Name] and [Supervisor Name] to ensure that [Number of Employees] employees were meeting their goals and fulfilling their responsibilities\"\n",
            "-------------------------\n",
            "Prompt: 'The president of the United States is',\n",
            " Generated text: \"The president of the United States is a man in a suit.\\n\\nPresident of the United States (POTUS)\\n\\nPOTUS is a man in a suit who stands at the forefront of American political power. The president is responsible for the direction of the country and represents the interests of the American people.\\n\\nThe president's responsibilities include:\\n\\n1. Signing bills into law\\n2. Formulating national security policies\\n3. Advising the president-elect on foreign policy\\n4. Conducting official diplomatic negotiations\\n5. Leading military operations\\n6. Serving as the commander-in-chief\\n\\nThe president's job is to uphold the Constitution and protect the rights and freedoms of all Americans. They are also responsible for overseeing the performance of government agencies and ensuring that public resources are used efficiently and effectively.\\n\\nThe president's role in American democracy is critical, as it sets the tone for the rest of the country. When the president is reactive and weak, the government is susceptible to corruption, abuse of power, and other forms of political failure. On the other hand, when the president is strong and effective, the\"\n",
            "-------------------------\n",
            "Prompt: 'The capital of France is',\n",
            " Generated text: 'The capital of France is also known as Paris. This city is one of the oldest in the world, and it has been a major city in Europe since the Middle Ages. Some of the most famous landmarks in Paris include the Eiffel Tower, Notre Dame Cathedral, and the Louvre Museum.\\n\\n4. New York City, USA: New York City is located in the northeastern United States and is known as the \"Big Apple.\" It is one of the most populous cities in the world, and it is home to some of the most famous landmarks in the world, including the Statue of Liberty, Times Square, and Central Park.\\n\\n5. Tokyo, Japan: Tokyo is located in the Tohoku region of Japan, and it is known as the \"Eternal City.\" It is one of the most bustling cities in the world, and it has an incredible variety of landmarks, including the Tokyo Tower, Senso-ji Temple, and the Tokyo Tower.\\n\\n6. London, UK: London is located in southeastern England, and it is one of the most famous cities in the world. It is known for its history, culture, and many of the world\\'s most famous land'\n",
            "-------------------------\n",
            "Prompt: 'The future of AI is',\n",
            " Generated text: \"The future of AI is in the hands of those who understand the role of this technology in enhancing and transforming our world, and I believe that's the responsibility that I take on.\"\n",
            "-------------------------\n",
            "Time 0:00:27.978188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del pipe\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Le-IeZR-LTRM",
        "outputId": "1b60f8ed-1d06-4a72-a103-5450d4668437"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}