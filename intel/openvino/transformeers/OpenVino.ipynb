{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f43992-e5f7-4cd1-b72e-fbf8af572846",
   "metadata": {},
   "source": [
    "# Intel OpenVino (Open Visual Inference and Neural Network Optimization)\n",
    "\n",
    "\n",
    "https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html\n",
    "\n",
    "The OpenVINO™ toolkit enables you to optimize a deep learning model from almost any framework and deploy it with best-in-class performance on a range of Intel® processors and other hardware platforms.\n",
    "\n",
    "OpenVINO™ toolkit is an open source toolkit that accelerates AI inference with lower latency and higher throughput while maintaining accuracy, reducing model footprint, and optimizing hardware use. It streamlines AI development and integration of deep learning in domains like computer vision, large language models (LLM), and generative AI.\n",
    "\n",
    "# Install\n",
    "\n",
    "\n",
    "```python\n",
    "python -m venv openvino_env\n",
    "\n",
    "openvino_env\\Scripts\\activate\n",
    "\n",
    "python -m pip install --upgrade pip\n",
    "\n",
    "pip install openvino-genai==2024.2.0\n",
    "\n",
    "pip install \"openvino>=2023.1.0\" transformers \"torch>=2.1\" tqdm --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "```\n",
    "# OpenVINO IR format\n",
    "https://docs.openvino.ai/2024/documentation/openvino-ir-format.html\n",
    "\n",
    "OpenVINO supports the following model formats:\n",
    "\n",
    "- PyTorch\n",
    "- TensorFlow\n",
    "- TensorFlow Lite\n",
    "- ONNX\n",
    "- PaddlePaddle\n",
    "- OpenVINO IR\n",
    "#  Neural Network Compression Framework (NNCF)\n",
    "\n",
    "Neural Network Compression Framework (NNCF) provides a suite of post-training and training-time algorithms for optimizing inference of neural networks in OpenVINO™ with a minimal accuracy drop.\n",
    "\n",
    "NNCF is designed to work with models from PyTorch, TensorFlow, ONNX and OpenVINO™\n",
    "\n",
    "![title](nncf.png)\n",
    "\n",
    "https://github.com/openvinotoolkit/nncf\n",
    "\n",
    "https://docs.openvino.ai/2024/openvino-workflow/model-optimization.html\n",
    "\n",
    "- Post-training Quantization is designed to optimize the inference of deep learning models by applying the post-training 8-bit integer quantization that does not require model retraining or fine-tuning.\n",
    "\n",
    "- Training-time Optimization , a suite of advanced methods for training-time model optimization within the DL framework, such as PyTorch and TensorFlow 2.x. It supports methods like Quantization-aware Training, Structured and Unstructured Pruning, etc.\n",
    "\n",
    "- Weight Compression, an easy-to-use method for Large Language Models footprint reduction and inference acceleration.\n",
    "\n",
    "![title](nncf2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb609b0-2348-472e-9f10-58325fcd45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "import time\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "793d441c-4e6a-4a8b-82e8-0418afc9d7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024.2.0-15519-5c0f38f83f6-releases/2024/2'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ov.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853d6607-35fa-4f36-a80d-7931b7b8c084",
   "metadata": {},
   "source": [
    "# Model\n",
    "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e1669d-7171-492f-8a08-d99c9715d22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d02e803cc54dd4ba61ce69409da253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28672a0c6b24d089b831c2f94a1ca3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc38dcb-1586-47e7-b3cd-eccac4461270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0196dff18934d1b838a91794841371b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37278c8d45d64eae95157e37955dc13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ff61d3-e937-4f8f-b42c-6243e01cb2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\repos\\openvino_env\\Lib\\site-packages\\transformers\\modeling_utils.py:4664: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "D:\\repos\\openvino_env\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:215: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ir_xml_name = checkpoint + \".xml\"\n",
    "MODEL_DIR = \"model/\"\n",
    "ir_xml_path = Path(MODEL_DIR) / ir_xml_name\n",
    "\n",
    "MAX_SEQ_LENGTH = 128\n",
    "input_info = [\n",
    "    (ov.PartialShape([1, -1]), ov.Type.i64),\n",
    "    (ov.PartialShape([1, -1]), ov.Type.i64),\n",
    "]\n",
    "default_input = torch.ones(1, MAX_SEQ_LENGTH, dtype=torch.int64)\n",
    "inputs = {\n",
    "    \"input_ids\": default_input,\n",
    "    \"attention_mask\": default_input,\n",
    "}\n",
    "\n",
    "ov_model = ov.convert_model(model, input=input_info, example_input=inputs)\n",
    "ov.save_model(ov_model, ir_xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "667e571b-3334-4a61-9cf4-b1bb5564493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96e8e95a-1122-4f79-83a4-c10d3e537019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPU', 'GPU']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core.available_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6678bd2-df39-4cf8-89a6-20dfb3fa4224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650 (dGPU)'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"GPU\"\n",
    "core.get_property(device, \"FULL_DEVICE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcf95a5a-14b3-4a83-b93e-4fd0e8e17cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU SUPPORTED_PROPERTIES:\n",
      "\n",
      "AVAILABLE_DEVICES               : ['0']\n",
      "RANGE_FOR_ASYNC_INFER_REQUESTS  : (1, 2, 1)\n",
      "RANGE_FOR_STREAMS               : (1, 2)\n",
      "OPTIMAL_BATCH_SIZE              : 1\n",
      "MAX_BATCH_SIZE                  : 1\n",
      "DEVICE_ARCHITECTURE             : GPU: vendor=0x10de arch=v7.5.0\n",
      "FULL_DEVICE_NAME                : NVIDIA GeForce GTX 1650 (dGPU)\n",
      "DEVICE_UUID                     : 14d4b062a125d17e3637a99793a8a24b\n",
      "DEVICE_LUID                     : 18da000000000000\n",
      "DEVICE_TYPE                     : Type.DISCRETE\n",
      "DEVICE_GOPS                     : {<Type: 'float16'>: 0.0, <Type: 'float32'>: 0.0, <Type: 'int8_t'>: 0.0, <Type: 'uint8_t'>: 0.0}\n",
      "OPTIMIZATION_CAPABILITIES       : ['FP32', 'BIN', 'INT8', 'EXPORT_IMPORT']\n",
      "GPU_DEVICE_TOTAL_MEM_SIZE       : 4294639616\n",
      "GPU_UARCH_VERSION               : 7.5.0\n",
      "GPU_EXECUTION_UNITS_COUNT       : 14\n",
      "GPU_MEMORY_STATISTICS           : {}\n",
      "PERF_COUNT                      : False\n",
      "MODEL_PRIORITY                  : Priority.MEDIUM\n",
      "GPU_HOST_TASK_PRIORITY          : Priority.MEDIUM\n",
      "GPU_QUEUE_PRIORITY              : Priority.MEDIUM\n",
      "GPU_QUEUE_THROTTLE              : Priority.MEDIUM\n",
      "GPU_ENABLE_SDPA_OPTIMIZATION    : True\n",
      "GPU_ENABLE_LOOP_UNROLLING       : True\n",
      "GPU_DISABLE_WINOGRAD_CONVOLUTION: False\n",
      "CACHE_DIR                       : \n",
      "CACHE_MODE                      : CacheMode.OPTIMIZE_SPEED\n",
      "PERFORMANCE_HINT                : PerformanceMode.LATENCY\n",
      "EXECUTION_MODE_HINT             : ExecutionMode.PERFORMANCE\n",
      "COMPILATION_NUM_THREADS         : 16\n",
      "NUM_STREAMS                     : 1\n",
      "PERFORMANCE_HINT_NUM_REQUESTS   : 0\n",
      "INFERENCE_PRECISION_HINT        : <Type: 'float16'>\n",
      "ENABLE_CPU_PINNING              : False\n",
      "DEVICE_ID                       : 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"{device} SUPPORTED_PROPERTIES:\\n\")\n",
    "supported_properties = core.get_property(device, \"SUPPORTED_PROPERTIES\")\n",
    "indent = len(max(supported_properties, key=len))\n",
    "\n",
    "for property_key in supported_properties:\n",
    "    if property_key not in ('SUPPORTED_METRICS', 'SUPPORTED_CONFIG_KEYS', 'SUPPORTED_PROPERTIES'):\n",
    "        try:\n",
    "            property_val = core.get_property(device, property_key)\n",
    "        except TypeError:\n",
    "            property_val = 'UNSUPPORTED TYPE'\n",
    "        print(f\"{property_key:<{indent}}: {property_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4348bd96-15c8-4b23-9c22-6ade98b28f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intel(R) Core(TM) i7-10870H CPU @ 2.20GHz'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"CPU\"\n",
    "core.get_property(device, \"FULL_DEVICE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a91d5154-04a4-49ce-b0f1-79f67abe9ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU SUPPORTED_PROPERTIES:\n",
      "\n",
      "AVAILABLE_DEVICES                    : ['']\n",
      "RANGE_FOR_ASYNC_INFER_REQUESTS       : (1, 1, 1)\n",
      "RANGE_FOR_STREAMS                    : (1, 16)\n",
      "EXECUTION_DEVICES                    : ['CPU']\n",
      "FULL_DEVICE_NAME                     : Intel(R) Core(TM) i7-10870H CPU @ 2.20GHz\n",
      "OPTIMIZATION_CAPABILITIES            : ['FP32', 'INT8', 'BIN', 'EXPORT_IMPORT']\n",
      "DEVICE_TYPE                          : Type.INTEGRATED\n",
      "DEVICE_ARCHITECTURE                  : intel64\n",
      "NUM_STREAMS                          : 1\n",
      "INFERENCE_NUM_THREADS                : 0\n",
      "PERF_COUNT                           : False\n",
      "INFERENCE_PRECISION_HINT             : <Type: 'float32'>\n",
      "PERFORMANCE_HINT                     : PerformanceMode.LATENCY\n",
      "EXECUTION_MODE_HINT                  : ExecutionMode.PERFORMANCE\n",
      "PERFORMANCE_HINT_NUM_REQUESTS        : 0\n",
      "ENABLE_CPU_PINNING                   : True\n",
      "SCHEDULING_CORE_TYPE                 : SchedulingCoreType.ANY_CORE\n",
      "MODEL_DISTRIBUTION_POLICY            : set()\n",
      "ENABLE_HYPER_THREADING               : True\n",
      "DEVICE_ID                            : \n",
      "CPU_DENORMALS_OPTIMIZATION           : False\n",
      "LOG_LEVEL                            : Level.NO\n",
      "CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE: 1.0\n",
      "DYNAMIC_QUANTIZATION_GROUP_SIZE      : 0\n",
      "KV_CACHE_PRECISION                   : <Type: 'float16'>\n",
      "AFFINITY                             : Affinity.NONE\n"
     ]
    }
   ],
   "source": [
    "print(f\"{device} SUPPORTED_PROPERTIES:\\n\")\n",
    "supported_properties = core.get_property(device, \"SUPPORTED_PROPERTIES\")\n",
    "indent = len(max(supported_properties, key=len))\n",
    "\n",
    "for property_key in supported_properties:\n",
    "    if property_key not in ('SUPPORTED_METRICS', 'SUPPORTED_CONFIG_KEYS', 'SUPPORTED_PROPERTIES'):\n",
    "        try:\n",
    "            property_val = core.get_property(device, property_key)\n",
    "        except TypeError:\n",
    "            property_val = 'UNSUPPORTED TYPE'\n",
    "        print(f\"{property_key:<{indent}}: {property_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ebb3a9d4-b770-47c3-8504-b1d4daedb3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7b038df11f402696518a069f6befa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "167c3e81-4c55-4ec5-b128-8c650233537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "compiled_model = core.compile_model(ov_model, device.value)\n",
    "infer_request = compiled_model.create_infer_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac4e363b-0cce-4ddf-8fdf-a3814eb69f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Defining a softmax function to extract\n",
    "    the prediction from the output of the IR format\n",
    "    Parameters: Logits array\n",
    "    Returns: Probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "715825bb-dd28-4178-9199-c409773841fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(input_text):\n",
    "    \"\"\"\n",
    "    Creating a generic inference function\n",
    "    to read the input and infer the result\n",
    "    into 2 classes: Positive or Negative.\n",
    "    Parameters: Text to be processed\n",
    "    Returns: Label: Positive or Negative.\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = tokenizer(\n",
    "        input_text,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    inputs = dict(input_text)\n",
    "    label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "    result = infer_request.infer(inputs=inputs)\n",
    "    for i in result.values():\n",
    "        probability = np.argmax(softmax(i))\n",
    "    return label[probability]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d433808-7255-49e3-a257-3dd4e0845cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  POSITIVE\n",
      "Total Time:  0.17  seconds\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I had a wonderful day\"\n",
    "start_time = time.perf_counter()\n",
    "result = infer(input_text)\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "print(\"Label: \", result)\n",
    "print(\"Total Time: \", \"%.2f\" % total_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c6d25a7-59b3-4c9a-b780-0c7f41a4ba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data\\food_reviews.txt' already exists.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "from notebook_utils import download_file\n",
    "\n",
    "# Download the text from the openvino_notebooks storage\n",
    "# vocab_file_path = download_file(\n",
    "#     \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/text/food_reviews.txt\",\n",
    "#     directory=\"data\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bdec535-ca8e-4087-b86b-d5b3a9252e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input:  The food was horrible.\n",
      "\n",
      "Label:  NEGATIVE \n",
      "\n",
      "User Input:  We went because the restaurant had good reviews\n",
      "\n",
      "Label:  POSITIVE \n",
      "\n",
      "User Input:  The trip turn into a nigthmare\n",
      "\n",
      "Label:  NEGATIVE \n",
      "\n",
      "User Input:  I dont really know what to say. I dont have a stand about that\n",
      "Label:  NEGATIVE \n",
      "\n",
      "Total Time:  0.61  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "with vocab_file_path.open(mode=\"r\") as f:\n",
    "    input_text = f.readlines()\n",
    "    for lines in input_text:\n",
    "        print(\"User Input: \", lines)\n",
    "        result = infer(lines)\n",
    "        print(\"Label: \", result, \"\\n\")\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "print(\"Total Time: \", \"%.2f\" % total_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "713ac79f-a060-4ac7-89b4-371ed6759702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7828fa33952e44e4baa92d9f6cea576e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e297475-e6ee-459d-9dc2-394ad94db8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "compiled_model = core.compile_model(ov_model, device.value)\n",
    "infer_request = compiled_model.create_infer_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "724bdaf8-46a6-42c4-b81c-6e52e763bda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  POSITIVE\n",
      "Total Time:  0.18  seconds\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I had a wonderful day\"\n",
    "start_time = time.perf_counter()\n",
    "result = infer(input_text)\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "print(\"Label: \", result)\n",
    "print(\"Total Time: \", \"%.2f\" % total_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a655845d-bb61-432d-bccd-15610008ef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input:  The food was horrible.\n",
      "\n",
      "Label:  NEGATIVE \n",
      "\n",
      "User Input:  We went because the restaurant had good reviews\n",
      "\n",
      "Label:  POSITIVE \n",
      "\n",
      "User Input:  The trip turn into a nigthmare\n",
      "\n",
      "Label:  NEGATIVE \n",
      "\n",
      "User Input:  I dont really know what to say. I dont have a stand about that\n",
      "Label:  NEGATIVE \n",
      "\n",
      "Total Time:  0.46  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "with vocab_file_path.open(mode=\"r\") as f:\n",
    "    input_text = f.readlines()\n",
    "    for lines in input_text:\n",
    "        print(\"User Input: \", lines)\n",
    "        result = infer(lines)\n",
    "        print(\"Label: \", result, \"\\n\")\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "print(\"Total Time: \", \"%.2f\" % total_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd10cd2-e585-45c1-b2a9-a948c9a1a6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
